# Dynamic Programming

## DP方法简介

- 由于其大量的计算损耗，已经不实用，但理论上非常重要。
- 本书后续的所有方法可以看做想要取得和DP类似的效果；只不过是减少了计算或者假设没有完美的环境模型。
- 假设解决的问题是有限的MDP，即给定动作a，状态s，和奖励r，可以通过p(s′,r|s,a)描述动态变化。



##

Value Iteration简单地说就是每次评估价值的时候直接用可能的用最优价值函数更新价值函数（这样的每一步不涉及策略本身）；在确定已经获得比较准确的价值评估之后，再一次性确定策略。

https://applenob.github.io/rl_note/intro-note-4/
