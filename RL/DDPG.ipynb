{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitpytorchconda70fdc7f787194f4c972bb3207dd25917",
   "display_name": "Python 3.8.3 64-bit ('pytorch': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "https://github.com/hangsz/reinforcement_learning/blob/master/DDPG/Pendulum-v0/ddpg.py"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        x = F.relu(self.linear1(s))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.tanh(self.linear3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.linear3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        x = torch.cat([s, a], 1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        s_dim = self.env.observation_space.shape[0]\n",
    "        a_dim = self.env.action_space.shape[0]\n",
    "\n",
    "        self.actor = Actor(s_dim, 256, a_dim)\n",
    "        self.actor_target = Actor(s_dim, 256, a_dim)\n",
    "        self.critic = Critic(s_dim+a_dim, 256, a_dim)\n",
    "        self.critic_target = Critic(s_dim+a_dim, 256, a_dim)\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr = self.actor_lr)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr = self.critic_lr)\n",
    "        self.buffer = []\n",
    "        \n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "    def act(self, s0):\n",
    "        s0 = torch.tensor(s0, dtype=torch.float).unsqueeze(0)\n",
    "        a0 = self.actor(s0).squeeze(0).detach().numpy()\n",
    "        return a0\n",
    "    \n",
    "    def put(self, *transition): \n",
    "        if len(self.buffer)== self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return \n",
    "        \n",
    "        samples = random.sample(self.buffer, self.batch_size)\n",
    "        \n",
    "        s0, a0, r1, s1 = zip(*samples)\n",
    "        \n",
    "        s0 = torch.tensor(s0, dtype=torch.float)\n",
    "        a0 = torch.tensor(a0, dtype=torch.float)\n",
    "        r1 = torch.tensor(r1, dtype=torch.float).view(self.batch_size,-1)\n",
    "        s1 = torch.tensor(s1, dtype=torch.float)\n",
    "        \n",
    "        def critic_learn():\n",
    "            a1 = self.actor_target(s1).detach()\n",
    "            y_true = r1 + self.gamma * self.critic_target(s1, a1).detach()\n",
    "            \n",
    "            y_pred = self.critic(s0, a0)\n",
    "            \n",
    "            loss_fn = nn.MSELoss()\n",
    "            loss = loss_fn(y_pred, y_true)\n",
    "            self.critic_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.critic_optim.step()\n",
    "            \n",
    "        def actor_learn():\n",
    "            loss = -torch.mean( self.critic(s0, self.actor(s0)) )\n",
    "            self.actor_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.actor_optim.step()\n",
    "                                           \n",
    "        def soft_update(net_target, net, tau):\n",
    "            for target_param, param  in zip(net_target.parameters(), net.parameters()):\n",
    "                target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "    \n",
    "        critic_learn()\n",
    "        actor_learn()\n",
    "        soft_update(self.critic_target, self.critic, self.tau)\n",
    "        soft_update(self.actor_target, self.actor, self.tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "    env = gym.make('Pendulum-v0')\n",
    "    env.reset()\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "    params = {\n",
    "        'env': env,\n",
    "        'gamma': 0.99,\n",
    "        'actor_lr': 0.001,\n",
    "        'critic_lr': 0.001,\n",
    "        'tau': 0.02,\n",
    "        'capacity': 10000,\n",
    "        'batch_size': 32,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    agent = Agent(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 :  -3970.910117875903\n",
      "1 :  -3001.3458883161006\n",
      "2 :  -3025.2261655720454\n",
      "3 :  -3179.6816078607235\n",
      "4 :  -3214.8002504733163\n",
      "5 :  -2811.8118567033425\n",
      "6 :  -2680.956813099739\n",
      "7 :  -2303.3345372199765\n",
      "8 :  -3959.862356360374\n",
      "9 :  -2545.114420703504\n",
      "10 :  -2479.175168807835\n",
      "11 :  -2435.637436696532\n",
      "12 :  -2319.5679755052865\n",
      "13 :  -2297.14164586658\n",
      "14 :  -2163.2838904836062\n",
      "15 :  -1897.6871076184445\n",
      "16 :  -2120.6505588962773\n",
      "17 :  -2139.176636241854\n",
      "18 :  -1882.2007534430124\n",
      "19 :  -1757.5266370716115\n",
      "20 :  -1636.5874586942996\n",
      "21 :  -1252.7430255808936\n",
      "22 :  -1764.2591395905768\n",
      "23 :  -1375.8823168875144\n",
      "24 :  -1378.46467999877\n",
      "25 :  -1375.846536601479\n",
      "26 :  -1245.4208984303996\n",
      "27 :  -1232.0974251695434\n",
      "28 :  -717.7921564943452\n",
      "29 :  -485.2930304098466\n",
      "30 :  -616.8954889928374\n",
      "31 :  -241.40203304260123\n",
      "32 :  -126.52738063403191\n",
      "33 :  -245.53847794700337\n",
      "34 :  -126.90372537355194\n",
      "35 :  -619.5565705003366\n",
      "36 :  -488.8260473036807\n",
      "37 :  -238.0653654883458\n",
      "38 :  -370.63672027715296\n",
      "39 :  -122.92879765625982\n",
      "40 :  -124.65844546049256\n",
      "41 :  -123.07984829731808\n",
      "42 :  -1922.3507671257394\n",
      "43 :  -773.8235087904432\n",
      "44 :  -636.0753093258124\n",
      "45 :  -241.7311100158536\n",
      "46 :  -243.3180142629058\n",
      "47 :  -611.1613818170114\n",
      "48 :  -588.6345689243174\n",
      "49 :  -244.40018112260475\n",
      "50 :  -486.30565880128637\n",
      "51 :  -237.06948927494696\n",
      "52 :  -357.83342068290466\n",
      "53 :  -237.46569120025816\n",
      "54 :  -124.29777120360201\n",
      "55 :  -487.170139806696\n",
      "56 :  -124.26421416338877\n",
      "57 :  -347.62470554780947\n",
      "58 :  -123.69187767016247\n",
      "59 :  -593.9909685851683\n",
      "60 :  -239.00520963927804\n",
      "61 :  -709.4919555105909\n",
      "62 :  -241.84306016825568\n",
      "63 :  -472.33642500154923\n",
      "64 :  -360.1577042812832\n",
      "65 :  -603.752056300843\n",
      "66 :  -349.31563903520794\n",
      "67 :  -124.91634027461076\n",
      "68 :  -236.6796537002195\n",
      "69 :  -237.18649133486016\n",
      "70 :  -619.5007008685513\n",
      "71 :  -125.8778760070121\n",
      "72 :  -489.8679243624699\n",
      "73 :  -488.00066166465854\n",
      "74 :  -365.58344663597984\n",
      "75 :  -517.7911422597007\n",
      "76 :  -356.39591180580686\n",
      "77 :  -352.1560404862336\n",
      "78 :  -125.83743975027978\n",
      "79 :  -123.65777935752403\n",
      "80 :  -505.77365983169767\n",
      "81 :  -624.7306249259068\n",
      "82 :  -243.63801846318847\n",
      "83 :  -121.7163199678968\n",
      "84 :  -497.87029959591314\n",
      "85 :  -1130.656607792954\n",
      "86 :  -667.3005308137319\n",
      "87 :  -493.7433999380554\n",
      "88 :  -126.05329286031403\n",
      "89 :  -610.5018420774529\n",
      "90 :  -353.7446759006232\n",
      "91 :  -1.522254109542643\n",
      "92 :  -241.52904252208535\n",
      "93 :  -119.32444495276546\n",
      "94 :  -119.0169522958894\n",
      "95 :  -242.22230531756796\n",
      "96 :  -122.23266772208933\n",
      "97 :  -243.16253611730093\n",
      "98 :  -124.27574702758564\n",
      "99 :  -501.31771149113035\n"
     ]
    }
   ],
   "source": [
    "    for episode in range(100):\n",
    "        s0 = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for step in range(500):\n",
    "            env.render()\n",
    "            a0 = agent.act(s0)\n",
    "            s1, r1, done, _ = env.step(a0)\n",
    "            agent.put(s0, a0, r1, s1)\n",
    "\n",
    "            episode_reward += r1\n",
    "            s0 = s1\n",
    "\n",
    "            agent.learn()\n",
    "\n",
    "        print(episode, ': ', episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}