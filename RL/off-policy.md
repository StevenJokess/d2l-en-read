# 离轨策略（off-policy）

- 强化学习的 数据往往只能通过与环境交互得出 ，这造成了其数据获取成本过大，且少之又少；
- 而书中 5.5 节前的简单直接的迭代式，只能 一边使用当前控制策略，一边改进当前控制策略（同轨策略，on-policy） ，这容易造成一些没有被探索过的方法，永远都不被尝试（固步自封），也会 导致我们无法使用之前的、别人的数据 。

同轨策略是离轨策略的一种特殊形式 ，在设计算法时， 如果其可以满足离轨策略要求，其一定可以进行同轨策略的学习。

且在实践中，我们 很难不使用 离轨策略：

- 在与环境的交互中，我们尽量不要使用当前的最优策略 （同轨策略学习方式） ，因为这样我们会“谨小慎微”，不敢做出有创意的尝试；
- 之前的数据要被复用，而之前的数据也是在不同于当前策略的策略下产生的。

## 数据采样

对于要拟合的分布 $f(x)$ ，我们想得到策略
$p(x)$ 下采样的期望，但目前只能从策略 $q(x)$ 下采样，因此做出推导:
$$
\begin{aligned}
E_{x \sim p}[f(x)] &=\int f(x) p(x) d x \\
&=\int f(x) \frac{p(x)}{q(x)} q(x) d x \\
&=E_{x \sim q}\left[f(x) \frac{p(x)}{q(x)}\right]
\end{aligned}
$$

### 重要度采样比

重要度采样比定义为：在目标策略和行动策略轨迹下的相对概率。经过推导, 即

$$
\rho_{t: T-1}=\prod_{k=t}^{T-1} \frac{\pi\left(A_{k} \mid S_{k}\right)}{b\left(A_{k} \mid S_{k}\right)}
$$

为什么这么定义这个参数, 或者说, 长成这样的公式有什么用?
我们来看其使用场景, 普通重要度采样：

$$
V(s)=\frac{\sum_{t \in \tau(s)} \rho_{t: T(t)-1} G_{t}}{|\tau(s)|}
$$

加权重要度采样:

$$
V(s)=\frac{\sum_{t \in \tau(s)} \rho_{t: T(t)-1} G_{t}}{\sum_{t \in \tau(s)} \rho_{t: T(t)-1}}
$$

其中, 二者的期望分别接近 $v_{\pi}(s)$ 与 $v_{b}(s),$ 而这个重要度采样比 在其中的作用为：当观测值贴近策略 $\pi$ 时, 此时取得的G 更加重要, 影响更大，因此其系数 重要度采样比 增大。。


当我们进行了足够多的采样后： 尽管 q(x) 下很小的概率在左侧获取数据，但一旦我们获取到，我们将通过采样率“很好地”对其进行利用。

如上图中左侧的绿点，因为 q(x) 在左侧值很小，而 p(x) 在左侧值很大，则根据采样率公式，我们给左侧的数据一个很大的权重，这样，我们便“修正了”偏差。在 足够的采样+采样率 的加持下，我们可以正确地估计出： p(x) 下采样的 f(x) 期望是某个负值。

[1]: https://mp.weixin.qq.com/s?__biz=Mzg5NjUzMTY3Mg==&mid=2247484619&idx=1&sn=92b9a803cfd06606a5bd28a617021f28&source=41#wechat_redirect
