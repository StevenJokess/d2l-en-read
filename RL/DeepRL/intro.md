

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-12-29 20:33:01
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 20:36:29
 * @Description:
 * @TODO::
 * @Reference:深度强化学习综述（上） - SIGAI的文章 - 知乎
https://zhuanlan.zhihu.com/p/48867049
-->

深度强化学习（DRL，deep reinforcement learning）是深度学习与强化学习相结合的产物，它集成了深度学习在视觉等感知问题上强大的理解能力，以及强化学习的决策能力，实现了端到端学习。

深度强化学习的出现使得强化学习技术真正走向实用，得以解决现实场景中的复杂问题。从2013年DQN（深度Q网络，deep Q network）出现到目前为止，深度强化学习领域出现了大量的算法，以及解决实际应用问题的论文。

在这篇文章中，SIGAI将对深度强化学习的算法与应用进行总结。整个综述分为上下两篇，本篇介绍强化学习的基本原理，深度强化学习的基本思想，以及基于价值函数的深度强化学习算法。下篇介绍基于策略的深度强化学习算法，基于搜索与监督的深度强化学习算法，以及深度强化学习算法的应用情况与未来的方向。


文献[13]提出了Double DQN（DDQN）算法。DDQN中有两组不同的参数，和θ和θ-。θ用于选择对应最大Q值的动作，θ-用于评估最优动作的Q值。这两组参数将动作选择和策略评估分离，降低了过高估计Q值的风险。DDQN 使用当前值网络的参数θ选择最优动作，用目标值网络的参数θ-评估该最优动作。实验结果证明，DDQN能更准确的估计Q函数值，使得训练算法和训练得到的策略更为稳定。


文献[14]提出了基于优先级采样的DQN，是对经验回放机制的改进。在之前的DQN中，经验回放通过经验池中的样本等概率随机抽样来获得每次迭代时的训练样本。显然，这没有利用每个样本的重要性。文献[14]的方法为经验池中的每个样本计算优先级，增大有价值的训练样本在采样时的概率。样本的优先级用时序差分算法的误差项进行构造，计算公式为：

这个值的绝对值越大，样本在采样时的概率越大。实验结果证明这种算法有更快的训练速度，并且在运行时有更好的效果。

文献[15]提出了基于竞争架构的 DQN。其主要改进是将CNN卷积层之后的全连接层替换为两个分支，其中一个分支拟合状态价值函数，另外一个分支拟合动作优势函数。最后将两个分支的输出值相加，形成Q函数值。实验表明，这种改进能够更准确的估计价值函数值。

DQN中的深度神经网络是卷积神经网络，不具有长时间的记忆能力。为此，文献[16]提出了一种整合了循环神经网络（RNN）的DQN算法（DRQN）。这种方法在CNN的卷积层之后加入了循环层（LSTM单元），能够记住之前的信息。
