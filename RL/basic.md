

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-08 19:50:12
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-15 13:28:21
 * @Description:
 * @TODO::
 * @Reference:
-->

强化学习( Reinforcement Learning),又称再励学习、评价学习,是机器学习的一个重要分支,传统上主要用于解决与环境交互过程中的自主决策和自动控制问题,通过不断改善智能体自身的行为,学得最优的行动策略。广义上说,任何有“决策”的任务都可以使用强化学习方法,比如无人驾驶、机器人控制、游戏竞技等,但也不限于此,比如个性化推荐算法、网络传输等非控制领域也可以使用强化学习方法。[3]

RL有3条发展主线,其中有两条主线具有重要的历史地位。一条是试错(tria|- anderror)学习,其来源于动物学习过程中的心理学,在学习过程中通过不断地尝试各种(错误或正确)行为以最终学习到最优的正确行为,即通过试错的方式去学习。该方法贯穿了人工智能领域最早的一些工作,促进了20世纪80年代RL的复兴。另一条则使用值函数( value- function)和动态规划( Dynamic Programming, DP)的方法来解决最优控制问题,在大多数情况下这条主线不涉及学习。在与现代R融合之前这两条主线彼此之间独立发展,相交甚少。虽然如此,但也有例外,即RL的第三条不太明显的发展主线——时间(序)差分( Temporal- Difference,TD)学习。20世纪80年代后期所有这三条主线汇集在一起,产生了现代RL领域。[3]

强化学习的算法非常多，大体上可以分为基于值函数的方法（包括动态规 划、时序差分学习等）、基于策略函数的方法（包括策略梯度等）以及融合两者 的方法．不同算法之间的关系如图14.4所示．

## MDP[1]

在现实环境中，从一个状态转化到下一个状态s′的概率不仅与当前状态s和动作a有关，还与之前的状态有关。如果考虑这么多的状态，会导致环境转化模型非常复杂，复杂到难以建模。马尔可夫性简化

MDP的解决方案是描述MDP中每个状态的最佳操作的策略，称为最优策略。这种最优策略可以通过各种方法找到，比如动态规划、蒙特卡洛和时序差分。

### 动态规划

动态规划（Dynamic Programming）可以用于在给定完整的环境模型作为MDP的情况下计算最优策略。动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解；二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题状态。动态规划算法基于遍历状态空间，使用全宽度的回溯机制进行状态价值的更新，也就是说，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。

### 蒙特卡洛

环境是未知的，也就是无法知道环境的状态转化模型P，这时动态规划法就无法使用。蒙特卡洛（Monte-Carlo，MC）方法不需要对环境进行完全建模，只需要经验，也就是实际或者仿真的与环境进行交互的整个样本序列，包括状态动作和反馈信息。所谓的整个样本序列，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，就可以近似估计状态价值，进而求解预测和控制问题了。

### 时序差分

时序差分算法（Temporal-Difference，TD）也是一种model-free的强化学习算法。它继承了动态规划和蒙特卡洛方法的优点。在用蒙特卡洛法求解中，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果没有完整的状态序列，那么就无法使用蒙特卡洛法求解了。这里就来讨论不使用完整状态序列求解强化学习问题的时序差分法。时序差分法的单步更新是在环境中每走一步，更新一次状态价值。如果每走n步更新一次状态价值，就叫n步时序差分。当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡洛法了。

## 基本模型

### 基于价值、值函数（value-based）的方法，

所谓基于价值的方法，就是先评估每个动作的Q值（Value），再根据Q值求最优策略的方法。强化学习的最终目标是求解策略，因此基于价值的方法是一种“曲线救国”。基于价值的强化学习策略（比如[插图]-贪婪法）是不变的，即在某个状态、选择哪种行动是固定的。基于价值的强化学习以通过潜在奖励计算出的动作回报期望作为选取动作的依据。代表算法有SARSA和Q-Learning。

#### SARSA算法

SARSA算法其实是由[插图]几个字母组成的，S代表状态（State）、A代表动作（Action）、R代表奖励（Reward）。简单理解SARSA算法就是使用[插图]-贪婪法来更新价值函数和选择新的动作。流程如图9-4所示。[插图]图9-4 SARSA算法示意图首先基于[插图]-贪婪法在当前状态s选择一个动作a，这时系统会转到一个新的状态s′，同时反馈一个即时奖励r，在新的状态s′，基于[插图]-贪婪法选择一个新的动作a′，但是并不执行，而是通过这个动作更新价值函数，价值函数的更新公式是：

[插图]其中，γ是衰减因子，α是迭代步长即学习率，收获Gt的表达式是[插图]。

#### Q-Learning

Q-Learning是时序差分法的离线控制算法，会使用两个控制策略，首先使用[插图]-贪婪法来选择新的动作，然后使用贪婪法进行价值更新。Q-Learning算法的流程如图9-5所示。[插图]图9-5 Q-Learning流程图首先基于状态s，用-贪婪法选择动作a，得到奖励r并进入到状态s′。然后基于状态s′，使用贪婪法选择a′，也就是说选择使[插图]最大的a作为a′来更新价值函数，用公式表示如下：[插图]此时选择的动作只会参与价值函数的更新，不会真正地执行。价值函数更新后，新的执行动作需要基于状态s′，用[插图]-贪婪法重新选择得到。


### 基于策略（policy-based）的方法。

除了本章中介绍的标准强化学习问题之外，还存在一些更加泛化的强化学 习问题． 部分可观测马尔可夫决策过程 部分可观测马尔可夫决策过程（Partially Observable MarkovDecision Processes，POMDP）是一个马尔可夫决策过程的泛 化．POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态𝑠，只 能知道部分观测值𝑜．比如在自动驾驶中，智能体只能感知传感器采集的有限的 环境信息． POMDP可以用一个7元组描述：(𝒮,𝒜,𝑇,𝑅,Ω,𝒪,𝛾)，其中𝒮表示状态空间，为隐变量 ，𝒜为动作空间，𝑇(𝑠′|𝑠,𝑎)为状态转移概率， 𝑅为奖励函数，Ω(𝑜|𝑠,𝑎)为观测概率 ， 𝒪为观测空间， 𝛾为折扣系数．逆向强化学习 强化学习的基础是智能体可以和环境进行交互，得到奖励．但 在某些情况下，智能体无法从环境得到奖励，只有一组轨迹示例（Demonstration）．比如在自动驾驶中，我们可以得到司机的一组轨迹数据，但并不知道司机 在每个时刻得到的即时奖励．虽然我们可以用监督学习来解决，称为行为克隆． 但行为克隆只是学习司机的行为，并没有深究司机行为的动机． 逆向强化学习（InverseReinforcementLearning，IRL）就是指一个不带奖 励的马尔可夫决策过程，通过给定的一组专家（或教师）的行为轨迹示例来逆向 估计出奖励函数𝑟(𝑠,𝑎,𝑠′)来解释专家的行为，然后再进行强化学习． 分层强化学习 分层强化学习（Hierarchical Reinforcement Learning，HRL） 是指将一个复杂的强化学习问题分解成多个小的、简单的子问题[Barto et al., 2003]，每个子问题都可以单独用马尔可夫决策过程来建模．这样，我们可以将智 能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何 执行低层次策略．这样，智能体就可以解决一些非常复杂的任务．[2]

### 基于模型（model-based）的方法
### 基于分层（hierarchical-based）的方法

[1]: https://weread.qq.com/web/reader/62332d007190b92f62371aek92c3210025c92cc22753209
[2]: https://nndl.github.io/ 14.5
[3]: https://www.hzmedia.com.cn/w/reader.aspx?id=378872d4-69a3-4208-958a-4bc3c48e0287_1
