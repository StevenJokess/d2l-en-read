

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-08 19:50:12
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-08 19:57:56
 * @Description:
 * @TODO::
 * @Reference:
-->

## MDP[1]

在现实环境中，从一个状态转化到下一个状态s′的概率不仅与当前状态s和动作a有关，还与之前的状态有关。如果考虑这么多的状态，会导致环境转化模型非常复杂，复杂到难以建模。马尔可夫性简化

MDP的解决方案是描述MDP中每个状态的最佳操作的策略，称为最优策略。这种最优策略可以通过各种方法找到，比如动态规划、蒙特卡洛和时序差分。

### 动态规划

动态规划（Dynamic Programming）可以用于在给定完整的环境模型作为MDP的情况下计算最优策略。动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解；二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题状态。动态规划算法基于遍历状态空间，使用全宽度的回溯机制进行状态价值的更新，也就是说，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。

### 蒙特卡洛

环境是未知的，也就是无法知道环境的状态转化模型P，这时动态规划法就无法使用。蒙特卡洛（Monte-Carlo，MC）方法不需要对环境进行完全建模，只需要经验，也就是实际或者仿真的与环境进行交互的整个样本序列，包括状态动作和反馈信息。所谓的整个样本序列，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，就可以近似估计状态价值，进而求解预测和控制问题了。

### 时序差分

时序差分算法（Temporal-Difference，TD）也是一种model-free的强化学习算法。它继承了动态规划和蒙特卡洛方法的优点。在用蒙特卡洛法求解中，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果没有完整的状态序列，那么就无法使用蒙特卡洛法求解了。这里就来讨论不使用完整状态序列求解强化学习问题的时序差分法。时序差分法的单步更新是在环境中每走一步，更新一次状态价值。如果每走n步更新一次状态价值，就叫n步时序差分。当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡洛法了。

## 基本模型

### 基于价值（value-based）的方法，

所谓基于价值的方法，就是先评估每个动作的Q值（Value），再根据Q值求最优策略的方法。强化学习的最终目标是求解策略，因此基于价值的方法是一种“曲线救国”。基于价值的强化学习策略（比如[插图]-贪婪法）是不变的，即在某个状态、选择哪种行动是固定的。基于价值的强化学习以通过潜在奖励计算出的动作回报期望作为选取动作的依据。代表算法有SARSA和Q-Learning。

#### SARSA算法

SARSA算法其实是由[插图]几个字母组成的，S代表状态（State）、A代表动作（Action）、R代表奖励（Reward）。简单理解SARSA算法就是使用[插图]-贪婪法来更新价值函数和选择新的动作。流程如图9-4所示。[插图]图9-4 SARSA算法示意图首先基于[插图]-贪婪法在当前状态s选择一个动作a，这时系统会转到一个新的状态s′，同时反馈一个即时奖励r，在新的状态s′，基于[插图]-贪婪法选择一个新的动作a′，但是并不执行，而是通过这个动作更新价值函数，价值函数的更新公式是：

[插图]其中，γ是衰减因子，α是迭代步长即学习率，收获Gt的表达式是[插图]。

#### Q-Learning

Q-Learning是时序差分法的离线控制算法，会使用两个控制策略，首先使用[插图]-贪婪法来选择新的动作，然后使用贪婪法进行价值更新。Q-Learning算法的流程如图9-5所示。[插图]图9-5 Q-Learning流程图首先基于状态s，用-贪婪法选择动作a，得到奖励r并进入到状态s′。然后基于状态s′，使用贪婪法选择a′，也就是说选择使[插图]最大的a作为a′来更新价值函数，用公式表示如下：[插图]此时选择的动作只会参与价值函数的更新，不会真正地执行。价值函数更新后，新的执行动作需要基于状态s′，用[插图]-贪婪法重新选择得到。


### 基于策略（policy-based）的方法。

[1]: https://weread.qq.com/web/reader/62332d007190b92f62371aek92c3210025c92cc22753209
