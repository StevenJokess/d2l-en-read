

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-08 19:50:12
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-30 19:48:39
 * @Description:
 * @TODO::
 * @Reference:
-->

强化学习( Reinforcement Learning),又称再励学习、评价学习,是机器学习的一个重要分支,传统上主要用于解决与环境交互过程中的自主决策和自动控制问题,通过不断改善智能体自身的行为,学得最优的行动策略。广义上说,任何有“决策”的任务都可以使用强化学习方法,比如无人驾驶、机器人控制、游戏竞技等,但也不限于此,比如个性化推荐算法、网络传输等非控制领域也可以使用强化学习方法。[3]

在连接主义学习（源于仿生学）中，把强化学习（Reinforcement learning） 作为与以上三类方法并列的一类机器学习方法，个人认为可以把强化学习看成是一种通过环境内部产生样本（特征和标签的匹配对）的监督学习。[4]

RL有3条发展主线,其中有两条主线具有重要的历史地位。一条是试错(tria|- anderror)学习,其来源于动物学习过程中的心理学,在学习过程中通过不断地尝试各种(错误或正确)行为以最终学习到最优的正确行为,即通过试错的方式去学习。该方法贯穿了人工智能领域最早的一些工作,促进了20世纪80年代RL的复兴。另一条则使用值函数( value- function)和动态规划( Dynamic Programming, DP)的方法来解决最优控制问题,在大多数情况下这条主线不涉及学习。在与现代R融合之前这两条主线彼此之间独立发展,相交甚少。虽然如此,但也有例外,即RL的第三条不太明显的发展主线——时间(序)差分( Temporal- Difference,TD)学习。20世纪80年代后期所有这三条主线汇集在一起,产生了现代RL领域。[3]

强化学习的算法非常多，大体上可以分为基于值函数的方法（包括动态规 划、时序差分学习等）、基于策略函数的方法（包括策略梯度等）以及融合两者 的方法．不同算法之间的关系如图14.4所示．

## MDP[1]

在现实环境中，从一个状态转化到下一个状态s′的概率不仅与当前状态s和动作a有关，还与之前的状态有关。如果考虑这么多的状态，会导致环境转化模型非常复杂，复杂到难以建模。马尔可夫性简化

MDP的解决方案是描述MDP中每个状态的最佳操作的策略，称为最优策略。这种最优策略可以通过各种方法找到，比如动态规划、蒙特卡洛和时序差分。

### 动态规划

动态规划（Dynamic Programming）可以用于在给定完整的环境模型作为MDP的情况下计算最优策略。动态规划的关键点有两个：一是问题的最优解可以由若干小问题的最优解构成，即通过寻找子问题的最优解来得到问题的最优解；二是可以找到子问题状态之间的递推关系，通过较小的子问题状态递推出较大的子问题状态。动态规划算法基于遍历状态空间，使用全宽度的回溯机制进行状态价值的更新，也就是说，在每一次回溯更新某一个状态的价值时，都要回溯到该状态的所有可能的后续状态，并利用贝尔曼方程更新该状态的价值。这种全宽度的价值更新方式对于状态数较少的强化学习问题还是比较有效的，但是当问题规模很大的时候，动态规划算法将会因贝尔曼维度灾难而无法使用。

### 蒙特卡洛

环境是未知的，也就是无法知道环境的状态转化模型P，这时动态规划法就无法使用。蒙特卡洛（Monte-Carlo，MC）方法不需要对环境进行完全建模，只需要经验，也就是实际或者仿真的与环境进行交互的整个样本序列，包括状态动作和反馈信息。所谓的整个样本序列，就是这个序列必须是达到终点的。比如下棋问题分出输赢，驾车问题成功到达终点或者失败。有了很多组这样经历完整的状态序列，就可以近似估计状态价值，进而求解预测和控制问题了。

### 时序差分

时序差分算法（Temporal-Difference，TD）也是一种model-free的强化学习算法。它继承了动态规划和蒙特卡洛方法的优点。在用蒙特卡洛法求解中，不需要环境的状态转化概率模型，但是它需要所有的采样序列都是经历完整的状态序列。如果没有完整的状态序列，那么就无法使用蒙特卡洛法求解了。这里就来讨论不使用完整状态序列求解强化学习问题的时序差分法。时序差分法的单步更新是在环境中每走一步，更新一次状态价值。如果每走n步更新一次状态价值，就叫n步时序差分。当n越来越大，趋于无穷，或者说趋于使用完整的状态序列时，n步时序差分就等价于蒙特卡洛法了。

## 基本模型

### 基于价值、值函数（value-based）的方法，

所谓基于价值的方法，就是先评估每个动作的Q值（Value），再根据Q值求最优策略的方法。强化学习的最终目标是求解策略，因此基于价值的方法是一种“曲线救国”。基于价值的强化学习策略（比如[插图]-贪婪法）是不变的，即在某个状态、选择哪种行动是固定的。基于价值的强化学习以通过潜在奖励计算出的动作回报期望作为选取动作的依据。代表算法有SARSA和Q-Learning。

#### SARSA算法

SARSA算法其实是由[插图]几个字母组成的，S代表状态（State）、A代表动作（Action）、R代表奖励（Reward）。简单理解SARSA算法就是使用[插图]-贪婪法来更新价值函数和选择新的动作。流程如图9-4所示。[插图]图9-4 SARSA算法示意图首先基于[插图]-贪婪法在当前状态s选择一个动作a，这时系统会转到一个新的状态s′，同时反馈一个即时奖励r，在新的状态s′，基于[插图]-贪婪法选择一个新的动作a′，但是并不执行，而是通过这个动作更新价值函数，价值函数的更新公式是：

[插图]其中，γ是衰减因子，α是迭代步长即学习率，收获Gt的表达式是[插图]。

#### Q-Learning

Q-Learning是时序差分法的离线控制算法，会使用两个控制策略，首先使用[插图]-贪婪法来选择新的动作，然后使用贪婪法进行价值更新。Q-Learning算法的流程如图9-5所示。[插图]图9-5 Q-Learning流程图首先基于状态s，用-贪婪法选择动作a，得到奖励r并进入到状态s′。然后基于状态s′，使用贪婪法选择a′，也就是说选择使[插图]最大的a作为a′来更新价值函数，用公式表示如下：[插图]此时选择的动作只会参与价值函数的更新，不会真正地执行。价值函数更新后，新的执行动作需要基于状态s′，用[插图]-贪婪法重新选择得到。


### 基于策略（policy-based）的方法。

除了本章中介绍的标准强化学习问题之外，还存在一些更加泛化的强化学 习问题． 部分可观测马尔可夫决策过程 部分可观测马尔可夫决策过程（Partially Observable MarkovDecision Processes，POMDP）是一个马尔可夫决策过程的泛 化．POMDP依然具有马尔可夫性质，但是假设智能体无法感知环境的状态𝑠，只 能知道部分观测值𝑜．比如在自动驾驶中，智能体只能感知传感器采集的有限的 环境信息． POMDP可以用一个7元组描述：(𝒮,𝒜,𝑇,𝑅,Ω,𝒪,𝛾)，其中𝒮表示状态空间，为隐变量 ，𝒜为动作空间，𝑇(𝑠′|𝑠,𝑎)为状态转移概率， 𝑅为奖励函数，Ω(𝑜|𝑠,𝑎)为观测概率 ， 𝒪为观测空间， 𝛾为折扣系数．逆向强化学习 强化学习的基础是智能体可以和环境进行交互，得到奖励．但 在某些情况下，智能体无法从环境得到奖励，只有一组轨迹示例（Demonstration）．比如在自动驾驶中，我们可以得到司机的一组轨迹数据，但并不知道司机 在每个时刻得到的即时奖励．虽然我们可以用监督学习来解决，称为行为克隆． 但行为克隆只是学习司机的行为，并没有深究司机行为的动机． 逆向强化学习（InverseReinforcementLearning，IRL）就是指一个不带奖 励的马尔可夫决策过程，通过给定的一组专家（或教师）的行为轨迹示例来逆向 估计出奖励函数𝑟(𝑠,𝑎,𝑠′)来解释专家的行为，然后再进行强化学习． 分层强化学习 分层强化学习（Hierarchical Reinforcement Learning，HRL） 是指将一个复杂的强化学习问题分解成多个小的、简单的子问题[Barto et al., 2003]，每个子问题都可以单独用马尔可夫决策过程来建模．这样，我们可以将智 能体的策略分为高层次策略和低层次策略，高层次策略根据当前状态决定如何 执行低层次策略．这样，智能体就可以解决一些非常复杂的任务．[2]


based on if there is a model

### 基于模型（model-based）的方法[3]

基于模型的RL算法是智能体通过与环境交互获得数据,并根据数据对环境进行建模拟合岀一个模型,然后智能体根据模型生成样本并利用RL算法优化自身。在这过程中,智能体将数据迸行了充分利用,因为模型一旦拟出来,智能体就可以根据该模型来生成样本。因此智能体与环境之间的交互次数会急剧減少,但拟合的模型往往存在偏差,因此基于模型的R算法一般不能保证收敛到最优解。

对环境有提前的认知，可以提前考虑规划，但是缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。

因而,为了减少与环境的交互次数,需要使用环境模型对训练过程进行加速。这样做的好处如下整体的更新方法仍为离线更新,降低了陷入局部最优解的风险。
使用在线样本对环境模型更新,确保了环境模型模拟的时效性。
使用环境模型进行加速,大大减少了智能体与环境的交互次数,加快了训练速度,降低了训练成本

## Model-free
Explicit: value function and/or policy function
No model

Recht 指出免模型方法自身存在以下几大缺陷：[5]

免模型方法无法从不带反馈信号的样本中学习，而反馈本身就是稀疏的，因此免模型方向样本利用率很低，而数据驱动的方法则需要大量采样。比如在 Atari 平台上的《Space Invader》和《Seaquest》游戏中，智能体所获得的分数会随训练数据增加而增加。利用免模型 DRL 方法可能需要 2 亿帧画面才能学到比较好的效果。AlphaGo 最早在 Nature 公布的版本也需要 3000 万个盘面进行训练。而但凡与机械控制相关的问题，训练数据远不如视频图像这样的数据容易获取，因此只能在模拟器中进行训练。而模拟器与现实世界间的 Reality Gap，直接限制了训练自其中算法的泛化性能。另外，数据的稀缺性也影响了其与 DL 技术的结合。

免模型方法不对具体问题进行建模，而是尝试用一个通用的算法解决所有问题。而基于模型的方法则通过针对特定问题建立模型，充分利用了问题固有的信息。免模型方法在追求通用性的同时放弃这些富有价值的信息。

基于模型的方法针对问题建立动力学模型，这个模型具有解释性。而免模型方法因为没有模型，解释性不强，调试困难；
相比基于模型的方法，尤其是基于简单线性模型的方法，免模型方法不够稳定，在训练中极易发散。

为了证实以上观点，Recht 将一个简单的基于 LQR 的随机搜索方法与最好的免模型方法在 MuJoCo 实验环境上进行了实验对比。在采样率相近的情况下，基于模型的随机搜索算法的计算效率至少比免模型方法高 15 倍 [19]。

### 基于分层（hierarchical-based）的方法[3]

在反馈稀疏的环境中学习目标导向的行为是RL算法的主要挑战。其中一个主要困难是探索不足,导致智能体无法学习鲁棒的策略。具有內在动机的智能体可以根据自己的利益探索新的行为,而不是直接解决外部目标。这种内在行为最终可以帮助智能体解决环境带来的任务。

分层DQN( hierarchy DQN, h-DQN)是一个整合分层 actor- critICl函数的框架1,可以在不同的时间尺度上运作,具有以目标驱动为内在动机的DRL。顶级Q值函数学习內在目标的策略,而较低级别的函数学习策略来执行原子级别的动作以满足给定目标。在目标空间的探索可以有效地处理稀疏奖劢和延迟奖劢问题。此外,在实体和关系空间中表达的目标可以限制探索空间,以便在复杂环境中进行数据有效的DRL。

RL将控制问题转化为找到最大化预期未来奖励的策略π。值函数∨(S)是RL的核心,它可以表示任何状态在实现智能体总体目标方面的效用。值函数也可被推广为V(Sg),以表示状态s对于实现给定目标g∈G的效用。当环境提供延迟奖励时,策略首先学习实现內在生成目标,然后学习将它们链接在一起的最佳策略。每个值函数∨(S,g)可生成在智能体达到目标状态g时终止的策略。这些策略的集合可以在半MDP的框架内以时间动态分层排列来学习或规划。在高维问题中,这些值函数可以通过神经网络近似为V(sg;()。


[1]: https://weread.qq.com/web/reader/62332d007190b92f62371aek92c3210025c92cc22753209
[2]: https://nndl.github.io/ 14.5
[3]: https://www.hzmedia.com.cn/w/reader.aspx?id=378872d4-69a3-4208-958a-4bc3c48e0287_1
[4]: https://kangcai.github.io/2019/02/09/ml-supervised-1-intro/
[5]: https://mp.weixin.qq.com/s/qHFeRS1xpztV8AGtLVk8Cg
