

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-12-29 20:28:18
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 20:29:14
 * @Description:
 * @TODO::
 * @Reference:https://rl.qiwihui.com/zh_CN/latest/chapter1/introduction.html#id4
-->

# 早期历史

强化学习的早期历史有两个主线，悠久和丰富，都是在现代强化学习交织之前独立进行的。

1. 一个主线涉及通过试错试验来学习，并且起源于动物学习的心理学。这个主线贯穿了人工智能领域的一些最早的工作，并导致了20世纪80年代早期强化学习的复兴。
2. 第二个主线涉及使用值函数和动态规划的最优控制问题及其解决方案。在大多数情况下，这个主线不涉及学习。
3. 这两个主线大多是独立的，但在某种程度上相互关联，围绕着时序差分方法的第三个不那么明显的线索，例如本章中的井字棋示例中使用的那些。 所有这三个主线在20世纪80年代后期汇集在一起，产生了们在本书中提到的现代强化学习领域。

专注于试错学习的主线是我们最熟悉的，也是我们在这个简短的历史中最可以说的。然而，在此之前，我们将简要讨论最优控制主线。


## 最优控制

动态规划被广泛认为是解决一般随机最优控制问题的唯一可行方法。它取决于贝尔曼所说的“维度的诅咒”， 意味着它的计算需求随着状态变量的数量呈指数增长，但它仍然比任何其他通用方法更有效，更广泛适用。 动态规划自20世纪50年代后期以来得到了广泛的发展，包括对部分可观察的MDP的扩展（Lovejoy，1991年调查）， 许多应用（White，1985,1988,1993），近似方法（由Rust调查，1996）和异步方法（Bertsekas，1982,1983）。 有许多可行的优秀的动态编程现代处理方法 （例如，Bertsekas，2005年，2012年；Puterman，1994; Ross，1983；以及Whittle，1982，1983）。 Bryson（1996）提供了最优控制的权威历史。

最优控制和动态规划之间的联系，以及另一方面的学习，很难被认识到。 我们无法确定这种分离的原因，但其主要原因可能是所涉及的学科与其不同目标之间的分离。 作为一种离线计算，动态规划的普遍观点也可能主要取决于准确的系统模型和Bellman方程的解析解。 此外，最简单的动态规划形式是一种在时间上倒退的计算，使得很难看到它如何参与必须在前进方向上进行的学习过程。 动态规划中的一些最早的工作，例如Bellman和Dreyfus（1959）的工作，现在可能被归类为遵循学习方法。 Witten（1977）的工作（下面讨论）当然有资格作为学习和动态规划思想的组合。 Werbos（1987）明确提出动态规划和学习方法的相互关系，以及动态规划与理解神经和认知机制的相关性。 对于我们来说，动态规划方法与在线学习的完全整合直到1989年Chris Watkins的工作才出现，他们使用MDP形式主义对强化学习的处理已被广泛采用。 从那以后，这些关系得到了许多研究人员的广泛发展，特别是Dimitri Bertsekas和John Tsitsiklis（1996）， 他们创造了术语“神经动力学规划”来指代动态规划和人工神经网络的结合。 目前使用的另一个术语是“近似动态规划”。这些不同的方法强调了主题的不同方面，但它们都与强化学习有共同的兴趣来规避动态规划的经典缺点。

从某种意义上说，我们认为所有最优控制工作都应用于强化学习。 我们将强化学习方法定义为解决强化学习问题的任何有效方法，现在很清楚这些问题与最优控制问题密切相关，尤其是随机最优控制问题，例如那些被称为MDP的问题。 因此，我们必须考虑最优控制的解决方法，如动态规划，也是强化学习方法。 因为几乎所有传统方法都需要完全掌握要控制的系统，所以说它们是强化学习的一部分感觉有点不自然。 另一方面，许多动态规划算法是递增的和迭代的。与学习方法一样，他们通过连续的近似逐渐达到正确的答案。 正如我们在本书其余部分所展示的那样，这些相似之处远非肤浅。 完整和不完整知识案例的理论和解决方法是如此密切相关，以至于我们认为必须将它们视为同一主题的一部分。

术语“最优控制”在20世纪50年代后期开始使用，用于描述设计控制器以最小化或最大化动态系统随时间变化的行为的问题。 解决这个问题的方法之一是由理查德·贝尔曼（Richard Bellman）和其他人在20世纪50年代中期 通过扩展19世纪汉密尔顿（Hamilton）和雅可比（Jacobi）理论而发展起来的。 该方法使用动态系统的状态和值函数或“最优返回函数”的概念来定义函数方程，现在通常称为Bellman方程。 通过求解该方程来解决最优控制问题的方法被称为动态规划（Bellman，1957a）。 Bellman（1957b）还引入了称为马尔可夫决策过程（MDPs）的最优控制问题的离散随机版本。 罗纳德霍华德（Ronald Howard，1960）设计了MDP的策略迭代方法。所有这些都是现代强化学习理论和算法的基本要素。

## 试错试验


现在让我们回到导向现代强化学习领域的另一个主要思路，该思路的核心是试错学习的思想。 我们只涉及这里的主要联系点，在第14.3节中更详细地讨论了这个主题。 根据美国心理学家R. S. Woodworth（1938）的说法，试验和错误学习的概念可以追溯到19世纪50年代， 亚历山大·贝恩（Alexander Bain）通过“摸索和实验”讨论学习， 更明确地和英国伦理学家和心理学家Conway Lloyd Morgan的1894年使用该术语来描述他对动物行为的观察进行讨论。 也许第一个简洁地表达试错学习作为学习原则的本质是Edward Thorndike：

在对同一情况作出的若干回应中，那些伴随或密切关注对动物满意的东西，在其他条件相同的情况下，与情况更紧密地联系在一起， 因此，当它再次发生时，它们将更有可能复发; 那些伴随或紧随动物不适的人，在其他条件相同的情况下，会与这种情况的关系减弱， 因此，当它再次出现时，它们不太可能发生。满意度或不适感越大，粘合剂的强化或弱化程度越大。（Thorndike，1911年，第244页）
Thorndike称之为“效果定律（Law of Effect）”，因为它描述了强化事件对选择行为倾向的影响。 Thorndike后来对定律进行了修改，以更好地考虑后续的动物学习数据（例如奖励和惩罚的影响之间的差异）， 各种形式的定律在学习理论家中产生了相当大的争议 （例如，见Gallistel，2005；Herrnstein ，1970；Kimble，1961,1967；Mazur，1994）。 尽管如此，效果定律以某种形式被广泛认为是基本原则的基本原则 （例如，Hilgard和Bower，1975；Dennett，1978; Campbell，1960；Cziko，1995）。 它是Clark Hull（1943年，1952年）有影响力的学习理论和B. F. Skinner（1938）的有影响力的实验方法的基础。

在动物学习背景下，“强化”这个术语在Thorndike表达效力定律后得到了很好的应用， 在1927年巴甫洛夫关于条件反射的专着的英文译本中，首先出现在这种背景下（据我们所知）。 巴甫洛夫将强化描述为由于动物接受刺激 - 一种强化剂 - 与另一种刺激或反应有适当的时间关系而加强行为模式。 一些心理学家将强化的观点扩展到包括削弱和加强行为，并扩展强化者的想法，包括可能忽略或终止刺激。 要被认为是增强剂，强化或弱化必须在强化剂被撤回后持续存在；仅仅吸引动物注意力或刺激其行为而不产生持久变化的刺激物不会被视为强化物。

在计算机中实现试错试验的想法似乎是关于人工智能可能性的最早想法。 在1948年的一份报告中，艾伦·图灵（Alan Turing）描述了一种“快乐 - 痛苦系统”的设计，该系统符合效果法则：

当达到未确定动作的配置时，对缺失数据进行随机选择，并暂时在该描述中进行适当的输入并应用。 当疼痛刺激发生时，所有暂定条目都被取消，当快乐刺激发生时，它们都是永久性的。（图灵，1948年）
我们构建了许多巧妙的机电机器以演示试错试验。最早的可能是由托马斯罗斯（Thomas Ross，1933）建造的机器， 它能够通过一个简单的迷宫找到它的路，并记住通过开关设置的路径。 1951年，W. Gray Walter建立了他的“机械乌龟”（Walter，1950）的一个版本，能够进行简单的学习。 1952年，Claude Shannon展示了一只名为Theseus的迷宫老鼠，它使用试错试验通过迷宫找到解决方式， 迷宫本身通过在其地板下的磁铁和继电器记住了成功方向（参见Shannon，1951）。 J. A. Deutsch（1954）描述了一种基于他的行为理论的迷宫解决机器（Deutsch，1953）， 它具有与基于模型的强化学习相同的一些性质（第8章）。 在他的博士学位博士论文Marvin Minsky（1954）讨论了强化学习的计算模型，并描述了他的模拟机器的构造， 该模拟机器由他称为SNARC（随机神经 - 模拟增强计算器）的组件组成，其意图类似于大脑中可修改的突触连接（第15章）。 网站 cyberneticzoo.com 包含有关这些和许多其他机电学习机器的大量信息。

建立机电学习机器让位于编程数字计算机以执行各种类型的学习，其中一些学习实现了试错试验。 Farley和Clark（1954）描述了通过试错试验学习的神经网络学习机的数字模拟。 但他们的兴趣很快就从试错学习转向泛化和模式识别，即从强化学习到监督学习（Clark and Farley，1955）。 这开始了对这些类型学习之间关系的混淆模式。许多研究人员似乎相信他们正在研究强化学习，实际他们却在学习监督学习。 例如，Rosenblatt（1962）和Widrow和Hoff（1960）等人工神经网络先驱显然受到强化学习的驱使， 他们使用了奖励和惩罚的语言，但他们研究的系统是适用于模式识别和感知学习（perceptual learning）的监督学习系统。 即便在今天，一些研究人员和教科书也最大限度地减少或模糊了这些学习类型之间的区别， 例如，一些人工神经网络教科书使用术语“试错试验”来描述从训练样本中学习的网络。 这是一个可以理解的混淆，因为这些网络使用错误信息来更新连接权重，但这忽略了试错学习的基本特征， 即在评估反馈的基础上选择行动，而不依赖于知道什么是正确行动。

部分因为这些混淆，对真正的试错学习的研究在20世纪60年代和70年代变得罕见，尽管有明显的例外。 在20世纪60年代，工程文献中首次使用术语“强化”和“强化学习”来描述试错学习的工程用途 （例如，Waltz和Fu，1965； Mendel，1966； Fu，1970；Mendel and McClaren，1970年）。 特别有影响力的是明斯基的论文“迈向人工智能的步骤”（Minsky，1961），该论文讨论了与试错学习相关的几个问题， 包括预测，期望以及他称之为 复杂加强的基本信用分配问题学习系统：你如何在许多可能参与制定它的决策中分配成功的信用？ 在某种意义上，我们在本书中讨论的所有方法都是针对解决这个问题的。Minsky的论文今天非常值得一读。

在接下来的几段中，我们将讨论在20世纪60年代和70年代相对忽略对真正的试错学习的计算和理论研究的一些例外和部分例外。

新西兰研究员John Andreae的工作是一个例外，他开发了一种名为STeLLA的系统，该系统通过与环境交互的试错试验来学习。 这个系统包括一个世界的内部模型，后来是一个处理隐藏状态问题的“内部独白”（Andreae，1963,1969a，b）。 Andreae后来的工作（1977）更加强调从老师那里学习，但仍然包括通过试错试验来学习，新一代事件的产生是系统的目标之一。 这项工作的一个特点是“泄漏过程”，在Andreae（1998）中进行了更全面的阐述，实现了类似于我们描述的更新操作的信用分配机制。 不幸的是，他的开创性研究并不为人所熟知，并且对随后的强化学习研究没有太大影响。最近的摘要是可用的（Andreae，2017a，b）。

更有影响力的是Donald Michie的作品。在1961年和1963年，他描述了一个简单的试错学习系统， 用于学习如何玩叫做MENACE（Matchbox Educable Naughts和Crosses Engine）的井字棋（或者naughts和十字架）。 它由每个可能的游戏位置的火柴盒组成，每个火柴盒包含许多彩色珠子，每个可能的移动位置都有不同的颜色。 通过从对应于当前游戏位置的火柴盒中随机抽取珠子，可以确定MENACE的移动。 当游戏结束时，在游戏过程中使用的盒子中添加或删除珠子以奖励或惩罚MENACE的决定。 Michie和Chambers（1968）描述了另一种名为GLEE（游戏学习预测引擎）的强大的井字棋强化学习器和一种名为BOXES的强化学习控制器。 他们将BOXES应用于学习根据仅在杆下落或推车到达轨道末端时发生的故障信号来平衡铰接到可移动推车的杆。 这项任务改编自Widrow和Smith（1964）的早期工作，他使用监督学习方法，假设教师的指导已经能够平衡极点。 Michie和Chambers的杆极平衡版本是在不完全知识条件下强化学习任务的最佳早期例子之一。 它影响了后来的强化学习工作，从我们自己的一些研究开始（Barto，Sutton和Anderson，1983；Sutton，1984）。 Michie一直强调试验和错误以及学习作为人工智能的重要方面的作用（Michie，1974）。

Widrow，Gupta和Maitra（1973）修改了Widrow和Hoff（1960）的最小均方（LMS）算法，以产生一个强化学习规则， 可以从成功和失败信号中学习而不是从训练样例中学习。他们将这种形式称为“选择性自适应适应”， 并将其描述为“与评论家一起学习”，而不是“与老师一起学习”。他们分析了这一规则并展示了它如何学习玩二十一点。 这是Widrow对强化学习的孤立尝试，他对监督学习的贡献更具影响力。 我们对“评论家”一词的使用来源于Widrow，Gupta和Maitra的论文。 Buchanan, Mitchell, Smith, and Johnson（1978）在机器学习的背景下独立使用了术语评论家（ 参见Dietterich和Buchanan，1984），但对于他们来说，评论家是一个专家系统，能够做的不仅仅是评估绩效。

学习自动机 的研究对导致现代强化学习研究的试错主线有更直接的影响。 这些是解决非联想性，纯粹选择性学习问题的方法，称为 k型武装强盗，类似于赌博机，或有k杠杆的“单臂强盗”（见第2章）。 学习自动机是简单的低内存机器，用于提高这些问题的奖励概率。 学习自动机起源于20世纪60年代俄罗斯数学家和物理学家M. L. Tsetlin及其同事（于1973年在Tsetlin出版，后期出版）的工作， 并从那时起在工程中得到了广泛的发展（见Narendra和Thathachar，1974,1989）。 这些发展包括随机学习自动机的研究，这是基于奖励信号更新动作概率的方法。 虽然没有在随机学习自动机的传统中发展，但Harth和Tzanakou（1974）的Alopex算法（用于模式提取算法）是一种用于检测行为和强化之间相关性的随机方法， 这些方法影响了我们早期的一些研究（Barto，Sutton和Brouwer，1981）。 早期的心理学研究预示着随机学习自动机，首先是威廉·埃斯特斯（William Estes）（1950）对统计学习理论的研究， 并由其他人进一步发展（例如，Bush和Mosteller，1955；Sternberg，1963）。

经济学研究人员采用了心理学中发展起来的统计学习理论，从而在该领域致力于强化学习。 这项工作始于1973年，将Bush and Mosteller的学习理论应用于一系列经典经济模型（Cross，1973）。 这项研究的一个目标是研究人工个体，其行为更像真实的人，而不是传统的理想经济个体（Arthur，1991）。 这种方法扩展到了博弈论背景下强化学习的研究。 经济学中的强化学习在很大程度上独立于人工智能强化学习的早期工作， 尽管博弈论仍然是这两个领域的一个主题（超出了本书的范围）。 Camerer（2011）讨论了经济学中的强化学习传统， Now ́e，Vrancx和De Hauwere（2012）从我们在本书中介绍的方法的多个体扩展的角度提供了该主题的概述。 在游戏理论的背景下强化是一个非常不同的主题，而不是强化学习在程序中用于玩井字棋，跳棋和其他娱乐游戏。 例如，参见Szita（2012）对强化学习和游戏这一方面的概述。

John Holland（1975）概述了基于选择原则的自适应系统的一般理论。 他的早期工作主要以非关联形式进行试验和错误，如进化方法和k个武装强盗。 1976年，更完全地在1986年，他引入了 分类器系统，这是真正的强化学习系统，包括关联和价值功能。 Holland分类器系统的一个关键组成部分是用于信用分配的“桶 - 旅算法”， 它与我们的井字游戏示例中使用的时序差分算法密切相关，并在第6章中讨论过。 另一个关键组成部分是遗传算法，一种进化方法，其作用是发展有用的表征。 许多研究人员已经广泛开发了分类器系统，以形成强化学习研究的一个主要分支（Urbanowicz和Moore评论，2009）， 虽然我们不认为遗传算法本身就是强化学习系统，但它与进化计算的其他方法一样（例如，Fogel，Owens和Walsh，1966，和Koza，1992）受到了更多的关注。

哈利·克洛普夫（Harry Klopf，1972,1975,1982）是负责恢复人工智能中强化学习的试错线索的最负责人。 Klopf认识到，随着学习研究人员几乎专注于监督学习，适应行为的基本方面正在丧失。 根据Klopf的说法，缺少的是行为的享乐方面，从环境中获得某些结果的驱动力，控制环境朝向期望的目的并远离不希望的目的（见第15.9节）。 这是试错学习的基本思想。 Klopf的思想对作者特别有影响，因为我们对它们的评估（Barto和Sutton，1981a）使我们对监督和强化学习之间的区别以及我们最终关注强化学习的理解有所了解。 我们和他的同事完成的大部分早期工作都是为了表明强化学习和监督学习确实是不同的 （Barto，Sutton和Brouwer，1981；Barto和Sutton，1981b；Barto和Anandan，1985）。 其他研究表明，强化学习如何解决人工神经网络学习中的重要问题， 特别是它如何为多层网络提供学习算法（Barto，Anderson和Sutton，1982；Barto和Anderson，1985；Barto，1985,1986;巴托和约旦，1987年；见第15.10节）。

## 时序差分学习

我们现在转向强化学习历史的第三个主线，即关于时序差分学习的历史。 时序差分学习方法的独特之处在于由相同数量的时间连续估计之间的差异驱动 - 例如，在井字棋子示例中获胜的概率。 这个主线比其他两个主线更小，更不明显，但它在该领域发挥了特别重要的作用，部分原因是时序差分方法似乎是强化学习的新特性。

时序差分学习的起源部分在于动物学习心理学，特别是在辅助强化学的概念中。 辅助强化剂是与主要强化物（例如食物或疼痛）配对的刺激物，因此已经具有类似的增强特性。 Minsky（1954）可能是第一个意识到这种心理学原理对人工学习系统很重要的人。 Arthur Samuel（1959）是第一个提出并实施包含时序差分思想的学习方法的人，这是他着名的跳棋游戏计划的一部分（第16.2节）。

Samuel没有提到明斯基的工作或可能与动物学习有关。他的灵感显然来自Claude Shannon（1950）的建议， 即计算机可以编程使用评估功能下棋，并且可以通过在线修改此功能来改进其游戏 （香农的这些观点也有可能影响Bellman，但我们知道没有证据证明这一点）。 Minsky（1961）在他的“步骤”论文中广泛讨论了塞缪尔的作品，暗示了与二级强化理论的联系，包括自然和人工。

正如我们所讨论的那样，在Minsky和Samuel的工作之后的十年中，在试错法学习方面的计算工作很少，显然在时序差分学习上根本没有计算工作。 1972年，Klopf将试错学习与时序差分学习的重要组成部分结合起来 Klopf对可扩展到大型系统学习的原理感兴趣，因此对局部强化的概念很感兴趣，因此整个学习系统的子组件可以相互加强。 他提出了“广义强化”的概念，即每个组成部分（名义上，每个神经元）都以强化术语来看待所有输入：作为奖励的兴奋性输入和作为惩罚的抑制性输入。 这与我们现在所知的时序差分学习并不是同一个想法，回想起它比Samuel的工作更远。 另一方面，Klopf将这一想法与试错学习联系起来，并将其与动物学习心理学的大量经验数据库联系起来。

Sutton（1978a，b，c）进一步发展了Klopf的思想，特别是与动物学习理论的联系，描述了由时间连续预测的变化驱动的学习规则。 他和Barto改进了这些观点，并开发了一种基于时差学习的经典条件心理模型（Sutton和Barto，1981a；Barto和Sutton，1982）。 接下来是基于时序差分学习的几种其他有影响的经典条件心理模型（例如，Klopf，1988；Moore等，1986；Sutton和Barto，1987,1990）。 此时开发的一些神经科学模型在时序差分学习方面得到了很好的解释 （Hawkins和Kandel，1984；Byrne，Gingrich和Baxter，1990； Gelperin，Hopfield和Tank，1985；Tesauro，1986; Friston等，1994）， 尽管在大多数情况下没有历史联系。

我们在时序差分学习方面的早期工作受到动物学习理论和Klopf工作的强烈影响。 Minsky的“步骤”论文和Samuel的跳棋运动员的关系后来才得到认可。 然而，到1981年，我们完全了解上面提到的所有先前工作，作为时序差分和试错法主线的一部分。 这时我们开发了一种使用时间差的方法学习与试错学习相结合，被称为 演员 - 评论家架构， 并将这种方法应用于Michie和Chambers的极点平衡问题（Barto，Sutton和Anderson，1983）。 这种方法在Sutton（1984）的博士论文中得到了广泛的研究。论文并扩展到Anderson（1986）博士论文中使用反向传播神经网络。 大约在这个时候，Holland（1986）以他的戽式（bucket-brigade）算法的形式将时序差分思想明确地纳入他的分类器系统。 Sutton（1988）采取了一个关键步骤，将时序差分学习与控制分开，将其作为一般预测方法。 该论文还介绍了TD(λ)算法并证明了它的一些收敛性。

当我们在1981年完成关于演员 - 评论家架构的工作时，我们发现了Ian Witten（1977,1976a）的一篇论文， 该论文似乎是时序差分学习规则的最早出版物。 他提出了我们现在称为表格TD(0)的方法，用作解决MDP的自适应控制器的一部分。 这项工作于1974年首次提交期刊出版，并出现在Witten 1976年的博士论文中。 Witten的工作是Andreae早期使用STeLLA和其他试错学习系统进行实验的后代。 因此，Witten的1977年论文涵盖了强化学习研究的主要思路 - 试错法学习和最优控制 - 同时对时序差分学习做出了明显的早期贡献。

1989年，Chris Watkins开发了Q-learning，将时序差分和最优控制线完全结合在一起。 这项工作扩展并整合了强化学习研究的所有三个主线的先前工作。 Paul Werbos（1987）通过争论自1977年以来试错学习和动态规划的融合，为这种整合做出了贡献。 到Watkins的工作时期，强化学习研究已经有了巨大的增长，主要是在人工智能的机器学习子领域， 而且在人工神经网络和人工智能方面也更广泛。1992年，Gerry Tesauro的十五子棋游戏项目TD-Gammon的成功引起了人们对该领域的更多关注。

自本书第一版出版以来，专注于强化学习算法与神经系统强化学习之间的关系的一个神经科学子领域蓬勃发展的。 正如许多研究人员所指出的那样，对此负责的是时间差算法的行为与大脑中多巴胺产生神经元的活动之间的不可思议的相似性 （Friston等，1994；Barto，1995a; Houk，Adams和Barto，1995； Montague，Dayan和Sejnowski，1996；Schultz，Dayan和Montague，1997）。 第15章介绍了强化学习这一激动人心的方面。在最近的强化学习历史中做出的其他重要贡献在这个简短的叙述中无法提及； 我们在其出现的各个章节的最后引用了更多这些内容。

书目备注
关于强化学习的其他一般性报道，我们建议读者参考阅读Szepesv ari（2010），Bertsekas和Tsitsiklis（1996）， Kaelbling（1993a）以及Sugiyama，Hachiya和Morimura（2013）的书籍。 从控制或操作研究角度出发的书籍包括Si，Barto，Powell和Wunsch（2004），Powell（2011），Lewis和Liu（2012）以及Bertsekas（2012）。 Cao（2009）的综述将强化学习置于其他学习和优化随机动力系统的背景下。 机器学习期刊的三个特刊专注于强化学习：Sutton（1992a），Kaelbling（1996）和Singh（2002）。 Barto（1995b）；Kaelbling，Littman和Moore（1996）以及Keerthi和Ravindran（1997）分别提供了有用的调查。 Weiring和van Otterlo（2012）编辑的卷提供了对最近发展的精彩概述。


相比有监督学习和无监督学习，强化学习在机器学习领域的起步更晚。虽然早在1980年代就出现了时序差分算法[42-44]，但对于很多实际问题，我们无法用表格的形式列举出所有的状态和动作，因此这些抽象的算法无法大规模实用。




神经网络与强化学习的结合，即深度强化学习[46-50]，才为强化学习带来了真正的机会。在这里，深度神经网络被用于拟合动作价值函数即Q函数，或者直接拟合策略函数，这使得我们可以处理各种复杂的状态和环境，在围棋、游戏、机器人控制等问题上真正得到应用。神经网络可以直接根据游戏画面，自动驾驶汽车的摄像机传来的图像，当前的围棋棋局，预测出需要执行的动作。其典型的代表是DQN[46]这样的用深度神经网络拟合动作价值函数的算法，以及直接优化策略函数的算法[47-50]。




[46]Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou. Playing Atari with Deep Reinforcement Learning. NIPS 2013.

[47]Mnih, V., Badia, A. P., Mirza, M., Graves, A., Harley, T., Lillicrap, T. P., Silver, D., and Kavukcuoglu, K. (2016). Asynchronous methods for deep reinforcement learning. In the International Conference on Machine Learning (ICML).

[48]Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229–256.

[49]Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M. (2014). Deterministic

policy gradient algorithms. In the International Conference on Machine Learning (ICML).

[50]Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.

(2016). Continuous control with deep reinforcement learning. In the International Conference on

Learning Representations (ICLR).



[51]S. Hochreiter, J. Schmidhuber. Long short-term memory. Neural computation, 9(8): 1735-1780, 1997.

[52] David Silver, et al. Mastering the Game of Go with Deep Neural Networks and Tree Search. Nature, 2016.

