{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tianshou1try.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u9VvM3lwVkO",
        "outputId": "c85e2243-a8de-4fb5-897f-0dd53285e825"
      },
      "source": [
        "!pip install git+https://github.com/thu-ml/tianshou.git@master --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/thu-ml/tianshou.git@master\n",
            "  Cloning https://github.com/thu-ml/tianshou.git (to revision master) to /tmp/pip-req-build-6wxsc28a\n",
            "  Running command git clone -q https://github.com/thu-ml/tianshou.git /tmp/pip-req-build-6wxsc28a\n",
            "Requirement already satisfied, skipping upgrade: gym>=0.15.4 in /usr/local/lib/python3.6/dist-packages (from tianshou==0.3.0) (0.17.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from tianshou==0.3.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy!=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tianshou==0.3.0) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard in /usr/local/lib/python3.6/dist-packages (from tianshou==0.3.0) (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from tianshou==0.3.0) (1.7.0+cu101)\n",
            "Collecting numba>=0.51.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/a5/a065e1e23a5ced6da6628bba7efec4de98f8970a59e034bb0639866631da/numba-0.52.0-cp36-cp36m-manylinux2014_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 8.8MB/s \n",
            "\u001b[?25hCollecting h5py>=3.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/7a/e53e500335afb6b1aade11227cdf107fca54106a1dca5c9d13242a043f3b/h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.15.4->tianshou==0.3.0) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.15.4->tianshou==0.3.0) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.15.4->tianshou==0.3.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (3.3.3)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (1.32.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->tianshou==0.3.0) (0.36.2)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->tianshou==0.3.0) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->tianshou==0.3.0) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->tianshou==0.3.0) (3.7.4.3)\n",
            "Collecting llvmlite<0.36,>=0.35.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/94/f05040ee3ec199c827a38426d78d0af9274c1e18ec9a592bb40954c952c8/llvmlite-0.35.0-cp36-cp36m-manylinux2010_x86_64.whl (25.3MB)\n",
            "\u001b[K     |████████████████████████████████| 25.3MB 1.3MB/s \n",
            "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/48/19/f2090f7dad41e225c7f2326e4cfe6fff49e57dedb5b53636c9551f86b069/cached_property-1.5.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->tianshou==0.3.0) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->tianshou==0.3.0) (3.3.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->tianshou==0.3.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->tianshou==0.3.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->tianshou==0.3.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->tianshou==0.3.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->tianshou==0.3.0) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->tianshou==0.3.0) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->tianshou==0.3.0) (4.2.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->tianshou==0.3.0) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->tianshou==0.3.0) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->tianshou==0.3.0) (0.4.8)\n",
            "Building wheels for collected packages: tianshou\n",
            "  Building wheel for tianshou (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tianshou: filename=tianshou-0.3.0-cp36-none-any.whl size=85468 sha256=b6f48df7576e10797504483d069b168f148beef164c6e128ded08f70bb3842da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j890kiue/wheels/b4/c3/9b/660291d6581e0488c3d5bed27f3dbfe5588076602228aba110\n",
            "Successfully built tianshou\n",
            "\u001b[31mERROR: tensorflow 2.4.0 has requirement h5py~=2.10.0, but you'll have h5py 3.1.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: llvmlite, numba, cached-property, h5py, tianshou\n",
            "  Found existing installation: llvmlite 0.31.0\n",
            "    Uninstalling llvmlite-0.31.0:\n",
            "      Successfully uninstalled llvmlite-0.31.0\n",
            "  Found existing installation: numba 0.48.0\n",
            "    Uninstalling numba-0.48.0:\n",
            "      Successfully uninstalled numba-0.48.0\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "Successfully installed cached-property-1.5.2 h5py-3.1.0 llvmlite-0.35.0 numba-0.52.0 tianshou-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG8x8BxMw283"
      },
      "source": [
        "import torch\r\n",
        "import numpy as np\r\n",
        "from numba import njit\r\n",
        "from typing import Any, Dict, Union, Optional, Tuple"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmkhb1EmyDjf"
      },
      "source": [
        "from tianshou.policy import DQNPolicy\r\n",
        "from tianshou.data import Batch, ReplayBuffer, to_torch_as, to_numpy"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ddagHzyX82"
      },
      "source": [
        "class C51Policy(DQNPolicy):\r\n",
        "    \"\"\"Implementation of Categorical Deep Q-network. arXiv:1707.06887.\r\n",
        "    :param torch.nn.Module model: a model following the rules in\r\n",
        "        :class:`~tianshou.policy.BasePolicy`. (s -> logits)\r\n",
        "    :param torch.optim.Optimizer optim: a torch.optim for optimizing the model.\r\n",
        "    :param float discount_factor: in [0, 1].\r\n",
        "    :param int num_atoms: the number of atoms in the support set of the\r\n",
        "        value distribution, defaults to 51.\r\n",
        "    :param float v_min: the value of the smallest atom in the support set,\r\n",
        "        defaults to -10.0.\r\n",
        "    :param float v_max: the value of the largest atom in the support set,\r\n",
        "        defaults to -10.0.\r\n",
        "    :param int estimation_step: greater than 1, the number of steps to look\r\n",
        "        ahead.\r\n",
        "    :param int target_update_freq: the target network update frequency (0 if\r\n",
        "        you do not use the target network).\r\n",
        "    :param bool reward_normalization: normalize the reward to Normal(0, 1),\r\n",
        "        defaults to False.\r\n",
        "    .. seealso::\r\n",
        "        Please refer to :class:`~tianshou.policy.DQNPolicy` for more detailed\r\n",
        "         explanation.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        model: torch.nn.Module,\r\n",
        "        optim: torch.optim.Optimizer,\r\n",
        "        discount_factor: float = 0.99,\r\n",
        "        num_atoms: int = 51,\r\n",
        "        v_min: float = -10.0,\r\n",
        "        v_max: float = 10.0,\r\n",
        "        estimation_step: int = 1,\r\n",
        "        target_update_freq: int = 0,\r\n",
        "        reward_normalization: bool = False,\r\n",
        "        **kwargs: Any,\r\n",
        "    ) -> None:\r\n",
        "        super().__init__(model, optim, discount_factor,\r\n",
        "                         estimation_step, target_update_freq,\r\n",
        "                         reward_normalization, **kwargs)\r\n",
        "        self._num_atoms = num_atoms\r\n",
        "        self._v_min = v_min\r\n",
        "        self._v_max = v_max\r\n",
        "        self.support = torch.linspace(self._v_min, self._v_max,\r\n",
        "                                      self._num_atoms)\r\n",
        "        self.delta_z = (v_max - v_min) / (num_atoms - 1)\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def prepare_n_step(\r\n",
        "        batch: Batch,\r\n",
        "        buffer: ReplayBuffer,\r\n",
        "        indice: np.ndarray,\r\n",
        "        gamma: float = 0.99,\r\n",
        "        n_step: int = 1,\r\n",
        "        rew_norm: bool = False,\r\n",
        "    ) -> Batch:\r\n",
        "        \"\"\"Modify the obs_next, done and rew in batch for computing n-step return.\r\n",
        "        :param batch: a data batch, which is equal to buffer[indice].\r\n",
        "        :type batch: :class:`~tianshou.data.Batch`\r\n",
        "        :param buffer: a data buffer which contains several full-episode data\r\n",
        "            chronologically.\r\n",
        "        :type buffer: :class:`~tianshou.data.ReplayBuffer`\r\n",
        "        :param indice: sampled timestep.\r\n",
        "        :type indice: numpy.ndarray\r\n",
        "        :param float gamma: the discount factor, should be in [0, 1], defaults\r\n",
        "            to 0.99.\r\n",
        "        :param int n_step: the number of estimation step, should be an int\r\n",
        "            greater than 0, defaults to 1.\r\n",
        "        :param bool rew_norm: normalize the reward to Normal(0, 1), defaults\r\n",
        "            to False.\r\n",
        "        :return: a Batch with modified obs_next, done and rew.\r\n",
        "        \"\"\"\r\n",
        "        buf_len = len(buffer)\r\n",
        "        if rew_norm:\r\n",
        "            bfr = buffer.rew[: min(buf_len, 1000)]  # avoid large buffer\r\n",
        "            mean, std = bfr.mean(), bfr.std()\r\n",
        "            if np.isclose(std, 0, 1e-2):\r\n",
        "                mean, std = 0.0, 1.0\r\n",
        "        else:\r\n",
        "            mean, std = 0.0, 1.0\r\n",
        "        buffer_n = buffer[(indice + n_step - 1) % buf_len]\r\n",
        "        batch.obs_next = buffer_n.obs_next\r\n",
        "        rew_n, done_n = _nstep_batch(buffer.rew, buffer.done,\r\n",
        "                                     indice, gamma, n_step, buf_len, mean, std)\r\n",
        "        batch.rew = rew_n\r\n",
        "        batch.done = done_n\r\n",
        "        return batch\r\n",
        "\r\n",
        "    def process_fn(\r\n",
        "        self, batch: Batch, buffer: ReplayBuffer, indice: np.ndarray\r\n",
        "    ) -> Batch:\r\n",
        "        \"\"\"Prepare the batch for calculating the n-step return.\r\n",
        "        More details can be found at\r\n",
        "        :meth:`~tianshou.policy.C51Policy.prepare_n_step`.\r\n",
        "        \"\"\"\r\n",
        "        batch = self.prepare_n_step(\r\n",
        "            batch, buffer, indice,\r\n",
        "            self._gamma, self._n_step, self._rew_norm)\r\n",
        "        return batch\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        batch: Batch,\r\n",
        "        state: Optional[Union[dict, Batch, np.ndarray]] = None,\r\n",
        "        model: str = \"model\",\r\n",
        "        input: str = \"obs\",\r\n",
        "        **kwargs: Any,\r\n",
        "    ) -> Batch:\r\n",
        "        \"\"\"Compute action over the given batch data.\r\n",
        "        :return: A :class:`~tianshou.data.Batch` which has 2 keys:\r\n",
        "            * ``act`` the action.\r\n",
        "            * ``state`` the hidden state.\r\n",
        "        .. seealso::\r\n",
        "            Please refer to :meth:`~tianshou.policy.DQNPolicy.forward` for\r\n",
        "            more detailed explanation.\r\n",
        "        \"\"\"\r\n",
        "        model = getattr(self, model)\r\n",
        "        obs = batch[input]\r\n",
        "        obs_ = obs.obs if hasattr(obs, \"obs\") else obs\r\n",
        "        dist, h = model(obs_, state=state, info=batch.info)\r\n",
        "        q = (dist * to_torch_as(self.support, dist)).sum(2)\r\n",
        "        act: np.ndarray = to_numpy(q.max(dim=1)[1])\r\n",
        "        if hasattr(obs, \"mask\"):\r\n",
        "            # some of actions are masked, they cannot be selected\r\n",
        "            q_: np.ndarray = to_numpy(q)\r\n",
        "            q_[~obs.mask] = -np.inf\r\n",
        "            act = q_.argmax(axis=1)\r\n",
        "        # add eps to act in training or testing phase\r\n",
        "        if not self.updating and not np.isclose(self.eps, 0.0):\r\n",
        "            for i in range(len(q)):\r\n",
        "                if np.random.rand() < self.eps:\r\n",
        "                    q_ = np.random.rand(*q[i].shape)\r\n",
        "                    if hasattr(obs, \"mask\"):\r\n",
        "                        q_[~obs.mask[i]] = -np.inf\r\n",
        "                    act[i] = q_.argmax()\r\n",
        "        return Batch(logits=dist, act=act, state=h)\r\n",
        "\r\n",
        "    def _target_dist(\r\n",
        "            self, batch: Batch\r\n",
        "    ) -> torch.Tensor:\r\n",
        "        if self._target:\r\n",
        "            a = self(batch, input=\"obs_next\").act\r\n",
        "            next_dist = self(\r\n",
        "                batch, model=\"model_old\", input=\"obs_next\"\r\n",
        "            ).logits\r\n",
        "        else:\r\n",
        "            next_b = self(batch, input=\"obs_next\")\r\n",
        "            a = next_b.act\r\n",
        "            next_dist = next_b.logits\r\n",
        "        batch_size = len(a)\r\n",
        "        next_dist = next_dist[np.arange(batch_size), a, :]\r\n",
        "        device = next_dist.device\r\n",
        "        reward = torch.from_numpy(batch.rew).to(device).unsqueeze(1)\r\n",
        "        done = torch.from_numpy(batch.rew).to(device).float().unsqueeze(1)\r\n",
        "        support = self.support.to(device)\r\n",
        "\r\n",
        "        # Compute the projection of bellman update Tz onto the support z.\r\n",
        "        target_support = reward + (self._gamma ** self._n_step\r\n",
        "                                   ) * (1.0 - done) * support.unsqueeze(0)\r\n",
        "        target_support = target_support.clamp(self._v_min, self._v_max)\r\n",
        "\r\n",
        "        # An amazing trick for calculating the projection gracefully.\r\n",
        "        # ref: https://github.com/ShangtongZhang/DeepRL\r\n",
        "        target_dist = (1 - (target_support.unsqueeze(1) -\r\n",
        "                            support.view(1, -1, 1)).abs() / self.delta_z\r\n",
        "                       ).clamp(0, 1) * next_dist.unsqueeze(1)\r\n",
        "        target_dist = target_dist.sum(-1)\r\n",
        "        return target_dist\r\n",
        "\r\n",
        "    def learn(self, batch: Batch, **kwargs: Any) -> Dict[str, float]:\r\n",
        "        if self._target and self._cnt % self._freq == 0:\r\n",
        "            self.sync_weight()\r\n",
        "        self.optim.zero_grad()\r\n",
        "        weight = batch.pop(\"weight\", 1.0)\r\n",
        "        with torch.no_grad():\r\n",
        "            target_dist = self._target_dist(batch)\r\n",
        "        curr_dist = self(batch).logits\r\n",
        "        act = batch.act\r\n",
        "        curr_dist = curr_dist[np.arange(len(act)), act, :]\r\n",
        "        cross_entropy = - (target_dist * torch.log(curr_dist + 1e-8)).sum(1)\r\n",
        "        loss = (cross_entropy * weight).mean()\r\n",
        "        batch.weight = cross_entropy.detach()  # prio-buffer\r\n",
        "        loss.backward()\r\n",
        "        self.optim.step()\r\n",
        "        self._cnt += 1\r\n",
        "        return {\"loss\": loss.item()}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4rXrPMUyxZw"
      },
      "source": [
        "@njit\r\n",
        "def _nstep_batch(\r\n",
        "    rew: np.ndarray,\r\n",
        "    done: np.ndarray,\r\n",
        "    indice: np.ndarray,\r\n",
        "    gamma: float,\r\n",
        "    n_step: int,\r\n",
        "    buf_len: int,\r\n",
        "    mean: float,\r\n",
        "    std: float,\r\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\r\n",
        "    rew_n = np.zeros(indice.shape)\r\n",
        "    done_n = done[indice]\r\n",
        "    for n in range(n_step - 1, -1, -1):\r\n",
        "        now = (indice + n) % buf_len\r\n",
        "        done_t = done[now]\r\n",
        "        done_n = np.bitwise_or(done_n, done_t)\r\n",
        "        rew_n = (rew[now] - mean) / std + (1.0 - done_t) * gamma * rew_n\r\n",
        "    return rew_n, done_n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "0RivCbAPy72S",
        "outputId": "b525a9d5-c5fe-4a48-f72f-727399896a43"
      },
      "source": [
        "class C51(nn.Module):\r\n",
        "    \"\"\"Reference: A distributional perspective on reinforcement learning.\r\n",
        "    For advanced usage (how to customize the network), please refer to\r\n",
        "    :ref:`build_the_network`.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        c: int,\r\n",
        "        h: int,\r\n",
        "        w: int,\r\n",
        "        action_shape: Sequence[int],\r\n",
        "        num_atoms: int,\r\n",
        "        device: Union[str, int, torch.device] = \"cpu\",\r\n",
        "    ) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.device = device\r\n",
        "        self.action_shape = action_shape\r\n",
        "        self.num_atoms = num_atoms\r\n",
        "\r\n",
        "        def conv2d_size_out(\r\n",
        "            size: int, kernel_size: int = 5, stride: int = 2\r\n",
        "        ) -> int:\r\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\r\n",
        "\r\n",
        "        def conv2d_layers_size_out(\r\n",
        "            size: int,\r\n",
        "            kernel_size_1: int = 8,\r\n",
        "            stride_1: int = 4,\r\n",
        "            kernel_size_2: int = 4,\r\n",
        "            stride_2: int = 2,\r\n",
        "            kernel_size_3: int = 3,\r\n",
        "            stride_3: int = 1,\r\n",
        "        ) -> int:\r\n",
        "            size = conv2d_size_out(size, kernel_size_1, stride_1)\r\n",
        "            size = conv2d_size_out(size, kernel_size_2, stride_2)\r\n",
        "            size = conv2d_size_out(size, kernel_size_3, stride_3)\r\n",
        "            return size\r\n",
        "\r\n",
        "        convw = conv2d_layers_size_out(w)\r\n",
        "        convh = conv2d_layers_size_out(h)\r\n",
        "        linear_input_size = convw * convh * 64\r\n",
        "\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Flatten(),\r\n",
        "            nn.Linear(linear_input_size, 512),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(512, np.prod(action_shape) * num_atoms),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        x: Union[np.ndarray, torch.Tensor],\r\n",
        "        state: Optional[Any] = None,\r\n",
        "        info: Dict[str, Any] = {},\r\n",
        "    ) -> Tuple[torch.Tensor, Any]:\r\n",
        "        r\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\r\n",
        "        if not isinstance(x, torch.Tensor):\r\n",
        "            x = to_torch(x, device=self.device, dtype=torch.float32)\r\n",
        "        x = self.net(x)\r\n",
        "        x = x.view(-1, self.num_atoms).softmax(dim=-1).\\\r\n",
        "            view(-1, np.prod(self.action_shape), self.num_atoms)\r\n",
        "        return x, state"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7f1b9f7bee32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mC51\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Reference: A distributional perspective on reinforcement learning.\n\u001b[1;32m      3\u001b[0m     \u001b[0mFor\u001b[0m \u001b[0madvanced\u001b[0m \u001b[0musage\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcustomize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mrefer\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbuild_the_network\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu-VZh94y-HI"
      },
      "source": [
        "from torch import nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5Ks74ZVzC1m"
      },
      "source": [
        "from tianshou.data import to_torch"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "D8tn_NqzzI5t",
        "outputId": "e687cafc-f266-404d-bea9-45e1ab5892f8"
      },
      "source": [
        "class C51(nn.Module):\r\n",
        "    \"\"\"Reference: A distributional perspective on reinforcement learning.\r\n",
        "    For advanced usage (how to customize the network), please refer to\r\n",
        "    :ref:`build_the_network`.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        c: int,\r\n",
        "        h: int,\r\n",
        "        w: int,\r\n",
        "        action_shape: Sequence[int],\r\n",
        "        num_atoms: int,\r\n",
        "        device: Union[str, int, torch.device] = \"cpu\",\r\n",
        "    ) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.device = device\r\n",
        "        self.action_shape = action_shape\r\n",
        "        self.num_atoms = num_atoms\r\n",
        "\r\n",
        "        def conv2d_size_out(\r\n",
        "            size: int, kernel_size: int = 5, stride: int = 2\r\n",
        "        ) -> int:\r\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\r\n",
        "\r\n",
        "        def conv2d_layers_size_out(\r\n",
        "            size: int,\r\n",
        "            kernel_size_1: int = 8,\r\n",
        "            stride_1: int = 4,\r\n",
        "            kernel_size_2: int = 4,\r\n",
        "            stride_2: int = 2,\r\n",
        "            kernel_size_3: int = 3,\r\n",
        "            stride_3: int = 1,\r\n",
        "        ) -> int:\r\n",
        "            size = conv2d_size_out(size, kernel_size_1, stride_1)\r\n",
        "            size = conv2d_size_out(size, kernel_size_2, stride_2)\r\n",
        "            size = conv2d_size_out(size, kernel_size_3, stride_3)\r\n",
        "            return size\r\n",
        "\r\n",
        "        convw = conv2d_layers_size_out(w)\r\n",
        "        convh = conv2d_layers_size_out(h)\r\n",
        "        linear_input_size = convw * convh * 64\r\n",
        "\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Flatten(),\r\n",
        "            nn.Linear(linear_input_size, 512),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(512, np.prod(action_shape) * num_atoms),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        x: Union[np.ndarray, torch.Tensor],\r\n",
        "        state: Optional[Any] = None,\r\n",
        "        info: Dict[str, Any] = {},\r\n",
        "    ) -> Tuple[torch.Tensor, Any]:\r\n",
        "        r\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\r\n",
        "        if not isinstance(x, torch.Tensor):\r\n",
        "            x = to_torch(x, device=self.device, dtype=torch.float32)\r\n",
        "        x = self.net(x)\r\n",
        "        x = x.view(-1, self.num_atoms).softmax(dim=-1).\\\r\n",
        "            view(-1, np.prod(self.action_shape), self.num_atoms)\r\n",
        "        return x, state"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7f1b9f7bee32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mC51\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Reference: A distributional perspective on reinforcement learning.\n\u001b[1;32m      3\u001b[0m     \u001b[0mFor\u001b[0m \u001b[0madvanced\u001b[0m \u001b[0musage\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcustomize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplease\u001b[0m \u001b[0mrefer\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbuild_the_network\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;32m<ipython-input-9-7f1b9f7bee32>\u001b[0m in \u001b[0;36mC51\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maction_shape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mnum_atoms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     ) -> None:\n\u001b[1;32m     16\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiQywX2MzN_9"
      },
      "source": [
        "from typing import Any, Dict, Tuple, Union, Optional, Sequence"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqlsnbAkzQ7I"
      },
      "source": [
        "class C51(nn.Module):\r\n",
        "    \"\"\"Reference: A distributional perspective on reinforcement learning.\r\n",
        "    For advanced usage (how to customize the network), please refer to\r\n",
        "    :ref:`build_the_network`.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    def __init__(\r\n",
        "        self,\r\n",
        "        c: int,\r\n",
        "        h: int,\r\n",
        "        w: int,\r\n",
        "        action_shape: Sequence[int],\r\n",
        "        num_atoms: int,\r\n",
        "        device: Union[str, int, torch.device] = \"cpu\",\r\n",
        "    ) -> None:\r\n",
        "        super().__init__()\r\n",
        "        self.device = device\r\n",
        "        self.action_shape = action_shape\r\n",
        "        self.num_atoms = num_atoms\r\n",
        "\r\n",
        "        def conv2d_size_out(\r\n",
        "            size: int, kernel_size: int = 5, stride: int = 2\r\n",
        "        ) -> int:\r\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\r\n",
        "\r\n",
        "        def conv2d_layers_size_out(\r\n",
        "            size: int,\r\n",
        "            kernel_size_1: int = 8,\r\n",
        "            stride_1: int = 4,\r\n",
        "            kernel_size_2: int = 4,\r\n",
        "            stride_2: int = 2,\r\n",
        "            kernel_size_3: int = 3,\r\n",
        "            stride_3: int = 1,\r\n",
        "        ) -> int:\r\n",
        "            size = conv2d_size_out(size, kernel_size_1, stride_1)\r\n",
        "            size = conv2d_size_out(size, kernel_size_2, stride_2)\r\n",
        "            size = conv2d_size_out(size, kernel_size_3, stride_3)\r\n",
        "            return size\r\n",
        "\r\n",
        "        convw = conv2d_layers_size_out(w)\r\n",
        "        convh = conv2d_layers_size_out(h)\r\n",
        "        linear_input_size = convw * convh * 64\r\n",
        "\r\n",
        "        self.net = nn.Sequential(\r\n",
        "            nn.Conv2d(c, 32, kernel_size=8, stride=4),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Flatten(),\r\n",
        "            nn.Linear(linear_input_size, 512),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Linear(512, np.prod(action_shape) * num_atoms),\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(\r\n",
        "        self,\r\n",
        "        x: Union[np.ndarray, torch.Tensor],\r\n",
        "        state: Optional[Any] = None,\r\n",
        "        info: Dict[str, Any] = {},\r\n",
        "    ) -> Tuple[torch.Tensor, Any]:\r\n",
        "        r\"\"\"Mapping: x -> Z(x, \\*).\"\"\"\r\n",
        "        if not isinstance(x, torch.Tensor):\r\n",
        "            x = to_torch(x, device=self.device, dtype=torch.float32)\r\n",
        "        x = self.net(x)\r\n",
        "        x = x.view(-1, self.num_atoms).softmax(dim=-1).\\\r\n",
        "            view(-1, np.prod(self.action_shape), self.num_atoms)\r\n",
        "        return x, state"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di2FbDCtzTYv"
      },
      "source": [
        "from tianshou.env import SubprocVectorEnv\r\n",
        "from tianshou.trainer import offpolicy_trainer\r\n",
        "from tianshou.data import Collector, ReplayBuffer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCJyY0rOzfWF"
      },
      "source": [
        "import os\r\n",
        "import torch\r\n",
        "import pprint\r\n",
        "import argparse\r\n",
        "import numpy as np\r\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "adlhbJF5zg3l",
        "outputId": "ebe24319-b1b3-4142-8a7b-fba5114b0f2c"
      },
      "source": [
        "from atari_wrapper import wrap_deepmind"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-a21820287095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0matari_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrap_deepmind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'atari_wrapper'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP6fhBGV0M7d",
        "outputId": "e88430dd-4f26-45d6-cd64-d50d4d0bb468"
      },
      "source": [
        "!git clone https://github.com/thu-ml/tianshou.git"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tianshou'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 5179 (delta 0), reused 2 (delta 0), pack-reused 5167\u001b[K\n",
            "Receiving objects: 100% (5179/5179), 3.62 MiB | 31.93 MiB/s, done.\n",
            "Resolving deltas: 100% (3559/3559), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tgKvwi-0liW",
        "outputId": "7a2fb7bf-8f8c-4001-87f6-c6aec9f97428"
      },
      "source": [
        "%cd \"content/tianshou/atari\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'content/tianshou/atari'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xOnFibJ05z1",
        "outputId": "c07e2acf-94cd-4e68-81e3-b04de54814e2"
      },
      "source": [
        "%cd \"/content/tianshou/atari\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/tianshou/atari'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rq9G4kvc0_Ld",
        "outputId": "0613722b-01ec-4b9d-b459-8e73ce463a1d"
      },
      "source": [
        "%cd '/content/tianshou/examples/atari'"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/tianshou/examples/atari\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA6_p5Gj1Oe3"
      },
      "source": [
        "def get_args():\r\n",
        "    parser = argparse.ArgumentParser()\r\n",
        "    parser.add_argument('--task', type=str, default='PongNoFrameskip-v4')\r\n",
        "    parser.add_argument('--seed', type=int, default=0)\r\n",
        "    parser.add_argument('--eps-test', type=float, default=0.005)\r\n",
        "    parser.add_argument('--eps-train', type=float, default=1.)\r\n",
        "    parser.add_argument('--eps-train-final', type=float, default=0.05)\r\n",
        "    parser.add_argument('--buffer-size', type=int, default=100000)\r\n",
        "    parser.add_argument('--lr', type=float, default=0.0001)\r\n",
        "    parser.add_argument('--gamma', type=float, default=0.99)\r\n",
        "    parser.add_argument('--num-atoms', type=int, default=51)\r\n",
        "    parser.add_argument('--v-min', type=float, default=-10.)\r\n",
        "    parser.add_argument('--v-max', type=float, default=10.)\r\n",
        "    parser.add_argument('--n-step', type=int, default=3)\r\n",
        "    parser.add_argument('--target-update-freq', type=int, default=500)\r\n",
        "    parser.add_argument('--epoch', type=int, default=100)\r\n",
        "    parser.add_argument('--step-per-epoch', type=int, default=10000)\r\n",
        "    parser.add_argument('--collect-per-step', type=int, default=10)\r\n",
        "    parser.add_argument('--batch-size', type=int, default=32)\r\n",
        "    parser.add_argument('--training-num', type=int, default=16)\r\n",
        "    parser.add_argument('--test-num', type=int, default=10)\r\n",
        "    parser.add_argument('--logdir', type=str, default='log')\r\n",
        "    parser.add_argument('--render', type=float, default=0.)\r\n",
        "    parser.add_argument(\r\n",
        "        '--device', type=str,\r\n",
        "        default='cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "    parser.add_argument('--frames_stack', type=int, default=4)\r\n",
        "    parser.add_argument('--resume_path', type=str, default=None)\r\n",
        "    parser.add_argument('--watch', default=False, action='store_true',\r\n",
        "                        help='watch the play of pre-trained policy only')\r\n",
        "    return parser.parse_args()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JutD1cip1Plr"
      },
      "source": [
        "def make_atari_env(args):\r\n",
        "    return wrap_deepmind(args.task, frame_stack=args.frames_stack)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BArXMoRR1Ryo"
      },
      "source": [
        "def make_atari_env_watch(args):\r\n",
        "    return wrap_deepmind(args.task, frame_stack=args.frames_stack,\r\n",
        "                         episode_life=False, clip_rewards=False)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "mjy-FPoV1TT9",
        "outputId": "4a17e851-12cb-4e7c-9273-23ec2995eab3"
      },
      "source": [
        "def test_c51(args=get_args()):\r\n",
        "    env = make_atari_env(args)\r\n",
        "    args.state_shape = env.observation_space.shape or env.observation_space.n\r\n",
        "    args.action_shape = env.env.action_space.shape or env.env.action_space.n\r\n",
        "    # should be N_FRAMES x H x W\r\n",
        "    print(\"Observations shape:\", args.state_shape)\r\n",
        "    print(\"Actions shape:\", args.action_shape)\r\n",
        "    # make environments\r\n",
        "    train_envs = SubprocVectorEnv([lambda: make_atari_env(args)\r\n",
        "                                   for _ in range(args.training_num)])\r\n",
        "    test_envs = SubprocVectorEnv([lambda: make_atari_env_watch(args)\r\n",
        "                                  for _ in range(args.test_num)])\r\n",
        "    # seed\r\n",
        "    np.random.seed(args.seed)\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "    train_envs.seed(args.seed)\r\n",
        "    test_envs.seed(args.seed)\r\n",
        "    # define model\r\n",
        "    net = C51(*args.state_shape, args.action_shape,\r\n",
        "              args.num_atoms, args.device).to(args.device)\r\n",
        "    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\r\n",
        "    # define policy\r\n",
        "    policy = C51Policy(net, optim, args.gamma, args.num_atoms,\r\n",
        "                       args.v_min, args.v_max, args.n_step,\r\n",
        "                       target_update_freq=args.target_update_freq)\r\n",
        "    # load a previous policy\r\n",
        "    if args.resume_path:\r\n",
        "        policy.load_state_dict(torch.load(\r\n",
        "            args.resume_path, map_location=args.device\r\n",
        "        ))\r\n",
        "        print(\"Loaded agent from: \", args.resume_path)\r\n",
        "    # replay buffer: `save_last_obs` and `stack_num` can be removed together\r\n",
        "    # when you have enough RAM\r\n",
        "    buffer = ReplayBuffer(args.buffer_size, ignore_obs_next=True,\r\n",
        "                          save_only_last_obs=True, stack_num=args.frames_stack)\r\n",
        "    # collector\r\n",
        "    train_collector = Collector(policy, train_envs, buffer)\r\n",
        "    test_collector = Collector(policy, test_envs)\r\n",
        "    # log\r\n",
        "    log_path = os.path.join(args.logdir, args.task, 'c51')\r\n",
        "    writer = SummaryWriter(log_path)\r\n",
        "\r\n",
        "    def save_fn(policy):\r\n",
        "        torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\r\n",
        "\r\n",
        "    def stop_fn(mean_rewards):\r\n",
        "        if env.env.spec.reward_threshold:\r\n",
        "            return mean_rewards >= env.spec.reward_threshold\r\n",
        "        elif 'Pong' in args.task:\r\n",
        "            return mean_rewards >= 20\r\n",
        "        else:\r\n",
        "            return False\r\n",
        "\r\n",
        "    def train_fn(epoch, env_step):\r\n",
        "        # nature DQN setting, linear decay in the first 1M steps\r\n",
        "        if env_step <= 1e6:\r\n",
        "            eps = args.eps_train - env_step / 1e6 * \\\r\n",
        "                (args.eps_train - args.eps_train_final)\r\n",
        "        else:\r\n",
        "            eps = args.eps_train_final\r\n",
        "        policy.set_eps(eps)\r\n",
        "        writer.add_scalar('train/eps', eps, global_step=env_step)\r\n",
        "\r\n",
        "    def test_fn(epoch, env_step):\r\n",
        "        policy.set_eps(args.eps_test)\r\n",
        "\r\n",
        "    # watch agent's performance\r\n",
        "    def watch():\r\n",
        "        print(\"Testing agent ...\")\r\n",
        "        policy.eval()\r\n",
        "        policy.set_eps(args.eps_test)\r\n",
        "        test_envs.seed(args.seed)\r\n",
        "        test_collector.reset()\r\n",
        "        result = test_collector.collect(n_episode=[1] * args.test_num,\r\n",
        "                                        render=args.render)\r\n",
        "        pprint.pprint(result)\r\n",
        "\r\n",
        "    if args.watch:\r\n",
        "        watch()\r\n",
        "        exit(0)\r\n",
        "\r\n",
        "    # test train_collector and start filling replay buffer\r\n",
        "    train_collector.collect(n_step=args.batch_size * 4)\r\n",
        "    # trainer\r\n",
        "    result = offpolicy_trainer(\r\n",
        "        policy, train_collector, test_collector, args.epoch,\r\n",
        "        args.step_per_epoch, args.collect_per_step, args.test_num,\r\n",
        "        args.batch_size, train_fn=train_fn, test_fn=test_fn,\r\n",
        "        stop_fn=stop_fn, save_fn=save_fn, writer=writer, test_in_train=False)\r\n",
        "\r\n",
        "    pprint.pprint(result)\r\n",
        "    watch()\r\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--task TASK] [--seed SEED]\n",
            "                             [--eps-test EPS_TEST] [--eps-train EPS_TRAIN]\n",
            "                             [--eps-train-final EPS_TRAIN_FINAL]\n",
            "                             [--buffer-size BUFFER_SIZE] [--lr LR]\n",
            "                             [--gamma GAMMA] [--num-atoms NUM_ATOMS]\n",
            "                             [--v-min V_MIN] [--v-max V_MAX] [--n-step N_STEP]\n",
            "                             [--target-update-freq TARGET_UPDATE_FREQ]\n",
            "                             [--epoch EPOCH] [--step-per-epoch STEP_PER_EPOCH]\n",
            "                             [--collect-per-step COLLECT_PER_STEP]\n",
            "                             [--batch-size BATCH_SIZE]\n",
            "                             [--training-num TRAINING_NUM]\n",
            "                             [--test-num TEST_NUM] [--logdir LOGDIR]\n",
            "                             [--render RENDER] [--device DEVICE]\n",
            "                             [--frames_stack FRAMES_STACK]\n",
            "                             [--resume_path RESUME_PATH] [--watch]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-d7fae77d-4e4b-4c79-8936-739a0fe31d1c.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRkMKAZw1oBC",
        "outputId": "9feca4c2-f4d1-41e9-aeff-ac55efa20311"
      },
      "source": [
        "%%python3 atari_dqn.py --task \"SpaceInvadersNoFrameskip-v4\" --test-num 100"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python3 is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAYoQ-FF2QBV",
        "outputId": "ff12dbbc-ba24-480a-fe02-9be8bdc54af8"
      },
      "source": [
        "%python3 atari_dqn.py --task \"SpaceInvadersNoFrameskip-v4\" --test-num 100"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python3` not found (But cell magic `%%python3` exists, did you mean that instead?).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJdACIN02iLY",
        "outputId": "8a0c952b-5a84-48ef-a555-bbf82f4dfd6b"
      },
      "source": [
        "%%python3 atari_dqn.py --task \"SpaceInvadersNoFrameskip-v4\" --test-num 10"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python3 is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "CXjckD-a1opT",
        "outputId": "160bfe7c-36a4-4d77-9c85-6caf99b05978"
      },
      "source": [
        "def test_c51(args=get_args()):\r\n",
        "    env = make_atari_env(args)\r\n",
        "    args.state_shape = env.observation_space.shape or env.observation_space.n\r\n",
        "    args.action_shape = env.env.action_space.shape or env.env.action_space.n\r\n",
        "    # should be N_FRAMES x H x W\r\n",
        "    print(\"Observations shape:\", args.state_shape)\r\n",
        "    print(\"Actions shape:\", args.action_shape)\r\n",
        "    # make environments\r\n",
        "    train_envs = SubprocVectorEnv([lambda: make_atari_env(args)\r\n",
        "                                   for _ in range(args.training_num)])\r\n",
        "    test_envs = SubprocVectorEnv([lambda: make_atari_env_watch(args)\r\n",
        "                                  for _ in range(args.test_num)])\r\n",
        "    # seed\r\n",
        "    np.random.seed(args.seed)\r\n",
        "    torch.manual_seed(args.seed)\r\n",
        "    train_envs.seed(args.seed)\r\n",
        "    test_envs.seed(args.seed)\r\n",
        "    # define model\r\n",
        "    net = C51(*args.state_shape, args.action_shape,\r\n",
        "              args.num_atoms, args.device).to(args.device)\r\n",
        "    optim = torch.optim.Adam(net.parameters(), lr=args.lr)\r\n",
        "    # define policy\r\n",
        "    policy = C51Policy(net, optim, args.gamma, args.num_atoms,\r\n",
        "                       args.v_min, args.v_max, args.n_step,\r\n",
        "                       target_update_freq=args.target_update_freq)\r\n",
        "    # load a previous policy\r\n",
        "    if args.resume_path:\r\n",
        "        policy.load_state_dict(torch.load(\r\n",
        "            args.resume_path, map_location=args.device\r\n",
        "        ))\r\n",
        "        print(\"Loaded agent from: \", args.resume_path)\r\n",
        "    # replay buffer: `save_last_obs` and `stack_num` can be removed together\r\n",
        "    # when you have enough RAM\r\n",
        "    buffer = ReplayBuffer(args.buffer_size, ignore_obs_next=True,\r\n",
        "                          save_only_last_obs=True, stack_num=args.frames_stack)\r\n",
        "    # collector\r\n",
        "    train_collector = Collector(policy, train_envs, buffer)\r\n",
        "    test_collector = Collector(policy, test_envs)\r\n",
        "    # log\r\n",
        "    log_path = os.path.join(args.logdir, args.task, 'c51')\r\n",
        "    writer = SummaryWriter(log_path)\r\n",
        "\r\n",
        "    def save_fn(policy):\r\n",
        "        torch.save(policy.state_dict(), os.path.join(log_path, 'policy.pth'))\r\n",
        "\r\n",
        "    def stop_fn(mean_rewards):\r\n",
        "        if env.env.spec.reward_threshold:\r\n",
        "            return mean_rewards >= env.spec.reward_threshold\r\n",
        "        elif 'Pong' in args.task:\r\n",
        "            return mean_rewards >= 20\r\n",
        "        else:\r\n",
        "            return False\r\n",
        "\r\n",
        "    def train_fn(epoch, env_step):\r\n",
        "        # nature DQN setting, linear decay in the first 1M steps\r\n",
        "        if env_step <= 1e6:\r\n",
        "            eps = args.eps_train - env_step / 1e6 * \\\r\n",
        "                (args.eps_train - args.eps_train_final)\r\n",
        "        else:\r\n",
        "            eps = args.eps_train_final\r\n",
        "        policy.set_eps(eps)\r\n",
        "        writer.add_scalar('train/eps', eps, global_step=env_step)\r\n",
        "\r\n",
        "    def test_fn(epoch, env_step):\r\n",
        "        policy.set_eps(args.eps_test)\r\n",
        "\r\n",
        "    # watch agent's performance\r\n",
        "    def watch():\r\n",
        "        print(\"Testing agent ...\")\r\n",
        "        policy.eval()\r\n",
        "        policy.set_eps(args.eps_test)\r\n",
        "        test_envs.seed(args.seed)\r\n",
        "        test_collector.reset()\r\n",
        "        result = test_collector.collect(n_episode=[1] * args.test_num,\r\n",
        "                                        render=args.render)\r\n",
        "        pprint.pprint(result)\r\n",
        "\r\n",
        "    if args.watch:\r\n",
        "        watch()\r\n",
        "        exit(0)\r\n",
        "\r\n",
        "    # test train_collector and start filling replay buffer\r\n",
        "    train_collector.collect(n_step=args.batch_size * 4)\r\n",
        "    # trainer\r\n",
        "    result = offpolicy_trainer(\r\n",
        "        policy, train_collector, test_collector, args.epoch,\r\n",
        "        args.step_per_epoch, args.collect_per_step, args.test_num,\r\n",
        "        args.batch_size, train_fn=train_fn, test_fn=test_fn,\r\n",
        "        stop_fn=stop_fn, save_fn=save_fn, writer=writer, test_in_train=False)\r\n",
        "\r\n",
        "    pprint.pprint(result)\r\n",
        "    watch()\r\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--task TASK] [--seed SEED]\n",
            "                             [--eps-test EPS_TEST] [--eps-train EPS_TRAIN]\n",
            "                             [--eps-train-final EPS_TRAIN_FINAL]\n",
            "                             [--buffer-size BUFFER_SIZE] [--lr LR]\n",
            "                             [--gamma GAMMA] [--num-atoms NUM_ATOMS]\n",
            "                             [--v-min V_MIN] [--v-max V_MAX] [--n-step N_STEP]\n",
            "                             [--target-update-freq TARGET_UPDATE_FREQ]\n",
            "                             [--epoch EPOCH] [--step-per-epoch STEP_PER_EPOCH]\n",
            "                             [--collect-per-step COLLECT_PER_STEP]\n",
            "                             [--batch-size BATCH_SIZE]\n",
            "                             [--training-num TRAINING_NUM]\n",
            "                             [--test-num TEST_NUM] [--logdir LOGDIR]\n",
            "                             [--render RENDER] [--device DEVICE]\n",
            "                             [--frames_stack FRAMES_STACK]\n",
            "                             [--resume_path RESUME_PATH] [--watch]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-d7fae77d-4e4b-4c79-8936-739a0fe31d1c.json\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "DZeb72S323_i",
        "outputId": "ebe5206c-d165-4a75-81a1-e69831f3d91e"
      },
      "source": [
        "%tb"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-70d8ec2c36ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtest_c51\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_atari_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# should be N_FRAMES x H x W\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-b454db464537>\u001b[0m in \u001b[0;36mget_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     parser.add_argument('--watch', default=False, action='store_true',\n\u001b[1;32m     30\u001b[0m                         help='watch the play of pre-trained policy only')\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1746\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1747\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2401\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2402\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hKJsBxv3EIU",
        "outputId": "6175a1d6-0e48-48aa-9ced-60df57ca0e3b"
      },
      "source": [
        "%%python3 atari_dqn.py --task \"PongNoFrameskip-v4\" --batch-size 64"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python3 is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkzz6bO53WRO",
        "outputId": "52009124-41c4-4010-f2f5-f461544780c4"
      },
      "source": [
        "%%python3 atari_dqn.py"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python3 is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNslIyLA3YqV",
        "outputId": "223c094f-86b2-474a-dfed-1ec7289d0e9c"
      },
      "source": [
        "%%python atari_dqn.py"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNGknAaR3jH4",
        "outputId": "9ed8e5f9-64be-4e0f-8d0a-ac5d050266aa"
      },
      "source": [
        "%%python atari_dqn.py --task \"PongNoFrameskip-v4\" --batch-size 64"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UsageError: %%python is a cell magic, but the cell body is empty.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pMPY1-T3rC8"
      },
      "source": [
        "https://python-forum.io/printthread.php?tid=1180\r\n",
        "The correct command I should type is \" %run hello.py\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "u75UVuIE3nnP",
        "outputId": "fb4c12be-741b-466c-ddc2-b663c74992a6"
      },
      "source": [
        "%run atari_dqn.py --task \"PongNoFrameskip-v4\" --batch-size 64"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observations shape: (4, 84, 84)\n",
            "Actions shape: 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #1: 10001it [04:56, 33.70it/s, env_step=101165, len=956, loss=0.025479, n/ep=1, n/st=956, rew=-20.00, v/ep=0.27, v/st=260.23]                           \n",
            "Epoch #2:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #1: test_reward: -20.900000 ± 0.300000, best_reward: -20.900000 ± 0.300000 in #1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #2: 10001it [04:44, 35.19it/s, env_step=202008, len=1117, loss=0.033165, n/ep=1, n/st=1117, rew=-20.00, v/ep=0.30, v/st=332.62]                           \n",
            "Epoch #3:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #2: test_reward: -20.900000 ± 0.300000, best_reward: -20.900000 ± 0.300000 in #1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #3: 10001it [04:49, 34.54it/s, env_step=303356, len=1281, loss=0.029361, n/ep=1, n/st=1281, rew=-18.00, v/ep=0.43, v/st=554.28]                           \n",
            "Epoch #4:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #3: test_reward: -11.600000 ± 2.059126, best_reward: -11.600000 ± 2.059126 in #3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #4: 10001it [04:52, 34.16it/s, env_step=403792, len=1442, loss=0.027678, n/ep=1, n/st=1442, rew=-18.00, v/ep=0.17, v/st=250.43]                           \n",
            "Epoch #5:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #4: test_reward: -10.200000 ± 2.675818, best_reward: -10.200000 ± 2.675818 in #4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #5: 10001it [04:45, 35.05it/s, env_step=505171, len=1644, loss=0.029317, n/ep=1, n/st=1644, rew=-16.00, v/ep=3.06, v/st=5028.97]                           \n",
            "Epoch #6:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #5: test_reward: -3.600000 ± 2.374868, best_reward: -3.600000 ± 2.374868 in #5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #6: 10001it [04:42, 35.39it/s, env_step=605886, len=2313, loss=0.022891, n/ep=1, n/st=2313, rew=-9.00, v/ep=1.52, v/st=3523.63]\n",
            "Epoch #7:   0%|          | 0/10000 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #6: test_reward: 14.300000 ± 1.345362, best_reward: 14.300000 ± 1.345362 in #6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch #7: 10001it [04:47, 34.78it/s, env_step=707013, len=2832, loss=0.021951, n/ep=1, n/st=2832, rew=-3.00, v/ep=0.14, v/st=408.89]                           \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/content/tianshou/examples/atari/atari_dqn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0mtest_dqn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/tianshou/examples/atari/atari_dqn.py\u001b[0m in \u001b[0;36mtest_dqn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_per_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         stop_fn=stop_fn, save_fn=save_fn, writer=writer, test_in_train=False)\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tianshou/trainer/offpolicy.py\u001b[0m in \u001b[0;36moffpolicy_trainer\u001b[0;34m(policy, train_collector, test_collector, max_epoch, step_per_epoch, collect_per_step, episode_per_test, batch_size, update_per_step, train_fn, test_fn, stop_fn, save_fn, writer, log_interval, verbose, test_in_train)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         result = test_episode(policy, test_collector, test_fn, epoch,\n\u001b[0;32m--> 138\u001b[0;31m                               episode_per_test, writer, env_step)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbest_reward\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rew\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mbest_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_reward_std\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rew\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rew_std\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tianshou/trainer/utils.py\u001b[0m in \u001b[0;36mtest_episode\u001b[0;34m(policy, collector, test_fn, epoch, n_episode, writer, global_step)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mn_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_episode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mn_episode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mglobal_step\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tianshou/data/collector.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self, n_step, n_episode, random, render, no_grad)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# step in env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_async\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mobs_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;31m# store computed actions, states, etc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tianshou/env/venvs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action, id)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tianshou/env/worker/subproc.py\u001b[0m in \u001b[0;36msend_action\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_remote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     def get_result(\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut-jPST_AXNt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}