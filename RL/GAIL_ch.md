# GAIL_ch

## 摘要

考虑从示例专家行为中学习策略，而不与专家交互或访问强化信号。一种方法是利用逆强化学习恢复专家的成本函数，然后利用强化学习从成本函数中提取策略。这种方法是间接的，可能是缓慢的。我们提出了一个新的框架来直接从数据中提取策略，就好像它是通过反向强化学习之后的强化学习获得的。我们表明，我们框架的某个实例将模仿学习和生成对抗网络进行类比，从中我们获得了一种无模型模仿学习算法，在大型、高维环境中模仿复杂行为时，该算法比现有的无模型方法获得了显著的性能收益

## 简介

我们对模仿学习的特定设置感兴趣-从专家演示中学到如何执行任务的问题-在学习中，仅向学习者提供专家轨迹的样本，不允许在培训过程中向专家查询更多数据，并且 没有提供任何形式的增强信号。 有两种主要的方法适合这种情况：行为克隆[20]，它从专家轨迹中通过状态-动作对学习策略，作为监督学习问题； 逆向强化学习[25,18]，找到了一个成本函数，专家在该函数下是最优的。行为克隆虽然很简单，但由于协变量移位引起的复合误差，仅倾向于成功处理大量数据[23,24] ]。 另一方面，逆强化学习（IRL）学习的成本函数将整个轨迹优先于其他轨迹，因此复合误差（适合单步决策的方法的问题）不是问题。 因此，IRL成功解决了许多问题，从预测出租车司机的行为[31]到规划四足机器人的脚步[22]。

不幸的是，许多IRL算法运行起来非常昂贵，需要在内部循环中进行强化学习。因此，将IRL方法扩展到大型环境已经成为许多近期工作的焦点[7,14]。然而，从根本上说，IRL学习了一个成本函数，它解释了专家的行为，但没有直接告诉学习者如何行动。考虑到学习者的真正目标通常是采取模仿专家的行动——事实上，许多IRL算法是根据他们学习的成本的最优行动的质量来评估的——那么，为什么我们必须学习一个成本函数，如果这样做可能会导致重大的计算费用，但不能直接产生行动?

我们需要一种算法，它可以通过直接学习一种策略明确地告诉我们如何采取行动。为了开发这样的算法，我们从第3节开始，在第3节中，我们对通过最大因果熵IRL学习的成本函数进行强化学习给出的策略进行了描述[31,32]。我们的描述引入了一个框架，可以直接从数据中学习策略，绕过任何中间的IRL步骤。

然后，我们在第4节和第5节用一个新的无模型模仿学习算法来实例化我们的框架。我们表明，我们的结果算法密切连接到生成的adversarialarXiv:1606.03476v1 [cs。网络[9]，一种来自深度学习社区的技术，最近在自然图像分布建模方面取得了成功:我们的算法利用生成对抗训练来适应定义专家行为的状态和行动分布。我们在第6节测试了我们的算法，其中我们发现，对于复杂的、基于高维物理的控制任务，在不同数量的专家数据上，我们的算法在训练策略方面比竞争方法有很大的优势






## 生成式对抗模仿学习

如第4节所讨论的，常量正则化导致模仿学习算法精确匹配占用度量，但在大型环境中是难以处理的。另一方面，线性代价函数类(10)的指标正则化，导致算法在没有仔细调整的情况下无法精确匹配占用措施，但在大型环境中是易于处理的。我们建议以下新的成本调节器，结合两个世界的最好的，我们将在接下来的部分显示:





## Discussion and outlook

正如我们所展示的，就专家数据而言，我们的方法通常是相当有效的样本。然而，在训练过程中的环境交互作用方面，它的样本效率并不特别高。估计模拟目标梯度(18)所需的样本数量与TRPO从强化信号中训练专家政策所需的样本数量相当。我们相信，我们可以通过使用行为克隆来初始化策略参数，从而显著提高算法的学习速度，而行为克隆完全不需要环境的交互作用。

从根本上说，我们的方法是无模型的，因此它通常比基于模型的方法需要更多的环境交互。例如，引导成本学习[7]是建立在引导策略搜索[13]的基础上，继承了其样本效率，但也继承了其通过迭代拟合时变线性动态来很好地近似模型的要求。有趣的是，我们的算法1和引导代价学习在策略优化步骤和代价拟合(我们称之为甄别拟合)之间交替，尽管这两种算法的推导方式完全不同。我们的方法建立在IRL的大量工作之上[31,1,29,28]，因此，就像IRL一样，我们的方法在培训期间不与专家互动。

我们的方法随机探索，以确定哪些行动使政策的占用度量更接近专家的，而与专家互动的方法，如DAgger[24]，可以简单地要求专家采取这些行动。最终，我们相信，在专家数据和环境交互的样本复杂性方面，将精心选择的环境模型与专家交互相结合的方法将会胜出

---

## Overall Algorithm

 Algorithm 1
            Input:
              set of demonstrations $\{z^d_t\}_{t=1\dots T^d}$ Initialize $\pi_g$ and $D_\phi$ randomly
            for $i$ in $1 \dots N$ do
              Sample rollouts $\{z^g_t\}_{t=1\dots T^g}$ from $\pi_g$ in the environment $E$
              Calculate rewards $\{r_t = - \log(1 - D_\phi(z^g_t))\}_{t=1\dots T^g}$
              Update $\theta$ with TRPO
            for $j$ in $1\dots M$ do
              $\mathscr{L}(\phi) = \Sigma_{t=1\dots T^g} \log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots T^d} \log(D_\phi(z^d_t))$
              Update $\phi$ with gradient descent
            Return: $\pi_g$

在标准GAN公式中，有两个损耗函数可以分别通过规则梯度下降算法进行优化:一个损耗函数用于鉴别器，一个用于产生器。然而，在GAIL中，generator network $G_\theta$编码策略$\pi_g$，不能用简单的梯度下降法训练。
我们使用策略梯度的方法来训练生成器，虽然也有非梯度的方法在文献中探索。基于初次展示，$\{z^g_t\}_{t=1\dots T^g}$ of $\pi_g$, and the rewards $\{r_t\}_{t=1\dots T^g}$

鉴别器$D_\phi$要优化的损失函数很像一个正则GAN: $$ mathscr{L}(\phi) = \Sigma_{t=1\dots t ^g} log(1 - D_\phi(z^g_t)) - \Sigma_{t=1\dots t ^d} \log(D_\phi(z^d_t))$$
生成器的参数$\theta$被更新后，鉴别器的参数$\phi$被更新$M$次。算法1给出了盖尔策略训练的伪代码。


华盛顿大学(University of Washington)开发的一种物理引擎，是一种很受尊敬的机器人研究实验室使用的模拟器，但需要付费许可证。个人和教育使用的免费许可证是可用的。

基于开源的Bullet物理引擎，是免费授权的，支持与MuJoCo相同的框架和环境定义。Bullet在许多情况下也可以更快，并具有活跃开发中的接触建模等功能。

我们最初的目标是将Roboschool专门用于我们的目标类人环境。然而，我们发现，基于开放的人工智能基线，我们的TRPO的实施对MuJoCo高度调整，并未能在机器人学校环境中训练成功的运动策略。本文所详述的实验均以MuJoCo为模拟环境。

我们设计了一个定制的MuJoCo类人骨骼，试图在关节的相对长度和重量方面近似真实的人体。最终的骨架如下所示。

注意，这个骨架与动作捕捉和真实视频数据中的真实骨架不一样。因此，我们的实现执行模仿和重定向。模仿学习评估和推出仅基于以下特性$z_t$，基于状态$\{s_t\}_{t=1\dots t}$:

- 脚、手和头的五个标准化3D矢量位置(相对于骨盆)
- 在最后的$k$时间步上骨盆位置的平均变化(在我们的设置中$k$被设置为10)。

生成的策略$\pi_g$对所有21个关节执行操作。

## 生成器策略网络$\pi_g$

我们最终的策略网络由3个隐藏层组成，每个隐藏层包含150个神经元，它们都具有切向激活函数。我们还试验了隐藏形状的双层网络(32,32)和(64,64)，但发现更大的网络对可靠的性能是必要的。我们发现，足够的性能所必需的网络规模与目标骨架中存在的自由度成比例。不幸的是，更大的网络也会显著降低训练速度。

### 专家政策$\pi_d$

专家技能可以是人体的任何运动，如走路或跑步。这种技能可以表示为由隐式专家策略$ pi_d$生成的rollout(以离散时间步$ {t_1, t_2， \ldots,s_n\}$生成的一系列状态$\{s_1, s_2， \ldots, t_n\}$)，该策略是一个函数，将专家$s$的状态映射到状态$a$的下一个动作。我们试图从三个来源的专家演示来训练系统，这些演示按照复杂性的增加顺序进行。

1. 强化学习政策
2. 动作捕捉数据
3. 视频的技能

这三种不同的演示源产生了重要的复杂性，而来自每一种的培训都有各自的挑战，我们将在下面详细讨论。


# 专家演示的来源

## 向人工专家学习(RL)

测试GAIL最简单的方法是模仿通过直接强化学习获得的策略，在这种策略中，agent与环境相互作用，对这些相互作用接受奖励或惩罚，并根据这些相互作用和奖励学习或更新策略。Ho等人利用这种方法获得专家政策。注意，RL策略同时提供状态和操作，但是在现实世界中很难观察到操作。下面的视频显示了我们的信任区域政策优化(TRPO) RL政策及其学习到的GAIL政策的结果。

## 从动作捕捉数据学习

一个更接近现实的设置是模仿动作捕捉数据。在这里，演示者既不提供行动也不提供奖励。动作捕捉数据是通过将可跟踪的标记附着在人类演员身上获得的，他们在一个特殊的跟踪系统前表演记录下来的技能演示。CMU图形实验室的动作捕捉数据库(MOCAP)提供了几种技能，如行走和跳跃。由于该数据仅提供各州，我们调整了GAIL，使其在没有专家行动的情况下发挥作用，如Merel等人，也称为S-GAIL。

## 运动捕捉提取和重采样

CMU动作捕捉数据集提供了AMC文件，其中包含了全类人骨骼在120Hz频率下的各种动作(如走路、跳舞等)的时间序列关节角度。我们创建了一个自动工具来解释AMC文件，以与模拟器相同的帧率对数据进行采样，并从动作捕捉数据生成展板。在这个项目中，我们使用三次样条插值将动画向下采样到大约66.67Hz的频率。

## 学习视频

从动作捕捉数据中学习需要在展示各种技能的演员身上做标记，并要求他们在昂贵的跟踪系统中表演。相反，我们更愿意直接从视频中学习技能，这更便宜。实现这一目标的第一步，在我们现有的GAIL框架内，是从一个人表演一种技能的视频中获得三维人体姿势。从原始图像中获取三维姿态的大多数方法都属于两类:

一种管道方法，其中首先从图像中估计2D姿态，然后估计3D姿态。

学习3D姿势直接从图像在端到端的方式。

我们首先尝试了流水线方法，使用2D姿态估计训练过的模型，并将其输出到另一个模块，该模块根据2D姿态估计3D姿态。我们能够在2D姿态估计方面取得良好的结果，但是3D姿态估计在行走等技能方面受到限制。连接两个不完善的模型的脆弱性告诉我们为什么端到端方法在这样的重建问题上越来越受欢迎。

接下来，我们实验了从图像中直接学习3D姿态的方法。这克服了流水线方法的局限性，通过反向传播关于骨架结构的3D信息到2D卷积层。这样一来，二维位姿的预测就可以从最终预测中编码的三维信息中受益。这种方法分为两个阶段。首先，利用前一阶段获得的信念图来预测一组更新的二维人体关节位置的信念图。其次，将基于cnn的信念图输出作为输入到新层，该层使用预先训练的概率3D人体姿态模型，将提出的2D姿态提升到3D。这种方法为我们提供了更高质量的3D姿态估计。

然后我们将这种方法从图像扩展到视频，对每一帧进行独立处理。我们在下面给出了这种方法的结果。

## Achievements

我们实现了一个深入的RL开发环境和许多用于深入研究RL的工具。我们培训了专家的RL政策，并从这些专家那里培训了GAIL政策。我们也实施了两种方法，从视频人体姿态重建，使用三个预先训练的网络从文献。我们计划将来继续这项研究。

使用Docker实现了深度RL训练环境，支持gpu、OpenGL可视化、TensorFlow、MuJoCo、Roboschool、木星笔记本等许多工具。该环境可以在Mac和Linux上运行，并且很容易与云计算提供商一起使用。

- 学会了如何使用谷歌云同时在多个cpu上进行培训
- 在MuJoCo和Roboschool创建了自定义的人形骨骼和训练环境
- 实现了动作捕捉数据解释器(针对ASF和AMC文件)和时间序列重采样管道
- 使用TRPO在自定义骨架上训练类人行走政策
- 从一个基于trpo的专家那里培训了GAIL行走策略
- 用JavaScript实现了一个具有特性覆盖的类人策略展示可视化工具
- 收集了人类行走的视频演示
- 利用文献中两个预先训练好的网络，尝试了视频→2D姿态→3D姿态管道方法从视频中进行人体姿态估计
- 利用文献中预先训练的网络，尝试了端到端视频→3D方法从视频中进行人体姿态估计
- 以RL策略为专家，对GAIL目标类人、策略网络形状和rollout特征表示进行了实验，得到了一个稳定的GAIL

## Lessons Learned

- 大多数深度RL软件都是非常年轻的，而且往往很脆弱，或者只适合一种环境(例如MuJoCo和TRPO)。
- GAIL，像所有的GAN一样，往往非常不稳定，很难训练
- 摄入外部数据源需要在解释和预处理数据方面做大量工作
- 管道方法更容易理解和训练，但管道中的每一步都会创建一个故障模式。端到端的方法允许学习者通过管道反向传播故障来补偿，并且可以更加健壮。
- Merel等人给出的类人模型的GAIL公式对于目标类人模型和为推广而选择的特征表示来说非常脆弱。这是未来工作的机会。
- 使用像humanoid这样的高维代理来训练深度RL策略需要非常大量的基于cpu的计算。

## 结论与未来工作

我们在这个项目中的目标是熟悉最先进的深度RL软件和研究技术，同时重新实现之前的工作，为未来的研究奠定基础。虽然我们并没有完全复制游戏的结果，但我们确实实现了另外两个目标。我们现在有一个深入的RL培训环境和工作流程，以及几个未来的研究方向，我们希望追求和发表在同行评议的论坛。

未来可能的研究方向包括:

- 利用一个模拟专家预先训练的盖尔网络从视频中进行一次模仿学习
- 学习专家演示的特征表示，以避免对每个新目标形态学进行特征工程
- 探索对于时域运动数据更有效的架构和展出表示(如傅立叶系数等)
- 使用GAIL作为多任务模仿学习的基础
