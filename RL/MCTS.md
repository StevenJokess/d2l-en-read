# 基于模拟的搜索与蒙特卡罗树搜索(MCTS)

什么是基于模拟的搜索呢？当然主要是两个点：一个是模拟，一个是搜索。模拟我们在上一篇也讨论过，就是基于强化学习模型进行采样，得到样本数据。但是这是数据不是基于和环境交互获得的真实数据，所以是“模拟”。对于搜索，则是为了利用模拟的样本结果来帮我们计算到底应该采用什么样的动作，以实现我们的长期受益最大化。

那么为什么要进行基于模拟的搜索呢？在这之前我们先看看最简单的前向搜索(forward search)。前向搜索算法从当前我们考虑的状态节点St开始考虑，怎么考虑呢？对该状态节点所有可能的动作进行扩展，建立一颗以St为根节点的搜索树，这个搜索树也是一个MDP，只是它是以当前状态为根节点，而不是以起始状态为根节点，所以也叫做sub-MDP。我们求解这个sub-MDP问题，然后得到St状态最应该采用的动作At。前向搜索的sub-MDP如下图：

前向搜索建立了一个sub-MDP来求解，这很精确，而且这在状态动作数量都很少的时候没有问题，但是只要稍微状态动作数量多一点，每个状态的选择就都特别慢了，因此不太实用，此时基于模拟的搜索就是一种比较好的折衷。

## MCTS小结

MCTS通过采样建立MCTS搜索树，并基于4大步骤选择，扩展，仿真和回溯来持续优化树内的策略，进而可以帮助对状态下的动作进行选择，非常适合状态数，动作数海量的强化学习问题。比如AlphaGo和AlphaGo Zero都重度使用了MCTS搜索，我们在下一篇讨论AlphaGo Zero如何结合MCTS和神经网络来求解围棋强化学习问题。
