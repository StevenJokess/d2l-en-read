

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-15 13:37:37
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-09-19 18:52:19
 * @Description:MT,improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/similarity-analogy.html
-->

# 寻找同义词和类比

在14.4节中，我们在一个小规模数据集上训练了 word2vec 单词嵌入模型，并使用单词向量的余弦距离搜索同义词。实际上，在大规模语料库上预先训练的词向量经常可以应用于下游的自然语言处理任务。本节将演示如何使用这些预先训练的词向量来查找同义词和类比。我们将继续在随后的章节中应用预先训练的单词向量。

TODO:CODE

## 使用预先训练的词向量

下面列出了预先培训过的50,100和300尺寸的 GloVe 嵌入，可以从 [GloVe 网站](https://nlp.stanford.edu/projects/glove/)下载。预先培训的 fastText 嵌入可用于多种语言。这里我们考虑一个可以从 [fastText 网站](https://fasttext.cc/)下载的英文版本(300维的“ wiki.en”)。

TODO:CODE

我们定义下面的 `TokenEmbedding` 类来加载上面的预训练的 Glove 和 fastText 嵌入。

TODO:CODE

接下来，我们使用预先在 Wikipedia 的一个子集上训练的50维 GloVe 嵌入。在我们第一次创建一个经过预训练的嵌入词实例时，自动下载相应的嵌入词。

TODO:CODE

输出字典大小。 该词典包含$400,000$个单词和一个特殊的未知标记。

TODO:CODE

我们可以使用一个词在字典中得到它的索引，或者我们可以从它的索引中得到这个词。

TODO:CODE

# 应用预先训练好的词向量

下面，我们以GloVe为例，演示预先训练好的词向量的应用。

## 找到同义词

在这里，我们重新实现14.1节中介绍的通过余弦相似度搜索同义词的算法

为了在寻找类比时重用寻找kk最近邻的逻辑，我们将这部分逻辑单独封装在knn (k-最近邻)函数中。

TODO:CODE

然后，我们通过预训练embed的单词向量实例来搜索同义词。

TODO:CODE

已经创建的预先训练好的单词向量实例`glove_6b50d`字典包含40万个单词和一个特殊的未知标记。排除输入词和未知词，我们搜索与chip意思最相似的三个词。

接下来，我们搜索“baby”和“beautiful”的同义词。

TODO:CODE

TODO:CODE

## 发现类比

除了查找同义词之外，我们还可以使用预先训练好的单词向量来查找单词之间的类比。例如， “man”:“woman”::“son”:“daughter” 是一个类比的例子， “man” 之于 “woman” 就像 “son” 之于“daughter”。寻找类比的问题可以这样定义:对于类比关系`a:b::c:d`中的四个单词，给定前三个单词`a`、`b`和`c`，我们要找到d。假设单词ww的单词向量是`vec(w)`。为了解决类比问题，我们需要找到与`vec(c)+vec(b)−vec(a)`的结果向量最相似的单词vector。

TODO:CODE

验证一下“male-female”的类比吧。

TODO:CODE

“首都-国家”类比:“北京”之于“中国”，就像“东京”之于什么?答案应该是“日本”。

TODO:CODE

“形容词-最高级”的类比:“bad”之于“worst”就像“big”之于什么?答案应该是“最大的”。

TODO:CODE

“现在时动词-过去时动词”类比:“do”是“did”，就像“go”是什么?答案应该是“去”。

TODO:CODE

## 总结

* 在大规模语料库上预先训练的词向量通常可以应用于下游自然语言处理任务。
* 我们可以使用预先训练好的词向量来寻找同义词和类比。

## 练习

1. 使用`TokenEmbedding('wiki.en')`测试fastText结果。
1. 如果字典非常大，我们如何加快查找同义词和类推?
