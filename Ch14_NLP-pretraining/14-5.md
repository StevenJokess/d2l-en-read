

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-15 11:54:01
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-21 22:57:26
 * @Description:MT, add math half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/glove.html
 * https://github.com/d2l-ai/d2l-en/blob/master/chapter_natural-language-processing-pretraining/glove.md
-->

# 全局向量的字嵌入(GloVe)

首先，我们应该回顾 word2vec 中的跳图模型。用流程图模型表示的条件概率 $P(w_j\mid w_i)$将被记录为 $q_{ij}$，即:

$$q_{ij}=\frac{\exp(\mathbf{u}_j^\top \mathbf{v}_i)}{ \sum_{k \in \mathcal{V}} \text{exp}(\mathbf{u}_k^\top \mathbf{v}_i)},$$

其中 $\mathbf{v}_i$ 和 $\mathbf{u}_i$ 分别作为索引 i 的中心词和上下文词的向量表示，$\mathcal{V} = \{0, 1, \ldots, |\mathcal{V}|-1\}$为词汇索引集。

对于word $w_i$，它可能在数据集中出现多次。当$w_i$是中心词时，我们每次收集所有上下文词并保留重复项，表示为multiset $\mathcal{C}_i$。一个多集合中元素的数量称为元素的多重性。例如，假设单词$w_i$在数据集中出现两次:当这两个$w_i$成为文本序列中包含上下文单词索引$ 2,1,5,2 $和$ 2,3,2,1 $的中心单词时，上下文窗口。然后multiset $\mathcal{C}_i = \{1,1,2,2,2,2,2,3,5 \}$，其中元素1的多重性为2，元素2的多重性为4，元素3和元素5的多重性均为1。表示元素$j$在multiset $\mathcal{C}_i$作为$x_{ij}$中的多重性:它是整个数据集中的中心字$w_i$在所有上下文窗口中的字$w_j$。因此，跃函数模型的损失函数可以有不同的表达方式:

$$-\sum_{i\in\mathcal{V}}\sum_{j\in\mathcal{V}} x_{ij} \log\,q_{ij}.$$

我们将中心目标词$w_i$加起来得到 $x_i$，然后记录条件概率 $x_{ij}/x_i$，根据中心目标词 $w_i$ as $p_{ij}$ 生成上下文词 $w_j$ 。我们可以重写跳图模型的损失函数

$$-\sum_{i\in\mathcal{V}} x_i \sum_{j\in\mathcal{V}} p_{ij} \log\,q_{ij}.$$

在上式中，$\sum_{j\in\mathcal{V} p_{ij} \log\中，q_{ij}$计算基于中心目标词$w_i$和模型预测的条件概率分布$q_{ij}$的交叉熵生成上下文词的条件概率分布$p_{ij}$。损失函数使用上下文单词数量和中心目标单词$w_i$的总和进行加权。如果我们将上述公式中的损失函数最小化，我们就能够让预测的条件概率分布尽可能接近真实的条件概率分布。

然而，虽然最常见的损失函数类型，交叉熵损失函数有时不是一个好的选择。一方面，正如我们在第14.2节中提到的，让模型预测 qij qij 成为法律概率分布的成本是整个词典所有项目的总和的分母。这很容易导致过多的计算开销。另一方面，字典中常常有许多不常用的词，而且它们很少出现在数据集中。在交叉熵损失函数中，条件概率分布对大量不常用词的最终预测可能是不准确的。

## GloVe 模型

为了解决这个问题，紧随 word2vec 之后的一个词嵌入模型 GloVe [ Pennington et al. 2014]采用了平方损失，并根据这一损失对跳字模型做了三处修改。

1. 这里，我们使用非概率分布变量$p'_{ij}=x_{ij}$和$q'_{ij}=\exp(\mathbf{u}_j^\top \mathbf{v}_i)$并取它们的日志。因此,我们得到了平方损失$\left(\log\,p'_{ij} - \log\,q'_{ij}\right)^2 = \left(\mathbf{u}_j^\top \mathbf{v}_i - \log\,x_{ij}\right)^2$。
2. 我们为每个单词$w_i$添加两个标量模型参数:偏差项$b_i$(用于中心目标单词)和$c_i$(用于上下文单词)。
3. 用函数$h(x_{ij})$替换每个损失的权重。权重函数$h(x)$是一个单调递增函数，其范围为$[0,1]$。

因此，GloVe的目标是使损失函数最小化。

$$\sum_{i\in\mathcal{V}} \sum_{j\in\mathcal{V}} h(x_{ij}) \left(\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j - \log\,x_{ij}\right)^2.$$

但是，尽管损失函数是最常见的类型，但是交叉熵损失函数有时并不是一个很好的选择。 一方面，正如我们在第14.2节中提到的那样，让模型预测qijqij成为合法概率分布的代价是整个词典中所有分母的总和。 这很容易导致过多的计算开销。 另一方面，字典中经常有很多不常见的单词，并且很少出现在数据集中。 在交叉熵损失函数中，对大量不常见单词的条件概率分布的最终预测可能不准确。

注意，如果word $w_i$出现在word $w_j$的上下文窗口中，那么word $w_j$也将出现在word $w_i$的上下文窗口中。因此,$x_{ij}=x_{ji}$。与word2vec不同，GloVe适用于对称的$\log\, x_{ij}$代替了不对称的条件概率$p_{ij}$。因此，任何词的中心目标词向量和上下文词向量在很大程度上是等价的。但是，由于初始化值不同，同一单词学习的两组单词向量最终可能会有所不同。在学习了所有的词向量之后，GloVe将使用中心目标词向量和上下文词向量的总和作为该词的最终词向量。



## 从条件概率比率来理解

我们也可以试着从另一个角度来理解手套词的嵌入。我们将继续使用本节前面的符号，$P(w_j \mid w_i)$表示在数据集中使用中心目标词$w_i$生成上下文词$w_j$的条件概率，它将被记录为$p_{ij}$。从一个大型语料库的实际例子中，我们得到了以下以“ice”和“steam”为中心目标词的两组条件概率以及它们之间的比率:

|$w_k$=|solid|gas|water|fashion|
|--:|:-:|:-:|:-:|:-:|
|$p_1=P(w_k\mid \text{ice})$|0.00019|0.000066|0.003|0.000017|
|$p_2=P(w_k\mid\text{steam})$|0.000022|0.00078|0.0022|0.000018|
|$p_1/p_2$|8.9|0.085|1.36|0.96|

我们可以观察到下列现象:

* 对于与“ice”相关但与“steam”不相关的单词wkwk，比如wk=wk=“solid”，我们希望有更大的条件概率比，比如上表最后一行中的值8.9。
* 对于与“蒸汽”相关但与“冰”无关的单词wkwk，例如wk=“gas”，我们期望一个较小的条件概率比，如上表最后一行中的值0.085。
* 对于同时与“冰”和“蒸汽”有关的单词wkwk，例如wk=wk=“水”，我们期望条件概率比接近1，如上表最后一行中的值1.36。
* 对于既不与“ice”也不与“steam”相关的单词wkwk，例如wk=wk=“fashion”，我们期望条件概率比接近1，如上表最后一行中的值0.96。

我们可以看到，条件概率比可以更直观地表示不同单词之间的关系。 我们可以构造一个词向量函数来更有效地拟合条件概率比。 众所周知，要获得这种比率，需要三个词wi，wj和wk。 以wi为中心目标词的条件概率比是pij / pik。 我们可以找到一个使用单词向量来拟合此条件概率比的函数。

功能$f$的可能设计在这里将不是唯一的。我们只需要考虑一个更合理的可能性。注意,条件概率比是一个标量,我们可以限制f是一个标量函数:$f(\mathbf{u}_j, \mathbf{u}_k, {\mathbf{v}}_i) = f\left((\mathbf{u}_j - \mathbf{u}_k)^\top {\mathbf{v}}_i\right)$。与 $k$交换指数 $j$ 之后,我们将能够看到,函数 $f$ 满足条件 $f(x)f(-x)=1$,所以一种可能性可以$f(x)=\exp(x)$。因此:

$$f(\mathbf{u}_j, \mathbf{u}_k, {\mathbf{v}}_i) = \frac{\exp\left(\mathbf{u}_j^\top {\mathbf{v}}_i\right)}{\exp\left(\mathbf{u}_k^\top {\mathbf{v}}_i\right)} \approx \frac{p_{ij}}{p_{ik}}.$$

一种可能满足的右侧近似符号是$\exp\left(\mathbf{u}_j^\top {\mathbf{v}}_i\right) \approx \alpha p_{ij}$,在$\alpha$ 是一个常数。考虑到$p_{ij}=x_{ij}/x_i$,取对数后我们得到了$\mathbf{u}_j^\top {\mathbf{v}}_i \approx \log\,\alpha + \log\,x_{ij} - \log\,x_i$。我们使用附加的偏置术语来拟合 $- \log\, \alpha + \log\, x_i$,如中心目标词偏置术语 $b_i$和上下文词偏置术语 $c_j$:

$$\mathbf{u}_j^\top \mathbf{v}_i + b_i + c_j \approx \log(x_{ij}).$$

对上述公式的左右两边取平方误差加权，得到GloVe的损失函数。

## 总结

* 在某些情况下，交叉熵损失函数可能有缺点。手套使用平方损失和单词向量来拟合基于整个数据集预先计算的全局统计信息。
* 任何词的中心目标词向量和上下文词向量在理论上是等价的。

## 练习

1. 如果一个单词出现在另一个单词的上下文窗口中，我们如何使用它们在文本序列中的距离来重新设计计算条件概率pijpij的方法?提示:见4.2节论文GloVe :cite:`Pennington.Socher.Manning.2014`。
1. 对于任何一个词，其中心目标词偏颇词和上下文词偏颇词是否完全相等?为什么?

[1]: https://nlp.stanford.edu/projects/glove/
