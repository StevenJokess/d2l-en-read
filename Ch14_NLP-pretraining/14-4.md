

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-08-15 11:43:07
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-15 13:17:57
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/word2vec-pretraining.html
-->

# 预训 word2vec

在本节中，我们将训练在14.1节中定义的跳图模型。
首先，导入实验所需的包和模块，并加载 PTB 数据集。

## skip-gram模型

我们将采用嵌入层和小批量乘法实现跳过skip-gram模型。这些方法还常常用于实现其他自然语言处理应用程序。

### 嵌入层

将获得的字嵌入其中的层称为嵌入层，可以通过创建 nn 来获得嵌入层。在 Gluon 中嵌入实例。嵌入层的权重是一个矩阵，其行数为字典大小(`input_dim`) ，列数为每个字向量维数(`output_dim`)。我们将字典大小设置为20，并将单词向量维度设置为4。

TODO:CODE

嵌入层的输入是词的索引。当我们输入一个词的索引 i 时，嵌入层返回权矩阵的第 i 行作为它的词向量。下面我们在嵌入层中输入一个形状索引(2,3)。由于词向量的维数是4，我们得到一个形状的词向量(2,3,4)。

TODO:CODE

### 小批量乘法

我们可以通过minibatch乘法运算batch_dot将矩阵在两个minibat中一一相乘。 假设第一批包含nn个形状为a×ba×b的矩阵X1，...，XnX1，...，Xn，第二批包含nn个形状为b×cb×的矩阵Y1，...，YnY1，...，Yn C 。 在这两个批次上的矩阵乘法输出为nn个矩阵，形式为a×cc。X1Y1，...，XnYnX1Y1，...，XnYn。 因此，给定两个形状为（n，a，b）和（n，b，c）的张量，小批量乘法输出的形状为（n，a，c）。

### Skip-gram模型

在前向计算中，skip-gram模型的输入包含中心目标词索引中心和连接的上下文和噪声词索引`contexts_and_negatives`。其中，center变量具有形状(batch size，1)，而contexts_and_negative变量具有形状(batch size，`max_len`)。首先通过单词嵌入层将这两个变量从单词索引转换为单词向量，然后通过minibatch乘法得到shape (batch size, 1, `max_len`)的输出。输出中的每个元素都是中心目标词向量和上下文词向量或噪声词向量的内积。

验证输出形状应为 (batch size, 1, max_len)

TODO:CODE

## 训练

在训练单词嵌入模型之前，我们需要定义模型的损失函数。

### 二元交叉熵损失函数

根据负采样中损失函数的定义，我们可以直接使用Gluon的二进制交叉熵损失函数`SigmoidBinaryCrossEntropyLoss`。

TODO:CODE

值得一提的是,我们可以使用面具变量来指定部分预测值和标签参与minibatch损失函数计算:当面具是1,对应位置的预报值和标签将参与计算的损失函数;当掩码为0时，相应位置的预测值和标签不参与损耗函数的计算。如前所述，可以使用掩码变量来避免填充对损失函数计算的影响。

对于两个相同的例子，不同的掩模导致不同的损失值。

TODO:CODE

我们可以对每个例子中由于不同长度造成的损失进行归一化

TODO:CODE

### 初始化模型参数

我们分别构造中心词和上下文词的嵌入层，并将超参数词向量维数`embed_size`设置为100。

TODO:CODE

### 训练

训练功能定义如下。 由于存在填充，因此损失函数的计算与以前的训练函数相比略有不同。

TODO:CODE

现在，我们可以使用负采样来训练跳跃图模型。

TODO:CODE

### 应用词嵌入模型

经过对单词嵌入模型的训练，我们可以根据两个单词向量的余弦相似度来表示单词之间的语义相似度。我们可以看到，在使用经过训练的单词嵌入模型时，与chip意思最接近的单词大多与chip有关。

TODO:CODE

## 总结

我们可以通过负采样来预训练跳图模型。

## 练习

1. 当创建`nn.Embedding`的实例时，设置`sparse_grad=True`。它能加速训练吗?查阅MXNet文档了解此参数的含义。
1. 试着找出其他单词的同义词。
1. 调优超参数，观察和分析实验结果。
1. 当数据集较大时，我们通常只在更新模型参数时对当前小批量的中心目标词抽取上下文词和噪声词。换句话说，同一个中心目标词在不同的时代可能会有不同的上下文词或噪声词。这种训练的好处是什么?试着实施这个训练方法。
