

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-15 14:33:14
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-14 18:26:07
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/bert-dataset.html
-->

# 预训练 BERT 的数据集

为了预先训练在14.8节中实现的 BERT 模型，我们需要以理想的格式生成数据集，以方便两个预先训练任务: 屏蔽语言建模和下一句子预测。一方面，原始的 BERT 模型是基于两个巨大的语料库 BookCorpus 和英文维基百科(见14.8.5节)的连接进行预先训练的，这使得本书的大多数读者很难运行它。另一方面，现成的预先训练的 BERT 模型可能不适合医学等特定领域的应用。因此，在定制的数据集上预先训练 BERT 变得越来越流行。为了方便 BERT 预训练的演示，我们使用了一个较小的 WikiText-2语料库。

与14.3节中用于 word2vec 预训练的 PTB 数据集相比，WikiText-2 i)保留了原来的标点符号，使其适合于下一句预测; ii)重新训练原来的大小写和数字; iii)大了两倍多。

TODO:CODE

在 WikiText-2数据集中，每一行代表一个段落，在该段落中，任何标点符号及其前面的标记之间都插入了空格。保留至少两句话的段落。为了简单起见，我们只使用句号作为分隔符来拆分句子。我们将在本节结尾的练习中讨论更复杂的句子分割技巧。

TODO:CODE

## 为预训练任务定义辅助功能

在下面，我们开始实现两个 BERT 预训练任务的辅助函数: 下一个句子预测和屏蔽语言建模。稍后，当将原始文本语料转换为理想格式的数据集以预训练 BERT 时，将调用这些辅助函数。

### 生成下一个句子预测任务

根据…的描述_subsec_nsp:，`_get_next_sentence`函数为二进制分类任务生成一个训练示例。

TODO:CODE

下面的函数通过调用`_get_next_sentence`函数，为从输入段落中预测下一个句子生成训练示例。在这里，段落是一个句子列表，其中每个句子是一个标记列表。参数`max_len`指定了预训练期间BERT输入序列的最大长度。

TODO:CODE

### 生成掩码语言建模任务

为了从一个BERT输入序列为掩码语言建模任务生成训练示例，我们定义了以下_replace_mlm_tokens函数。在其输入,令牌是一个列表的令牌代表伯特输入序列,candidate_pred_positions是令牌列表伯特输入序列的指数不包括那些特殊令牌(特殊令牌不是在掩盖语言建模预测任务),和num_mlm_preds表明预测的数量(记得15%随机标记预测)。根据14.8.5.1节中蒙面语言建模任务的定义，在每个预测位置，输入可以被一个特殊的“<mask>”令牌或一个随机的令牌代替，或者保持不变。最后，该函数返回可能替换后的输入标记、发生预测的标记索引以及这些预测的标签。

TODO:CODE

通过调用前面提到的`_replace_mlm_tokens`函数，下面的这个函数接受一个BERT输入序列(tokens)作为输入，并返回输入令牌的索引(如14.8.5.1节所述，在可能的令牌替换之后)、发生预测的令牌索引，以及为这些预测标注索引。

TODO:CODE

## 将文本转换为预训练数据集

现在，我们几乎已经准备好定制一个数据集类来预训练BERT。在此之前，我们仍然需要定义一个助手函数`_pad_bert_input`，以将特殊的“<pad>”标记附加到输入中。它的参数示例包含帮助函数`_get_nsp_data_from_`段落和`_get_mlm_data_from_tokens`对两个预训练任务的输出。

TODO:CODE

将用于生成两个预训练任务的训练示例的辅助函数以及用于填充输入的辅助函数放在一起，我们将以下_WikiTextDataset类自定义为用于预训练BERT的WikiText-2数据集。 通过实现__getitem__函数，我们可以任意访问从WikiText-2语料库的一对句子生成的预训练（屏蔽语言建模和下一个句子预测）示例。

最初的BERT模型使用的词汇量为30,000的WordPiece嵌入[Wu等，2016]。 WordPiece的标记化方法是对14.6.2节中原始字节对编码算法的略微修改。 为简单起见，我们使用d2l.tokenize函数进行令牌化。 出现少于五次的不频繁标记将被滤除。

TODO:CODE

通过使用`_read_wiki`函数和`_WikiTextDataset`类，我们定义了以下load_data_wiki来下载和WikiText-2数据集，并从中生成预训练示例。

TODO:CODE

将批大小设置为512，并将BERT输入序列的最大长度设置为64，我们将打印出一小批BERT预训练示例的形状。注意，在每个BERT输入序列中，为掩码语言建模任务预测了10(64×0.15)个位置。

TODO:CODE

最后，让我们看看词汇量的大小。即使在过滤掉不常见的令牌之后，它仍然比PTB数据集大两倍以上。

TODO:CODE

## 总结

* 与PTB数据集相比，WikiText-2数据集保留了原始的标点符号、大小写和数字，并且大于原来的两倍。
* 我们可以任意访问由WikiText-2语料库中的一对句子生成的预训练(掩蔽语言建模和下一句预测)示例。

## 练习

1. 为简单起见，句点用作分隔句子的唯一分隔符。尝试其他的句子分割技术，比如spaCy和NLTK。以NLTK为例。首先需要安装NLTK: `pip install nltk`。在代码中，首先导入nltk。然后，下载Punkt句子标记器:`nltk.download(' Punkt ')`。拆分句子，比如`sentences = 'This is great ! Why not ?'`，调用nltk.tokenize.sent_tokenize(sentence)将返回两个句子字符串的列表:`['This is great !'， 'Why not ?']`。
1. 如果我们不过滤掉任何不常见的标记，那么词汇表的大小是多少?
