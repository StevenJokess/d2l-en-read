

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-15 14:11:46
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-09-19 19:00:31
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/bert.html
 * https://easyai.tech/ai-definition/bert/
 * https://github.com/d2l-ai/d2l-en/tree/master/chapter_natural-language-processing-pretraining/bert.md
-->

# 双向Transformer的Encoder--BERT(Bidirectional Encoder Representation from Transformers)
:label:`sec_bert`

本文介绍了几种自然语言理解的嵌入词模型。在预训练之后，输出可以看作是一个矩阵，其中每一行都是一个向量，表示预定义词汇表中的一个单词。事实上，这些嵌入单词的模型都是上下文无关的。让我们先举例说明这个性质。

## 从语境无关到语境敏感

回忆:numref:`sec_word2vec_pretraining` 和 :numref:`sec_synonyms`中的实验。例如，word2vec 和 GloVe 都为同一个单词指定了相同的预训练向量，而与单词的上下文无关(如果有的话)。在形式上，任何标记 x的上下文无关表示都是函数 f (x) ，它只接受x作为输入。考虑到自然语言中丰富的多义性和复杂的语义，与上下文无关的表示方法具有明显的局限性。例如，上下文中的“crane”和“a crane driver came”有完全不同的意思; 因此，同一个词可能根据上下文分配不同的表示。

这促进了上下文敏感词语表征的发展，其中词语表征取决于它们的上下文。因此，令牌 x 的上下文敏感表示是函数 f (x，c (x)) ，取决于 x 及其上下文 c (x)。流行的上下文敏感的表示包括 TagLM (语言模型-增强的序列标记器):cite:`Peters.Ammar.Bhagavatula.ea.2017`，CoVe (上下文向量) :cite:`McCann.Bradbury.Xiong.ea.2017` ，ELMo (来自语言模型的嵌入) :cite:`Peters.Neumann.Iyyer.ea.2018`。

例如，通过将整个序列作为输入，ELMo 是一个为输入序列中的每个单词分配表示形式的函数。具体地说，ELMo 组合了来自预先训练的双向 LSTM 的所有中间层表示作为输出表示。然后，ELMo 表示将作为附加特性添加到下游任务的现有监督模型中，例如通过连接 ELMo 表示和现有模型中标记的原始表示(例如，GloVe)。一方面，加入 ELMo 表示后，预先训练的双向 LSTM 模型中的所有权重被冻结;。另一方面，现有的监督模型是专门为给定的任务定制的。利用当时不同任务的不同最佳模型，添加 ELMo 改善了六种自然语言处理任务的最新状态: 情感分析、自然语言推理、语义角色标注、共指解析、命名实体识别和问题回答。

## 从任务特定到任务不可知

虽然 ELMo 已经显著改进了针对各种自然语言处理任务的解决方案，但每个解决方案仍然依赖于特定于任务的体系结构。然而，为每个自然语言处理任务制定一个特定的体系结构实际上并非易事。GPT (生成性预训练)模型代表了为上下文相关表示设计一个通用任务不可知模型的努力[ Radford 等，2018]。GPT 构建在 Transformer 解码器上，预先训练了一个将用于表示文本序列的语言模型。当将 GPT 应用于下游任务时，语言模型的输出将被反馈到一个增加的线性输出层，以预测任务的标签。与冻结预训练模型参数的 ELMo 形成鲜明对比的是，GPT 在下游任务的监督式学习一个小时内，对预训练变压器解码器中的所有参数进行微调。在自然语言推理、问题回答、句子相似性和分类十二个任务上对 GPT 进行了评估，并且在模型结构的最小改变的情况下提高了其中9个任务的技术水平。

然而，由于语言模型的自回归性质，GPT 只能从左向右展望。在上下文中，“i went to the bank to deposit cash”和“ i went to the bank to sit down” ，由于“ bank”对左边的上下文很敏感，GPT 将返回“ bank”的相同表示，尽管它有不同的含义。

## BERT: 结合两个世界的优点

正如我们所看到的，ELMo 双向编码上下文，但使用特定于任务的体系结构; 而 GPT 与任务无关，但从左到右编码上下文。结合两个世界的最佳，BERT (来自变压器的双向编码表示)双向编码上下文，需要最小的架构变化为广泛的自然语言处理任务[德夫林等人，2018]。使用一个预先训练的变压器编码器，BERT 能够表示任何令牌基于它的双向上下文。在下游任务的监督式学习，BERT 在两个方面类似于 GPT。首先，BERT 表示将被提供到一个额外的输出层，根据任务的性质对模型架构进行最小的更改，例如对每个令牌进行预测和对整个序列进行预测。其次，所有参数的预训练变压器编码器是微调，而额外的输出层将训练从头开始。图14.8.1描绘了 ELMo、 GPT 和 BERT 之间的区别。

[比较ELMo、GPT和BERT](../img/elmo-gpt-bert.svg)
:label:`fig_elmo-gpt-bert`

BERT进一步提高了11个自然语言处理任务的艺术水平，这些任务包括:1)单一文本分类(如情绪分析)、2)文本对分类(如自然语言推理)、3)问题回答、4)文本标注(如命名实体识别)。从上下文敏感的ELMo到任务不确定的GPT和BERT，这些都是在2018年提出的，对自然语言的深层表示进行概念上简单但经验上强大的预训练，彻底改变了对各种自然语言处理任务的解决方案。

在本章的剩余部分，我们将深入了解BERT的预培训。在第15节中解释自然语言处理应用程序时，我们将说明BERT对下游应用程序的微调

TODO:CODE

## 输入表示

在自然语言处理中，有些任务(如情绪分析)以单个文本作为输入，而在其他任务(如自然语言推理)中，输入是一对文本序列。BERT输入序列明确地表示单个文本和文本对。在前者中，BERT输入序列是特殊分类令牌“<cls>”、文本序列的令牌和特殊分离令牌“<sep>”的串联。后者中，BERT输入序列为“<cls>”、第一个文本序列的令牌、“<sep>”、第二个文本序列的令牌、“<sep>”的串联。我们将始终将术语“BERT输入序列”与其他类型的“序列”区分开来。例如，一个BERT输入序列可能包括一个文本序列或两个文本序列。

为了区分文本对，分别将已学习片段嵌入eA和eB添加到第一个序列和第二个序列的标记嵌入中。对于单个文本输入，只使用eA。

下面的`get_tokens_and_segment`接受一个句子或两个句子作为输入，然后返回BERT输入序列的标记及其对应的段id。

TODO:CODE

BERT选用变压器编码器作为其双向架构。在Transformer编码器中常见的是在BERT输入序列的每个位置添加位置嵌入。然而，与原始的Transformer编码器不同，BERT使用了可学习的位置嵌入。综上所述，从图14.8.2可以看出，BERT输入序列的嵌入是token嵌入、segment嵌入和positional嵌入的和。

图14.8.2 BERT输入序列的嵌入是令牌嵌入、段嵌入和位置嵌入的和。

下面的`BERTEncoder`类类似于10.3节中实现的`TransformerEncoder`类。与`TransformerEncoder`不同，`BERTEncoder`使用段嵌入和可学习的位置嵌入。

TODO:CODE

假设词汇表大小为10,000。为了演示BERTEncoder的前向推断，让我们创建它的一个实例并初始化它的参数。

TODO:CODE

我们将令牌定义为长度为8的2个BERT输入序列，其中每个令牌都是词汇表的索引。带有输入令牌的BERTEncoder的前向推断返回编码的结果，其中每个令牌都由一个向量表示，其长度由超参数num_hiddens预定义。这个超参数通常被称为Transformer编码器的隐藏尺寸(隐藏单元的数量)。

TODO:CODE

## 预训练任务

BERTEncoder的前向推理给出了输入文本的每个令牌以及插入的特殊令牌“<cls>”和“<seq>”的BERT表示。接下来，我们将使用这些表示来计算预训练BERT的损失函数。预训练包括以下两项任务:掩蔽语言建模和下一句预测。

### 掩盖语言建模

如第8.3节所述，语言模型会使用其左侧的上下文来预测令牌。 为了双向编码上下文以表示每个令牌，BERT会随机屏蔽令牌，并使用双向上下文中的令牌来预测掩码的令牌。 该任务称为屏蔽语言模型。

在此预训练任务中，将随机选择15％的令牌作为用于预测的屏蔽令牌。 为了通过使用标签来预测被屏蔽的令牌而不作弊，一种直接的方法是在BERT输入序列中始终将其替换为特殊的“ <mask>”令牌。 但是，人工特殊标记“ <mask>”将不会在微调中出现。 为了避免预训练和微调之间的这种不匹配，如果屏蔽了令牌以进行预测（例如，在“这部电影很棒”中选择了“屏蔽”并进行了预测，则在输入中将其替换为：

* 在80％的时间内有一个特殊的“<mask>”令牌（例如，“这部电影很棒”变成了“这部电影是<mask>”）；
* 在10％的时间内有一个随机令牌（例如，“这部电影很棒”变成“这部电影是饮料”）；
* 在10％的时间内未更改标签标记的情况（例如，“这部电影很棒”变为“这部电影很棒”）。

请注意，在15％的时间中有10％会插入随机令牌。 这种偶然的噪音促使BERT在其双向上下文编码中对掩盖的令牌（特别是当标签令牌保持不变时）的偏向较小。

在BERT预训练的掩码语言模型任务中，我们实现了以下`MaskLM类`来预测掩码令牌。该预测使用了一个隐藏层`MLP(self。MLP)`。在前向推理中，它接受两个输入:BERTEncoder的编码结果和用于预测的令牌位置。输出是这些位置的预测结果。

TODO:CODE

为了演示MaskLM的正向推理，我们创建它的实例mlm并对其进行初始化。回想一下，来自BERTEncoder前向推断的`encoded_X`表示2个BERT输入序列。我们将`mlm_position`定义为在`encoded_X`的BERT输入序列中预测的3个索引。mlm的正向推理返回encoded_X的所有掩码位置上的预测结果`mlm_Y_hat`。对于每个预测，结果的大小等于词汇表的大小。

利用掩模下预测令牌`mlm_Y_hat`的`ground truth`标签`mlm_Y`，可以计算出BERT预训练中掩模语言模型任务的交叉熵损失。

TODO:CODE

## 预测下一个句子

尽管掩码语言建模能够对表示单词的双向上下文进行编码，但它并不显式地对文本对之间的逻辑关系建模。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二值分类任务——下一个句子预测。在进行预训练生成句子对时，有一半的时间它们确实是标签为“True”的连续句子;而另一半时间则从标签为“False”的语料库中随机抽取第二句话。

下面的NextSentencePred类使用一个隐藏层MLP来预测第二个句子是否是BERT输入序列中第一个句子的下一个句子。由于变压器编码器中的自注意，特殊令牌“<cls>”的BERT表示对输入的两个句子进行编码。因此MLP分类器的输出层(self.output)以X为输入，其中X是MLP隐含层的输出，MLP隐含层的输入是经过编码的“<cls>”令牌。

TODO:CODE

我们可以看到，NextSentencePred实例的前向推断返回每个BERT输入序列的二进制预测。

TODO:CODE

两种二元分类的交叉熵损失也可以计算出来。

TODO:CODE

值得注意的是，上述两个预训练任务中的所有标签都可以从预训练语料库中轻松获得，无需人工标注。原来的BERT已经在图书语料库:cite:`Zhu.Kiros.Zemel.ea.2015`和英文维基百科的串联上进行了预先训练。这两个语料库非常庞大:分别有8亿字和25亿字。

## 把所有东西放在一起

在对BERT进行预训练时，最终的损失函数是掩蔽语言建模的损失函数和下一句预测的损失函数的线性组合。现在我们可以通过实例化三个类`BERTEncoder`、`MaskLM`和`NextSentencePred`来定义`BERTModel`类。前向推理返回已编码的BERT表示`encoded_X`、对掩码语言建模`mlm_Y_hat`的预测，以及对下一个句子的预测`nsp_Y_hat`。

TODO:CODE

## 总结

* 单词嵌入模型，如word2vec和GloVe，是与上下文无关的。他们把同样的预先训练过的向量分配给同样的单词，而不管这个单词的上下文(如果有的话)。他们很难很好地处理自然语言中的一词多义或复杂语义。
* 对于上下文敏感的单词表示，如ELMo和GPT，单词的表示依赖于它们的上下文。
* ELMo对上下文进行双向编码，但使用特定于任务的架构(然而，为每个自然语言处理任务编写特定的架构实际上并非易事);而GPT是任务不可知的，但从左到右编码上下文。
* BERT结合了这两个世界的优点:它双向编码上下文，并且对大量自然语言处理任务要求最小的架构更改。
* BERT输入序列的嵌入是令牌嵌入、段嵌入和位置嵌入的总和。
* BERT预训练包括两个任务:掩蔽语言建模和下一句预测。前者能够编码表示单词的双向上下文，而后者显式地对文本对之间的逻辑关系建模。

## 练习

1. 为什么BERT会成功?
1. 在其他条件相同的情况下，掩体语言模型比从左到右的语言模型需要更多还是更少的预训练步骤来收敛?为什么?
1. 在最初的BERT实现中，`BERTEncoder`中的位置前馈网络(通过`d2l.EncoderBlock`)和`MaskLM`中的全连接层都使用高斯误差线性单位(GELU) :cite:`Hendrycks.Gimpel.2016`作为激活函数。GELU和ReLU的区别研究。

---

https://easyai.tech/ai-definition/bert/

训练双向语言模型时以减小的概率把少量的词替成了Mask或者另一个随机的词。我个人感觉这个目的在于使模型被迫增加对上下文的记忆。

增加了一个预测下一句的loss。这个看起来就比较新奇了。??

是这个模型非常的深，12层，并不宽(wide），中间层只有1024，而之前的Transformer模型中间层有2048。
其次，对于Mask（遮挡）在语言模型上的应用，已经被Ziang Xie提出了（我很有幸的也参与到了这篇论文中）：[1703.02573] Data Noising as Smoothing in Neural Network Language Models。这也是篇巨星云集的论文：Sida Wang，Jiwei Li（香侬科技的创始人兼CEO兼史上发文最多的NLP学者），Andrew Ng，Dan Jurafsky都是Coauthor。
