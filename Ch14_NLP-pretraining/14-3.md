

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-30 20:06:35
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-05 19:10:45
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/word-embedding-dataset.html
-->

# 预训练词嵌入的数据集
:label:`sec_word2vec_data`

在这一节中，我们将介绍如何用负采样预处理:numref:`sec_approx_train` 中的数据集，并将其加载到 word2vec 训练的小批量中。我们使用的数据集是 [Penn Tree Bank (PTB)](https://catalog.ldc.upenn.edu/LDC99T42) ，这是一个小型但常用的语料库。它从《华尔街日报》的文章中采集样本，包括培训集、验证集和测试集。

首先，导入实验所需的软件包和模块。

TODO:CODE

## 读取和预处理数据集

此数据集已经被预处理。数据集的每一行充当一个句子。句子中的所有单词都用空格隔开。在单词嵌入任务中，每个单词都是一个标记。

TODO:CODE

接下来，我们构建一个词汇表，其中出现次数不超过10次的单词映射到一个“<unk>”标记中。注意，预处理的PTB数据还包含表示罕见词的“<unk>”令牌。

## 加载数据集

接下来，我们将带有令牌指标的语料库读入数据批次进行训练。

### 提取中心目标词和上下文词

我们使用与中心目标词距离不超过上下文窗口大小的词作为给定中心目标词的上下文词。下面的定义函数提取所有中心目标词及其上下文词。它均匀且随机地抽取一个整数作为上下文窗口大小，位于整数1和`max_window_size`(最大上下文窗口)之间。

TODO:CODE

接下来，我们创建一个包含两个分别由7个单词和3个单词组成的句子的人工数据集。 假定最大上下文窗口为2，并打印所有中心目标词及其上下文词。

TODO:CODE

我们将最大上下文窗口大小设置为5。下面将提取数据集中的所有中心目标词及其上下文词。

TODO:CODE

### 负抽样

我们使用负抽样进行近似训练。对于中心词和上下文词对，我们随机抽取KK个噪声词(实验中K=5)。根据Word2vec论文中提出的建议，噪声词采样概率P(w)是w的词频率与总词频率的提升比率为0.75[2]。

我们首先定义一个类来根据抽样权值绘制候选对象。它缓存一个10000大小的随机数字银行，而不是调用随机。每一次选择。

TODO:CODE

## 读入批次

我们从数据集中提取每个中心目标词的所有中心目标词`all_centers`，以及上下文词`all_contexts`和噪声词`all_negative`。我们将以随机的小批量读取它们。

在一小批数据中，ithith示例包括一个中心词及其对应的nini上下文词和mimi噪声词。由于每个示例的上下文窗口大小可能不同，因此上下文词和噪声词的总和ni+mini+mi将不同。在构建一个minibatch时，我们将每个示例的上下文词和噪声词连接起来，并添加0作为填充，直到连接的长度相同，即所有连接的长度为maxini+mimaxini+mi (max_len)。为了避免填充对损失函数计算的影响，我们构造了掩码变量掩码，其中的每个元素对应于上下文和噪声词contexts_negatives中的一个元素。当变量contexts_negatives中的元素是padding时，掩码变量掩码中位于相同位置的元素将为0。否则，它的值为1。为了区分正例和负例，我们还需要区分contexts_negatives变量中的上下文词和杂音词。根据掩码变量的构造，我们只需要创建一个与contexts_negatives变量具有相同形状的label变量label，并将与上下文单词(正例)对应的元素设置为1，其余设置为0。

接下来，我们将实现minibatch读取函数`batchify`。它的minibatch输入数据是一个长度为batch size的列表，其中每个元素包含中心目标词center、上下文词context和noise词`negative`。这个函数返回的minibatch数据符合我们需要的格式，例如，它包含mask变量。

TODO:CODE

构造两个简单例子：

TODO:CODE

我们使用刚才定义的`batchify`函数来指定`DataLoader`实例中的minibatch读取方法。

## 把所有东西放在一起

最后，我们定义`load_data_ptb`函数，该函数读取PTB数据集并返回数据迭代器。

TODO:CODE

让我们打印数据迭代器的第一个小批次。

TODO:CODE

## 小结

* Subsampling试图最小化高频词对单词嵌入模型训练的影响。
* 我们可以填充不同长度的示例来创建具有相同长度的示例的minibatch，并使用掩码变量来区分padding和non-padding元素，这样只有non-padding元素才会参与loss函数的计算。

## 练习

1. 我们使用`batchify`函数在DataLoader实例中指定minibatch读取方法，并在第一次批量读取时打印每个变量的形状。如何计算这些形状?
