

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-30 20:06:35
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-30 20:18:31
 * @Description:MT half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/word-embedding-dataset.html
-->

# 预训练词嵌入的数据集

在这一节中，我们将介绍如何用负采样预处理14.2节中的数据集，并将其加载到 word2vec 训练的小批量中。我们使用的数据集是 Penn Tree Bank (PTB) ，这是一个小型但常用的语料库。它从《华尔街日报》的文章中采集样本，包括培训集、验证集和测试集。

首先，导入实验所需的软件包和模块。

TODO:CODE

读取和预处理数据集

把所有东西放在一起

最后，我们定义load_data_ptb函数，该函数读取PTB数据集并返回数据迭代器。

TODO:CODE

让我们打印数据迭代器的第一个小批。

TODO:CODE

## 小结

Subsampling试图最小化高频词对单词嵌入模型训练的影响。

我们可以填充不同长度的示例来创建具有相同长度的示例的minibatch，并使用掩码变量来区分padding和non-padding元素，这样只有non-padding元素才会参与loss函数的计算。

## 练习

我们使用batchify函数在DataLoader实例中指定minibatch读取方法，并在第一次批量读取时打印每个变量的形状。如何计算这些形状?
