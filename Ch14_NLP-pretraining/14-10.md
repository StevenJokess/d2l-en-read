

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-15 14:46:54
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-07 13:24:37
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-pretraining/bert-pretraining.html
-->

# 预训练BERT
:label:`sec_bert-pretraining`

使用:numref:`sec_bert`中实现的BERT模型和:numref:`sec_bert-dataset`中从WikiText-2数据集生成的预训练示例，我们将在本节中对WikiText-2数据集进行BERT预训练。

TODO:CODE

首先，我们加载WikiText-2数据集作为小批量的预训练示例，用于掩码语言建模和下一个句子预测。批大小为512,BERT输入序列的最大长度为64。注意，在原来的BERT模型中，最大长度是512。

TODO:CODE

## 预训练BERT

原始的BERT有两个版本，具有不同的模型大小:cite:`Devlin.Chang.Lee.ea.2018`。 基本模型（$\text{BERT}_{\text{BASE}}$）使用具有768个隐藏单元（隐藏大小）和12个自动关注头的12层（变压器编码器块）。 大型模型（$\text{BERT}_{\text{LARGE}}$）使用24个图层，其中包含1024个隐藏单元和16个自动关注头。 值得注意的是，前者有1.1亿个参数，而后者有3.4亿个参数。 为了便于演示，我们定义了一个小BERT，它使用2层，128个隐藏单元和2个自我关注头。

TODO:CODE

在定义训练循环之前，我们先定义一个助手函数`_get_batch_loss_bert`。给定训练示例的分片，该函数计算掩蔽语言建模和下一个句子预测任务的损失。注意，BERT预训练的最终损失是掩蔽语言建模损失和下一句预测损失的总和。

TODO:CODE

下面的`train_bert`函数调用前面提到的两个辅助函数，定义在WikiText-2 (`train_iter`)数据集上预训练BERT(`net`)的过程。训练伯特要花很长时间。下面函数的输入`num_steps`指定了训练的迭代步骤数，而不是像`train_ch13`函数那样指定训练的epoch数(参见13.1节)。

TODO:CODE

我们可以在BERT预训练中绘制出掩蔽语言建模损失和下一句预测损失。

TODO:CODE

## 用BERT表示文本

在对BERT进行预训练之后，我们可以使用它来表示单个文本、文本对或其中的任何标记。下面的函数返回`tokens_a`和`tokens_b`中所有令牌的BERT(net)表示。

TODO:CODE

想想这句话"a crane is flying"。回想一下14.8.4节中讨论的BERT的输入表示。插入特殊令牌“&lt;cls&gt;”(用于分类)和“&lt;sep&gt;” (用于分离)后，BERT输入序列的长度为6。因为0是“&lt;cls&gt;”标记的索引，所以`encoded_text[:， 0，:]`是整个输入句子的BERT表示。为了计算一词多义标记“crane”，我们还打印标记的BERT表示的前三个元素。

TODO:CODE

现在让我们来考虑这样一对句子:“a crane driver came”和“he just left”。同样，`encoded_pair[:，0，:]`是来自预先训练好的BERT的整个句子对的编码结果。注意，当上下文不同时，一词多义标记“crane”的前三个元素与它们不同。这就支持了BERT表示是上下文敏感的。

TODO:CODE

在:numref:`chap_nlp_app`中，我们将为下游自然语言处理应用程序调整一个预先训练好的BERT模型。

## 总结

* 原BERT有两个版本，基本模型有1.1亿个参数，大模型有3.4亿个参数。
* 在对BERT进行预训练之后，我们可以使用它来表示单个文本、文本对或其中的任何标记。
* 在实验中，同一标记在不同的上下文下具有不同的BERT表示。这就支持了BERT表示是上下文敏感的。

## 练习

1. 在实验中我们可以看到，掩蔽语言建模损失明显高于下一句预测损失。为什么?
2. 将BERT输入序列的最大长度设置为512(与原始BERT模型相同)。使用原始BERT模型的配置，如$\text{BERT}_{\text{LARGE}}$。运行此部分时是否遇到任何错误?为什么?
