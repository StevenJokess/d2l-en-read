{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "similarity-analogy.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swb_rZrA28do",
        "colab_type": "text"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSRvppWm28dp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3dc93880-44cd-4f32-820e-820ff82a0393"
      },
      "source": [
        "!pip install -U mxnet-cu101==1.7.0\n",
        "!pip install d2l==0.14.4\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1MB 1.2MB/s eta 0:00:10tcmalloc: large alloc 1147494400 bytes == 0x64af2000 @  0x7f80c2eed615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
            "\u001b[K     |████████████████████████████████| 846.0MB 22kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (2.23.0)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n",
            "Collecting d2l==0.14.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/9b/7fd84dcfa6d4f51f5671ed581e749586beb6a7432804bcf28fde32df4df7/d2l-0.14.4-py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (3.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.0.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.7.7)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (7.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.10.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.14.4) (2018.9)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (1.0.18)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (0.2.0)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (4.6.3)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (19.0.2)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (1.9.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.14.4) (5.0.7)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.14.4) (3.5.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (1.5.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (0.8.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (2.11.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.4.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->d2l==0.14.4) (1.15.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (50.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.14.4) (0.2.5)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter->d2l==0.14.4) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->d2l==0.14.4) (1.1.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (20.4)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.14.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "AYTPwy4p28du",
        "colab_type": "text"
      },
      "source": [
        "# Finding Synonyms and Analogies\n",
        ":label:`sec_synonyms`\n",
        "\n",
        "In :numref:`sec_word2vec_pretraining` we trained a word2vec word embedding model\n",
        "on a small-scale dataset and searched for synonyms using the cosine similarity\n",
        "of word vectors. In practice, word vectors pretrained on a large-scale corpus\n",
        "can often be applied to downstream natural language processing tasks. This\n",
        "section will demonstrate how to use these pretrained word vectors to find\n",
        "synonyms and analogies. We will continue to apply pretrained word vectors in\n",
        "subsequent sections.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "3PFqc0UD28dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from d2l import mxnet as d2l\n",
        "from mxnet import np, npx\n",
        "import os\n",
        "\n",
        "npx.set_np()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "id": "5lMR6HBP28d0",
        "colab_type": "text"
      },
      "source": [
        "## Using Pretrained Word Vectors\n",
        "\n",
        "Below lists pretrained GloVe embeddings of dimensions 50, 100, and 300,\n",
        "which can be downloaded from the [GloVe website](https://nlp.stanford.edu/projects/glove/).\n",
        "The pretrained fastText embeddings are available in multiple languages.\n",
        "Here we consider one English version (300-dimensional \"wiki.en\") that can be downloaded from the\n",
        "[fastText website](https://fasttext.cc/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "35"
        },
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "2MajRcgh28d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip',\n",
        "                                '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip',\n",
        "                                 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip',\n",
        "                                  'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip',\n",
        "                           'c1816da3821ae9f43899be655002f6c723e91b88')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "JAgMpOkA28d5",
        "colab_type": "text"
      },
      "source": [
        "We define the following `TokenEmbedding` class to load the above pretrained Glove and fastText embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "erlWyz6e28d5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@save\n",
        "class TokenEmbedding:\n",
        "    \"\"\"Token Embedding.\"\"\"\n",
        "    def __init__(self, embedding_name):\n",
        "        self.idx_to_token, self.idx_to_vec = self._load_embedding(\n",
        "            embedding_name)\n",
        "        self.unknown_idx = 0\n",
        "        self.token_to_idx = {token: idx for idx, token in\n",
        "                             enumerate(self.idx_to_token)}\n",
        "\n",
        "    def _load_embedding(self, embedding_name):\n",
        "        idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "        data_dir = d2l.download_extract(embedding_name)\n",
        "        # GloVe website: https://nlp.stanford.edu/projects/glove/\n",
        "        # fastText website: https://fasttext.cc/\n",
        "        with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
        "            for line in f:\n",
        "                elems = line.rstrip().split(' ')\n",
        "                token, elems = elems[0], [float(elem) for elem in elems[1:]]\n",
        "                # Skip header information, such as the top row in fastText\n",
        "                if len(elems) > 1:\n",
        "                    idx_to_token.append(token)\n",
        "                    idx_to_vec.append(elems)\n",
        "        idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec\n",
        "        return idx_to_token, np.array(idx_to_vec)\n",
        "\n",
        "    def __getitem__(self, tokens):\n",
        "        indices = [self.token_to_idx.get(token, self.unknown_idx)\n",
        "                   for token in tokens]\n",
        "        vecs = self.idx_to_vec[np.array(indices)]\n",
        "        return vecs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx_to_token)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "JK2BipJX28d-",
        "colab_type": "text"
      },
      "source": [
        "Next, we use 50-dimensional GloVe embeddings pretrained on a subset of the Wikipedia. The corresponding word embedding is automatically downloaded the first time we create a pretrained word embedding instance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "NUIDjtSy28d-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47e6aa53-a4b3-45c6-c6b8-a2bf10b85a34"
      },
      "source": [
        "glove_6b50d = TokenEmbedding('glove.6b.50d')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/glove.6B.50d.zip from http://d2l-data.s3-accelerate.amazonaws.com/glove.6B.50d.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "m7csAo5f28eE",
        "colab_type": "text"
      },
      "source": [
        "Output the dictionary size. The dictionary contains $400,000$ words and a special unknown token.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 9,
        "tab": [
          "mxnet"
        ],
        "id": "NsA9x-Ju28eF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "018160fd-8f63-41d7-d3ff-e41b282deb04"
      },
      "source": [
        "len(glove_6b50d)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "GZuVl08728eK",
        "colab_type": "text"
      },
      "source": [
        "We can use a word to get its index in the dictionary, or we can get the word from its index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "12"
        },
        "origin_pos": 11,
        "tab": [
          "mxnet"
        ],
        "id": "ulZvs0a828eL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7c4b1935-03ae-4a2e-a984-cb09e8e5a1bd"
      },
      "source": [
        "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3367, 'beautiful')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "iyFQdMXW28eP",
        "colab_type": "text"
      },
      "source": [
        "## Applying Pretrained Word Vectors\n",
        "\n",
        "Below, we demonstrate the application of pretrained word vectors, using GloVe as an example.\n",
        "\n",
        "### Finding Synonyms\n",
        "\n",
        "Here, we re-implement the algorithm used to search for synonyms by cosine\n",
        "similarity introduced in :numref:`sec_word2vec`\n",
        "\n",
        "In order to reuse the logic for seeking the $k$ nearest neighbors when\n",
        "seeking analogies, we encapsulate this part of the logic separately in the `knn`\n",
        "($k$-nearest neighbors) function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 13,
        "tab": [
          "mxnet"
        ],
        "id": "qdJDbTIa28eP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def knn(W, x, k):\n",
        "    # The added 1e-9 is for numerical stability\n",
        "    cos = np.dot(W, x.reshape(-1,)) / (\n",
        "        np.sqrt(np.sum(W * W, axis=1) + 1e-9) * np.sqrt((x * x).sum()))\n",
        "    topk = npx.topk(cos, k=k, ret_typ='indices')\n",
        "    return topk, [cos[int(i)] for i in topk]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "2xzLHF0c28eT",
        "colab_type": "text"
      },
      "source": [
        "Then, we search for synonyms by pre-training the word vector instance `embed`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 15,
        "tab": [
          "mxnet"
        ],
        "id": "c7hZaadS28eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "    topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1)\n",
        "    for i, c in zip(topk[1:], cos[1:]):  # Remove input words\n",
        "        print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "88UIOUXl28eW",
        "colab_type": "text"
      },
      "source": [
        "The dictionary of pretrained word vector instance `glove_6b50d` already created contains 400,000 words and a special unknown token. Excluding input words and unknown words, we search for the three words that are the most similar in meaning to \"chip\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "mxnet"
        ],
        "id": "0iyEJPVR28eX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "72d4a144-e751-4c41-a104-7215c87edc5d"
      },
      "source": [
        "get_similar_tokens('chip', 3, glove_6b50d)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine sim=0.856: chips\n",
            "cosine sim=0.749: intel\n",
            "cosine sim=0.749: electronics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "CHviB-h-28eb",
        "colab_type": "text"
      },
      "source": [
        "Next, we search for the synonyms of \"baby\" and \"beautiful\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 19,
        "tab": [
          "mxnet"
        ],
        "id": "FleHH81S28eb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "516d4599-b315-4031-bd3c-c5df5b659bea"
      },
      "source": [
        "get_similar_tokens('baby', 3, glove_6b50d)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine sim=0.839: babies\n",
            "cosine sim=0.800: boy\n",
            "cosine sim=0.792: girl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 20,
        "tab": [
          "mxnet"
        ],
        "id": "sAmunDda28ee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "de7ce07d-1eaa-4fff-d56f-503182f2518c"
      },
      "source": [
        "get_similar_tokens('beautiful', 3, glove_6b50d)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosine sim=0.921: lovely\n",
            "cosine sim=0.893: gorgeous\n",
            "cosine sim=0.830: wonderful\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 21,
        "id": "majHXWs928eh",
        "colab_type": "text"
      },
      "source": [
        "### Finding Analogies\n",
        "\n",
        "In addition to seeking synonyms, we can also use the pretrained word vector to seek the analogies between words. For example, “man”:“woman”::“son”:“daughter” is an example of analogy, “man” is to “woman” as “son” is to “daughter”. The problem of seeking analogies can be defined as follows: for four words in the analogical relationship $a : b :: c : d$, given the first three words, $a$, $b$ and $c$, we want to find $d$. Assume the word vector for the word $w$ is $\\text{vec}(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $\\text{vec}(c)+\\text{vec}(b)-\\text{vec}(a)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 22,
        "tab": [
          "mxnet"
        ],
        "id": "aL3j_nzv28eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_analogy(token_a, token_b, token_c, embed):\n",
        "    vecs = embed[[token_a, token_b, token_c]]\n",
        "    x = vecs[1] - vecs[0] + vecs[2]\n",
        "    topk, cos = knn(embed.idx_to_vec, x, 1)\n",
        "    return embed.idx_to_token[int(topk[0])]  # Remove unknown words"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "id": "4bC73qA928ek",
        "colab_type": "text"
      },
      "source": [
        "Verify the \"male-female\" analogy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "18"
        },
        "origin_pos": 24,
        "tab": [
          "mxnet"
        ],
        "id": "U4tk0txl28el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1f886c9-a737-43e6-faeb-553c54d4a609"
      },
      "source": [
        "get_analogy('man', 'woman', 'son', glove_6b50d)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'daughter'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 25,
        "id": "I21RwYOA28eo",
        "colab_type": "text"
      },
      "source": [
        "“Capital-country” analogy: \"beijing\" is to \"china\" as \"tokyo\" is to what? The answer should be \"japan\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "19"
        },
        "origin_pos": 26,
        "tab": [
          "mxnet"
        ],
        "id": "FBwpGKD028eo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7cc9a834-4a2c-4f54-b786-dfa7eef5d9f0"
      },
      "source": [
        "get_analogy('beijing', 'china', 'tokyo', glove_6b50d)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'japan'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 27,
        "id": "mbFQ0UFB28er",
        "colab_type": "text"
      },
      "source": [
        "\"Adjective-superlative adjective\" analogy: \"bad\" is to \"worst\" as \"big\" is to what? The answer should be \"biggest\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "20"
        },
        "origin_pos": 28,
        "tab": [
          "mxnet"
        ],
        "id": "WNk1-0YC28es",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2735ccfc-f0fc-46dd-df81-587dafdfc4f7"
      },
      "source": [
        "get_analogy('bad', 'worst', 'big', glove_6b50d)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'biggest'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 29,
        "id": "a7IJr3O228ev",
        "colab_type": "text"
      },
      "source": [
        "\"Present tense verb-past tense verb\" analogy: \"do\" is to \"did\" as \"go\" is to what? The answer should be \"went\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "21"
        },
        "origin_pos": 30,
        "tab": [
          "mxnet"
        ],
        "id": "h7yzciin28ew",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "edc70430-89a7-4e36-dc52-4ef9b12dc100"
      },
      "source": [
        "get_analogy('do', 'did', 'go', glove_6b50d)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'went'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 31,
        "id": "pxioQsUv28ey",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Word vectors pre-trained on a large-scale corpus can often be applied to downstream natural language processing tasks.\n",
        "* We can use pre-trained word vectors to seek synonyms and analogies.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Test the fastText results using `TokenEmbedding('wiki.en')`.\n",
        "1. If the dictionary is extremely large, how can we accelerate finding synonyms and analogies?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 32,
        "tab": [
          "mxnet"
        ],
        "id": "7QE23KOe28ez",
        "colab_type": "text"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/387)\n"
      ]
    }
  ]
}