{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c2w602scb6V",
        "outputId": "4b7051d6-86ae-43ec-c9de-c40169a1f625"
      },
      "source": [
        "!pip install -U git+https://github.com/d2l-ai/d2l-en.git@master"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/d2l-ai/d2l-en.git@master\n",
            "  Cloning https://github.com/d2l-ai/d2l-en.git (to revision master) to /tmp/pip-req-build-58qec9r4\n",
            "  Running command git clone -q https://github.com/d2l-ai/d2l-en.git /tmp/pip-req-build-58qec9r4\n",
            "Requirement already satisfied, skipping upgrade: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (7.5.1)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.7.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.1) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.0.8)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (4.7.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.15.1) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.1) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.1) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (20.0.0)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->d2l==0.15.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->d2l==0.15.1) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->d2l==0.15.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->jupyter->d2l==0.15.1) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.15.1) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.15.1) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (0.5.1)\n",
            "Building wheels for collected packages: d2l\n",
            "  Building wheel for d2l (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d2l: filename=d2l-0.15.1-cp36-none-any.whl size=65521 sha256=338471244e08c2e10db1a381da2198ce69bd3718f8880d18651d4dffb004b964\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mxobqr89/wheels/0f/41/8f/72ece70ede8a0e37eec72c03087eb4604925ba212b804f8cad\n",
            "Successfully built d2l\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LADrtFZ7cnXl"
      },
      "source": [
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn7uYHa6cvYt"
      },
      "source": [
        "#@save\n",
        "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
        "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
        "    # 0 and 1 are marking segment A and B, respectively\n",
        "    segments = [0] * (len(tokens_a) + 2)\n",
        "    if tokens_b is not None:\n",
        "        tokens += tokens_b + ['<sep>']\n",
        "        segments += [1] * (len(tokens_b) + 1)\n",
        "    return tokens, segments"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPYoZNkXcxEK"
      },
      "source": [
        "#@save\n",
        "class BERTEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 **kwargs):\n",
        "        super(BERTEncoder, self).__init__(**kwargs)\n",
        "        self.token_embedding = nn.Embedding(vocab_size, num_hiddens)\n",
        "        self.segment_embedding = nn.Embedding(2, num_hiddens)\n",
        "        self.blks = nn.Sequential()\n",
        "        for i in range(num_layers):\n",
        "            self.blks.add_module(f\"{i}\", d2l.EncoderBlock(\n",
        "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
        "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
        "        # In BERT, positional embeddings are learnable, thus we create a\n",
        "        # parameter of positional embeddings that are long enough\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len,\n",
        "                                                      num_hiddens))\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens):\n",
        "        # Shape of `X` remains unchanged in the following code snippet:\n",
        "        # (batch size, max sequence length, `num_hiddens`)\n",
        "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
        "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
        "        for blk in self.blks:\n",
        "            X = blk(X, valid_lens)\n",
        "        return X"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1hyr5dYczxP"
      },
      "source": [
        "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024, 4\n",
        "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
        "encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                      ffn_num_hiddens, num_heads, num_layers, dropout)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeOZg17gc07j",
        "outputId": "d746b908-863f-47ff-f52e-166472a88fc5"
      },
      "source": [
        "tokens = torch.randint(0, vocab_size, (2, 8))\n",
        "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
        "encoded_X = encoder(tokens, segments, None)\n",
        "encoded_X.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 8, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn0V4cn-c19R"
      },
      "source": [
        "#@save\n",
        "class MaskLM(nn.Module):\n",
        "    def __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs):\n",
        "        super(MaskLM, self).__init__(**kwargs)\n",
        "        self.mlp = nn.Sequential(nn.Linear(num_inputs, num_hiddens),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.LayerNorm(num_hiddens),\n",
        "                                 nn.Linear(num_hiddens, vocab_size))\n",
        "\n",
        "    def forward(self, X, pred_positions):\n",
        "        num_pred_positions = pred_positions.shape[1]\n",
        "        pred_positions = pred_positions.reshape(-1)\n",
        "        batch_size = X.shape[0]\n",
        "        batch_idx = torch.arange(0, batch_size)\n",
        "        # Suppose that `batch_size` = 2, `num_pred_positions` = 3, then\n",
        "        # `batch_idx` is `torch.tensor([0, 0, 0, 1, 1, 1])`\n",
        "        batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)\n",
        "        masked_X = X[batch_idx, pred_positions]\n",
        "        masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))\n",
        "        mlm_Y_hat = self.mlp(masked_X)\n",
        "        return mlm_Y_hat"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zf1j3Hxxc3d-",
        "outputId": "aa527eec-8302-4b4c-b814-77ea8b995ee6"
      },
      "source": [
        "mlm = MaskLM(vocab_size, num_hiddens)\n",
        "mlm_positions = torch.tensor([[1, 5, 2], [6, 1, 5]])\n",
        "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
        "mlm_Y_hat.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 10000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_N5Pkc-c5Vu",
        "outputId": "3b9beb87-339b-4d66-8896-aa39d0cd4b1c"
      },
      "source": [
        "mlm_Y = torch.tensor([[7, 8, 9], [10, 20, 30]])\n",
        "loss = nn.CrossEntropyLoss(reduction='none')\n",
        "mlm_l = loss(mlm_Y_hat.reshape((-1, vocab_size)), mlm_Y.reshape(-1))\n",
        "mlm_l.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK2JZjv1c6kl"
      },
      "source": [
        "#@save\n",
        "class NextSentencePred(nn.Module):\n",
        "    def __init__(self, num_inputs, **kwargs):\n",
        "        super(NextSentencePred, self).__init__(**kwargs)\n",
        "        self.output = nn.Linear(num_inputs, 2)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # `X` shape: (batch size, `num_hiddens`)\n",
        "        return self.output(X)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpGfy6M1c8Pk",
        "outputId": "136c3fb3-49c7-4893-9464-3116a7b924a3"
      },
      "source": [
        "# PyTorch by default won't flatten the tensor as seen in mxnet where, if\n",
        "# flatten=True, all but the first axis of input data are collapsed together\n",
        "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
        "# input_shape for NSP: (batch size, `num_hiddens`)\n",
        "nsp = NextSentencePred(encoded_X.shape[-1])\n",
        "nsp_Y_hat = nsp(encoded_X)\n",
        "nsp_Y_hat.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6VXxSa-c9fB",
        "outputId": "137eb599-03bb-40c1-f6f0-96f2eb45c613"
      },
      "source": [
        "nsp_y = torch.tensor([0, 1])\n",
        "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
        "nsp_l.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfamx3eOc_z3"
      },
      "source": [
        "#@save\n",
        "class BERTModel(nn.Module):\n",
        "    def __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input,\n",
        "                 ffn_num_hiddens, num_heads, num_layers, dropout,\n",
        "                 max_len=1000, key_size=768, query_size=768, value_size=768,\n",
        "                 hid_in_features=768, mlm_in_features=768,\n",
        "                 nsp_in_features=768):\n",
        "        super(BERTModel, self).__init__()\n",
        "        self.encoder = BERTEncoder(vocab_size, num_hiddens, norm_shape,\n",
        "                    ffn_num_input, ffn_num_hiddens, num_heads, num_layers,\n",
        "                    dropout, max_len=max_len, key_size=key_size,\n",
        "                    query_size=query_size, value_size=value_size)\n",
        "        self.hidden = nn.Sequential(nn.Linear(hid_in_features, num_hiddens),\n",
        "                                    nn.Tanh())\n",
        "        self.mlm = MaskLM(vocab_size, num_hiddens, mlm_in_features)\n",
        "        self.nsp = NextSentencePred(nsp_in_features)\n",
        "\n",
        "    def forward(self, tokens, segments, valid_lens=None, pred_positions=None):\n",
        "        encoded_X = self.encoder(tokens, segments, valid_lens)\n",
        "        if pred_positions is not None:\n",
        "            mlm_Y_hat = self.mlm(encoded_X, pred_positions)\n",
        "        else:\n",
        "            mlm_Y_hat = None\n",
        "        # The hidden layer of the MLP classifier for next sentence prediction.\n",
        "        # 0 is the index of the '<cls>' token\n",
        "        nsp_Y_hat = self.nsp(self.hidden(encoded_X[:, 0, :]))\n",
        "        return encoded_X, mlm_Y_hat, nsp_Y_hat"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}