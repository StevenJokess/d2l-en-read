{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "bert-dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJDWOVBLekdo"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaIaFyY_ekdp",
        "outputId": "1a8cd406-5b17-4113-e23d-dd181c7ff2f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install -U mxnet-cu101==1.7.0 --default-timeout=1000 # https://github.com/pypa/pypi-support/issues/175\n",
        "!pip install d2l==0.14.4\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1MB 1.3MB/s eta 0:00:10tcmalloc: large alloc 1147494400 bytes == 0x65d1a000 @  0x7f1a3a5ea615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
            "\u001b[K     |████████████████████████████████| 846.0MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (1.18.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n",
            "Requirement already satisfied: d2l==0.14.4 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (3.2.2)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.2.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.10.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.7.7)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.14.4) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.14.4) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (1.2.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (4.3.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.4.4)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (2.11.2)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (3.2.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (2.6.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (4.6.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (5.0.7)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (0.2.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.1.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (0.9.1)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.3.5)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (1.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (5.5.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.14.4) (3.5.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (19.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->d2l==0.14.4) (1.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->nbconvert->jupyter->d2l==0.14.4) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l==0.14.4) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (0.5.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l==0.14.4) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.14.4) (0.2.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (50.3.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->d2l==0.14.4) (0.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "KDRN0p4eekdt"
      },
      "source": [
        "# The Dataset for Pretraining BERT\n",
        ":label:`sec_bert-dataset`\n",
        "\n",
        "To pretrain the BERT model as implemented in :numref:`sec_bert`,\n",
        "we need to generate the dataset in the ideal format to facilitate\n",
        "the two pretraining tasks:\n",
        "masked language modeling and next sentence prediction.\n",
        "On one hand,\n",
        "the original BERT model is pretrained on the concatenation of\n",
        "two huge corpora BookCorpus and English Wikipedia (see :numref:`subsec_bert_pretraining_tasks`),\n",
        "making it hard to run for most readers of this book.\n",
        "On the other hand,\n",
        "the off-the-shelf pretrained BERT model\n",
        "may not fit for applications from specific domains like medicine.\n",
        "Thus, it is getting popular to pretrain BERT on a customized dataset.\n",
        "To facilitate the demonstration of BERT pretraining,\n",
        "we use a smaller corpus WikiText-2 :cite:`Merity.Xiong.Bradbury.ea.2016`.\n",
        "\n",
        "Comparing with the PTB dataset used for pretraining word2vec in :numref:`sec_word2vec_data`,\n",
        "WikiText-2 i) retains the original punctuation, making it suitable for next sentence prediction; ii) retains the original case and numbers; iii) is over twice larger.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "1"
        },
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "ksyUE5U3ekdu"
      },
      "source": [
        "import collections\n",
        "from d2l import mxnet as d2l\n",
        "import mxnet as mx\n",
        "from mxnet import autograd, gluon, init, np, npx\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import zipfile\n",
        "\n",
        "npx.set_np()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "id": "QZCMPcfFekdw"
      },
      "source": [
        "In the WikiText-2 dataset,\n",
        "each line represents a paragraph where\n",
        "space is inserted between any punctuation and its preceding token.\n",
        "Paragraphs with at least two sentences are retained.\n",
        "To split sentences, we only use the period as the delimiter for simplicity.\n",
        "We leave discussions of more complex sentence splitting techniques in the exercises\n",
        "at the end of this section.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "2"
        },
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "sjWk6H0Vekdx"
      },
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['wikitext-2'] = (\n",
        "    'https://s3.amazonaws.com/research.metamind.io/wikitext/'\n",
        "    'wikitext-2-v1.zip', '3c914d17d80b1459be871a5039ac23e752a53cbe')\n",
        "\n",
        "#@save\n",
        "def _read_wiki(data_dir):\n",
        "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
        "    with open(file_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    # Uppercase letters are converted to lowercase ones\n",
        "    paragraphs = [line.strip().lower().split(' . ')\n",
        "                  for line in lines if len(line.split(' . ')) >= 2]\n",
        "    random.shuffle(paragraphs)\n",
        "    return paragraphs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "z9WA366Nekd0"
      },
      "source": [
        "## Defining Helper Functions for Pretraining Tasks\n",
        "\n",
        "In the following,\n",
        "we begin by implementing helper functions for the two BERT pretraining tasks:\n",
        "next sentence prediction and masked language modeling.\n",
        "These helper functions will be invoked later\n",
        "when transforming the raw text corpus\n",
        "into the dataset of the ideal format to pretrain BERT.\n",
        "\n",
        "### Generating the Next Sentence Prediction Task\n",
        "\n",
        "According to descriptions of :numref:`subsec_nsp`,\n",
        "the `_get_next_sentence` function generates a training example\n",
        "for the binary classification task.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "3"
        },
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "J0MsrJCcekd0"
      },
      "source": [
        "#@save\n",
        "def _get_next_sentence(sentence, next_sentence, paragraphs):\n",
        "    if random.random() < 0.5:\n",
        "        is_next = True\n",
        "    else:\n",
        "        # `paragraphs` is a list of lists of lists\n",
        "        next_sentence = random.choice(random.choice(paragraphs))\n",
        "        is_next = False\n",
        "    return sentence, next_sentence, is_next"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "a0MtT4CKekd3"
      },
      "source": [
        "The following function generates training examples for next sentence prediction\n",
        "from the input `paragraph` by invoking the `_get_next_sentence` function.\n",
        "Here `paragraph` is a list of sentences, where each sentence is a list of tokens.\n",
        "The argument `max_len` specifies the maximum length of a BERT input sequence during pretraining.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "WotZaevYekd4"
      },
      "source": [
        "#@save\n",
        "def _get_nsp_data_from_paragraph(paragraph, paragraphs, vocab, max_len):\n",
        "    nsp_data_from_paragraph = []\n",
        "    for i in range(len(paragraph) - 1):\n",
        "        tokens_a, tokens_b, is_next = _get_next_sentence(\n",
        "            paragraph[i], paragraph[i + 1], paragraphs)\n",
        "        # Consider 1 '<cls>' token and 2 '<sep>' tokens\n",
        "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
        "            continue\n",
        "        tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
        "        nsp_data_from_paragraph.append((tokens, segments, is_next))\n",
        "    return nsp_data_from_paragraph"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "ec7pgHXpekd6"
      },
      "source": [
        "### Generating the Masked Language Modeling Task\n",
        ":label:`subsec_prepare_mlm_data`\n",
        "\n",
        "In order to generate training examples\n",
        "for the masked language modeling task\n",
        "from a BERT input sequence,\n",
        "we define the following `_replace_mlm_tokens` function.\n",
        "In its inputs, `tokens` is a list of tokens representing a BERT input sequence,\n",
        "`candidate_pred_positions` is a list of token indices of the BERT input sequence\n",
        "excluding those of special tokens (special tokens are not predicted in the masked language modeling task),\n",
        "and `num_mlm_preds` indicates the number of predictions (recall 15% random tokens to predict).\n",
        "Following the definition of the masked language modeling task in :numref:`subsec_mlm`,\n",
        "at each prediction position, the input may be replaced by\n",
        "a special “&lt;mask&gt;” token or a random token, or remain unchanged.\n",
        "In the end, the function returns the input tokens after possible replacement,\n",
        "the token indices where predictions take place and labels for these predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "origin_pos": 9,
        "tab": [
          "mxnet"
        ],
        "id": "YU2C8nQxekd7"
      },
      "source": [
        "#@save\n",
        "def _replace_mlm_tokens(tokens, candidate_pred_positions, num_mlm_preds,\n",
        "                        vocab):\n",
        "    # Make a new copy of tokens for the input of a masked language model,\n",
        "    # where the input may contain replaced '<mask>' or random tokens\n",
        "    mlm_input_tokens = [token for token in tokens]\n",
        "    pred_positions_and_labels = []\n",
        "    # Shuffle for getting 15% random tokens for prediction in the masked\n",
        "    # language modeling task\n",
        "    random.shuffle(candidate_pred_positions)\n",
        "    for mlm_pred_position in candidate_pred_positions:\n",
        "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
        "            break\n",
        "        masked_token = None\n",
        "        # 80% of the time: replace the word with the '<mask>' token\n",
        "        if random.random() < 0.8:\n",
        "            masked_token = '<mask>'\n",
        "        else:\n",
        "            # 10% of the time: keep the word unchanged\n",
        "            if random.random() < 0.5:\n",
        "                masked_token = tokens[mlm_pred_position]\n",
        "            # 10% of the time: replace the word with a random word\n",
        "            else:\n",
        "                masked_token = random.randint(0, len(vocab) - 1)\n",
        "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
        "        pred_positions_and_labels.append(\n",
        "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
        "    return mlm_input_tokens, pred_positions_and_labels"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "5AqMgCI-ekd-"
      },
      "source": [
        "By invoking the aforementioned `_replace_mlm_tokens` function,\n",
        "the following function takes a BERT input sequence (`tokens`)\n",
        "as an input and returns indices of the input tokens\n",
        "(after possible token replacement as described in :numref:`subsec_mlm`),\n",
        "the token indices where predictions take place,\n",
        "and label indices for these predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "origin_pos": 11,
        "tab": [
          "mxnet"
        ],
        "id": "NiX9I11mekd_"
      },
      "source": [
        "#@save\n",
        "def _get_mlm_data_from_tokens(tokens, vocab):\n",
        "    candidate_pred_positions = []\n",
        "    # `tokens` is a list of strings\n",
        "    for i, token in enumerate(tokens):\n",
        "        # Special tokens are not predicted in the masked language modeling\n",
        "        # task\n",
        "        if token in ['<cls>', '<sep>']:\n",
        "            continue\n",
        "        candidate_pred_positions.append(i)\n",
        "    # 15% of random tokens are predicted in the masked language modeling task\n",
        "    num_mlm_preds = max(1, round(len(tokens) * 0.15))\n",
        "    mlm_input_tokens, pred_positions_and_labels = _replace_mlm_tokens(\n",
        "        tokens, candidate_pred_positions, num_mlm_preds, vocab)\n",
        "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
        "                                       key=lambda x: x[0])\n",
        "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
        "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
        "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "cXvcs5BJekeB"
      },
      "source": [
        "## Transforming Text into the Pretraining Dataset\n",
        "\n",
        "Now we are almost ready to customize a `Dataset` class for pretraining BERT.\n",
        "Before that, \n",
        "we still need to define a helper function `_pad_bert_inputs`\n",
        "to append the special “&lt;mask&gt;” tokens to the inputs.\n",
        "Its argument `examples` contain the outputs from the helper functions `_get_nsp_data_from_paragraph` and `_get_mlm_data_from_tokens` for the two pretraining tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "7"
        },
        "origin_pos": 13,
        "tab": [
          "mxnet"
        ],
        "id": "A2X_TLV2ekeD"
      },
      "source": [
        "#@save\n",
        "def _pad_bert_inputs(examples, max_len, vocab):\n",
        "    max_num_mlm_preds = round(max_len * 0.15)\n",
        "    all_token_ids, all_segments, valid_lens,  = [], [], []\n",
        "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
        "    nsp_labels = []\n",
        "    for (token_ids, pred_positions, mlm_pred_label_ids, segments,\n",
        "         is_next) in examples:\n",
        "        all_token_ids.append(np.array(token_ids + [vocab['<pad>']] * (\n",
        "            max_len - len(token_ids)), dtype='int32'))\n",
        "        all_segments.append(np.array(segments + [0] * (\n",
        "            max_len - len(segments)), dtype='int32'))\n",
        "        # `valid_lens` excludes count of '<pad>' tokens\n",
        "        valid_lens.append(np.array(len(token_ids), dtype='float32'))\n",
        "        all_pred_positions.append(np.array(pred_positions + [0] * (\n",
        "            max_num_mlm_preds - len(pred_positions)), dtype='int32'))\n",
        "        # Predictions of padded tokens will be filtered out in the loss via\n",
        "        # multiplication of 0 weights\n",
        "        all_mlm_weights.append(\n",
        "            np.array([1.0] * len(mlm_pred_label_ids) + [0.0] * (\n",
        "                max_num_mlm_preds - len(pred_positions)), dtype='float32'))\n",
        "        all_mlm_labels.append(np.array(mlm_pred_label_ids + [0] * (\n",
        "            max_num_mlm_preds - len(mlm_pred_label_ids)), dtype='int32'))\n",
        "        nsp_labels.append(np.array(is_next))\n",
        "    return (all_token_ids, all_segments, valid_lens, all_pred_positions,\n",
        "            all_mlm_weights, all_mlm_labels, nsp_labels)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "g47HB044ekeF"
      },
      "source": [
        "Putting the helper functions for\n",
        "generating training examples of the two pretraining tasks,\n",
        "and the helper function for padding inputs together,\n",
        "we customize the following `_WikiTextDataset` class as the WikiText-2 dataset for pretraining BERT.\n",
        "By implementing the `__getitem__ `function,\n",
        "we can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples \n",
        "generated from a pair of sentences from the WikiText-2 corpus.\n",
        "\n",
        "The original BERT model uses WordPiece embeddings whose vocabulary size is 30,000 :cite:`Wu.Schuster.Chen.ea.2016`.\n",
        "The tokenization method of WordPiece is a slight modification of\n",
        "the original byte pair encoding algorithm in :numref:`subsec_Byte_Pair_Encoding`.\n",
        "For simplicity, we use the `d2l.tokenize` function for tokenization.\n",
        "Infrequent tokens that appear less than five times are filtered out.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "8"
        },
        "origin_pos": 15,
        "tab": [
          "mxnet"
        ],
        "id": "73oqYMzoekeG"
      },
      "source": [
        "#@save\n",
        "class _WikiTextDataset(gluon.data.Dataset):\n",
        "    def __init__(self, paragraphs, max_len):\n",
        "        # Input `paragraphs[i]` is a list of sentence strings representing a\n",
        "        # paragraph; while output `paragraphs[i]` is a list of sentences\n",
        "        # representing a paragraph, where each sentence is a list of tokens\n",
        "        paragraphs = [d2l.tokenize(\n",
        "            paragraph, token='word') for paragraph in paragraphs]\n",
        "        sentences = [sentence for paragraph in paragraphs\n",
        "                     for sentence in paragraph]\n",
        "        self.vocab = d2l.Vocab(sentences, min_freq=5, reserved_tokens=[\n",
        "            '<pad>', '<mask>', '<cls>', '<sep>'])\n",
        "        # Get data for the next sentence prediction task\n",
        "        examples = []\n",
        "        for paragraph in paragraphs:\n",
        "            examples.extend(_get_nsp_data_from_paragraph(\n",
        "                paragraph, paragraphs, self.vocab, max_len))\n",
        "        # Get data for the masked language model task\n",
        "        examples = [(_get_mlm_data_from_tokens(tokens, self.vocab)\n",
        "                      + (segments, is_next))\n",
        "                     for tokens, segments, is_next in examples]\n",
        "        # Pad inputs\n",
        "        (self.all_token_ids, self.all_segments, self.valid_lens,\n",
        "         self.all_pred_positions, self.all_mlm_weights,\n",
        "         self.all_mlm_labels, self.nsp_labels) = _pad_bert_inputs(\n",
        "            examples, max_len, self.vocab)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.all_token_ids[idx], self.all_segments[idx],\n",
        "                self.valid_lens[idx], self.all_pred_positions[idx],\n",
        "                self.all_mlm_weights[idx], self.all_mlm_labels[idx],\n",
        "                self.nsp_labels[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_token_ids)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "IXu3Py-0ekeI"
      },
      "source": [
        "By using the `_read_wiki` function and the `_WikiTextDataset` class,\n",
        "we define the following `load_data_wiki` to download and WikiText-2 dataset\n",
        "and generate pretraining examples from it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "9"
        },
        "origin_pos": 17,
        "tab": [
          "mxnet"
        ],
        "id": "OGJCkxQNekeJ"
      },
      "source": [
        "#@save\n",
        "def load_data_wiki(batch_size, max_len):\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
        "    paragraphs = _read_wiki(data_dir)\n",
        "    train_set = _WikiTextDataset(paragraphs, max_len)\n",
        "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                                       num_workers=num_workers)\n",
        "    return train_iter, train_set.vocab"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 18,
        "id": "tBs0f31ZekeL"
      },
      "source": [
        "Setting the batch size to 512 and the maximum length of a BERT input sequence to be 64,\n",
        "we print out the shapes of a minibatch of BERT pretraining examples.\n",
        "Note that in each BERT input sequence,\n",
        "$10$ ($64 \\times 0.15$) positions are predicted for the masked language modeling task.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "10"
        },
        "origin_pos": 19,
        "tab": [
          "mxnet"
        ],
        "id": "ci1XM_InekeM",
        "outputId": "bb0534fb-c28f-4e8b-98ea-8cffaa671301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "batch_size, max_len = 512, 64\n",
        "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
        "\n",
        "for (tokens_X, segments_X, valid_lens_x, pred_positions_X, mlm_weights_X,\n",
        "     mlm_Y, nsp_y) in train_iter:\n",
        "    print(tokens_X.shape, segments_X.shape, valid_lens_x.shape,\n",
        "          pred_positions_X.shape, mlm_weights_X.shape, mlm_Y.shape,\n",
        "          nsp_y.shape)\n",
        "    break"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/wikitext-2-v1.zip from https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip...\n",
            "(512, 64) (512, 64) (512,) (512, 10) (512, 10) (512, 10) (512,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 20,
        "id": "4hFf7dnBekeQ"
      },
      "source": [
        "In the end, let us take a look at the vocabulary size.\n",
        "Even after filtering out infrequent tokens,\n",
        "it is still over twice larger than that of the PTB dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "11"
        },
        "origin_pos": 21,
        "tab": [
          "mxnet"
        ],
        "id": "oZVh0gcUekeQ",
        "outputId": "f7f6d84d-3b4b-4608-aba6-6a7b8ed320f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20256"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 22,
        "id": "E0xYz_HcekeT"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Comparing with the PTB dataset, the WikiText-2 dateset retains the original punctuation, case and numbers, and is over twice larger.\n",
        "* We can arbitrarily access the pretraining (masked language modeling and next sentence prediction) examples generated from a pair of sentences from the WikiText-2 corpus.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. For simplicity, the period is used as the only delimiter for splitting sentences. Try other sentence splitting techniques, such as the spaCy and NLTK. Take NLTK as an example. You need to install NLTK first: `pip install nltk`. In the code, first `import nltk`. Then, download the Punkt sentence tokenizer: `nltk.download('punkt')`. To split sentences such as `sentences = 'This is great ! Why not ?'`, invoking `nltk.tokenize.sent_tokenize(sentences)` will return a list of two sentence strings: `['This is great !', 'Why not ?']`.\n",
        "1. What is the vocabulary size if we do not filter out any infrequent token?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 23,
        "tab": [
          "mxnet"
        ],
        "id": "F26sNAzTekeU"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/389)\n"
      ]
    }
  ]
}