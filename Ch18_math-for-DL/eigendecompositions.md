

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-07 13:17:16
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-07 13:32:05
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1111/chapter_appendix-mathematics-for-deep-learning/eigendecomposition.html
-->

# 特征分解

特征值通常是我们在学习线性代数时会遇到的最有用的概念之一，然而，作为初学者，很容易忽略它们的重要性。下面，我们介绍特征分解，并试图传达一些为什么它如此重要的意义。

假设我们有一个矩阵 A，其中包含下列项:

TODO:MATH

如果我们将 a a 应用于任意向量 v = [ x，y ] something v = [ x，y ] something，我们得到向量 vA = [2x,-y ] something vA = [2x,-y ] something。这有一个直观的解释: 将矢量在 x 方向上拉伸为两倍宽，然后在 y 方向上翻转它。

然而，有些向量是不变的。也就是说[1,0] something [1,0] something 发送到[2,0] something [2,0] something，[0,1] something [0,1] something 发送到[0,-1] something [0,-1] something。这些向量仍然在同一条直线上，唯一的修改是，矩阵将它们分别拉伸2倍和-1倍。我们称这样的向量为本征向量，它们的因子被特征值拉伸。

一般来说，如果我们能找到一个数和一个向量

TODO:MATH

我们说 v 是 a 的特征向量，是特征值。


## 修复标准化

现在，从上面的讨论，我们得出结论，我们根本不希望一个随机向量被拉伸或压缩，我们希望随机向量在整个过程中保持相同的大小。为了做到这一点，我们现在用这个主特征值来重新缩放矩阵所以最大的特征值现在是1。让我们看看在这种情况下会发生什么。

TODO:CODE

我们还可以像以前一样绘制连续规范之间的比率，可以看到它确实稳定了。

TODO:CODE

## 总结

现在，我们可以确切地看到我们所希望的！ 用原理特征值对矩阵进行归一化后，我们看到随机数据不会像以前那样爆炸，而是最终平衡为一个特定值。能够从第一性原理开始做这些事情将是一件很高兴的事，结果发现，如果我们深入研究它的数学运算，我们可以看到，一个独立均值为零，方差为一个高斯的大型随机矩阵的最大特征值 由于一个被称为循环法则的引人入胜的事实[Ginibre，1965]，平均条目平均约为n-√，在本例中为5-√≈2.2。如[Pennington等人，2017]和后续工作中所讨论的，随机矩阵的特征值（和一个称为奇异值的相关对象）之间的关系已证明与神经网络的正确初始化有着深厚的联系。

## 小结

* 特征向量是由一个矩阵拉伸而不改变方向的向量。
* 特征值是特征向量被应用矩阵拉伸的量。
* 矩阵的特征分解可以使许多运算简化为对特征值的运算。
* 格尔什戈林圆定理可以给出矩阵特征值的近似值。
* 迭代矩阵幂的性质主要取决于最大特征值的大小。这种理解在神经网络初始化理论中有很多应用。

## 练习

1. 特征值和特征向量是什么
1. 下面矩阵的特征值和特征向量是什么，这个例子和之前的例子相比有什么奇怪的地方
1. 不计算特征值，下面矩阵的最小特征值是否可能小于0。50.5 ?注意:这个问题可以心算。
