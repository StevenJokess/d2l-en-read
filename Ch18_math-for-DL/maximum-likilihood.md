

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-25 17:45:41
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-25 18:49:14
 * @Description:translate by machine half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_appendix-mathematics-for-deep-learning/maximum-likelihood.html
-->

# 极大似然

机器学习中最常见的思维方式之一是最大似然观点。这个概念认为，当处理带有未知参数的概率模型时，使数据具有最高概率的参数是最有可能的参数。

## 极大似然原则

这有一个贝叶斯的解释，可以有助于思考。假设我们有一个带参数的模型和一个数据点集合 x。对于具体性，我们可以想象这是一个单一的值，表示抛硬币时正面朝上的概率，而 x x 是一系列独立的抛硬币。我们稍后将深入研究这个例子。

如果我们想要找到模型参数的最可能的值，那就意味着我们想要找到

TODO:MATH

根据贝叶斯的规则，这和

表达式 p (x) p (x) ，一个生成数据的参数不可知概率，根本不依赖于，因此可以在不改变最佳选择的情况下删除。类似地，我们现在可能假定我们没有预先假设哪一组参数比其他参数更好，所以我们可以声明 p () p ()也不依赖于 θ！例如，这在我们抛硬币的例子中是有意义的，在这个例子中，出现正面的概率可以是[0,1][0,1]中的任意值，而不必事先相信它是公平的或不公平的(通常被称为先验概率)。因此，我们看到，我们的贝叶斯规则的应用表明，我们的最佳选择是最大似然估计为:

TODO:MATH

作为一个通用术语，给定参数(p (x 节) p (x 节))的数据的概率被称为概率。

## 一个具体的例子

让我们通过一个具体的例子来看看它是如何工作的。假设我们有一个单一的参数来表示抛硬币是正面的概率。那么得到一条尾的概率是1-1-，所以如果我们的观测数据 x x 是一个有 n h nH 正面 n t t 反面的序列，我们可以用独立概率乘以这个事实来看

TODO:MATH

如果我们翻转13个13枚硬币，得到序列“ HHHTHTTHHHHHT” ，它有 n h = 9 nH = 9和 n t = 4 nT = 4，我们看到这是

TODO:MATH

这个例子的一个好处就是我们知道答案。事实上，如果我们口头上说，“我掷了13枚硬币，9枚是正面朝上，我们对这枚硬币掷向我们的概率的最佳猜测是什么？每个人都猜对了9/139/13。这个最大可能性方法给我们的是一种从第一主体那里得到这个数字的方法，这种方法将推广到更加复杂的情况。

以我们的例子为例，p (x 行程节点) p (x 行程节点)的坐标图如下:

TODO:CODE

它的最大值接近我们预期的9/13≈0.7…9/13≈0.7…看它是否在这里，我们可以用微积分。注意，在最大值处，函数是平的。因此，我们可以通过求出导数为零的取值，并找出概率最大的取值，来得到最大似然估计值(18.7.1)。我们计算:

这具有三个解决方案：00，11和9/139/13。 前两个显然是最小值，而不是最大值，因为它们将概率00分配给我们的序列。 最终值不会为我们的序列分配零概率，因此最终值必须为最大似然估计θ^ = 9 /13θ^ = 9/13。

连续变量的最大似然性

到目前为止，我们所做的都是假设我们处理的是离散随机变量，但如果我们想处理连续随机变量呢?

简短的总结是，除了我们用概率密度替换概率的所有实例外，没有任何变化。回想一下，我们用小写的pp来写密度，这意味着我们现在说

问题变成：“为什么可以吗？” 毕竟，我们引入密度的原因是因为本身获得特定结果的概率为零，因此针对任何参数集生成数据的概率不为零吗？

的确是这样，了解为什么我们可以转向密度是追踪ε发生了什么的练习。

让我们首先重新定义我们的目标。 假设对于连续随机变量，我们不再希望计算获得正确值的概率，而是希望在range范围内进行匹配。 为简单起见，我们假设我们的数据是对相同分布的随机变量X1，...，XNX1，...，XN的重复观察x1，...，xNx1，...，xN。 如我们先前所见，这可以写成

TODO:MATH

因此，如果我们对此取负对数，则可以得出

TODO:MATH

如果我们检查这个表达式,唯一ϵϵ发生在加常数−Nlog(ϵ)−Nlog⁡(ϵ)。这并不取决于参数θθ,所以最优选择ϵϵθθ并不取决于我们的选择的!如果我们要求四位数或者四百，那么最佳的选择是保持不变的，因此我们可以自由地去掉来看看我们想要优化的是什么

TODO:MATH

因此，我们看到，通过用概率密度替换概率，最大似然观点可以对连续随机变量进行操作，就像对离散随机变量一样容易。


## 总结

最大似然原理告诉我们，对于给定的数据集，最适合的模型是生成概率最大的数据的模型。

人们通常使用负对数似然来代替，原因有很多:数值稳定性、乘积到和的转换(以及由此产生的梯度计算的简化)，以及与信息理论的理论联系。

虽然最简单的激励在离散设置，它可以自由推广到连续设置以及通过最大的概率密度分配给数据点。

## 练习

假设您知道某个变量的某个值αα的密度为1αe-αx。 您可以从随机变量33中获得一个观测值。 αα的最大似然估计是多少？

假设您有一个样本集{xi} Ni = 1，它是从均值未知但方差为11的高斯中抽取的。 均值的最大似然估计是多少？
