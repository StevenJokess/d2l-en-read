

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-25 13:20:41
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-27 17:08:31
 * @Description:translate by machine，improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_appendix-mathematics-for-deep-learning/information-theory.html
-->


# 信息理论

宇宙充满了信息。信息提供了跨学科裂痕的通用语言：从莎士比亚的十四行诗到研究人员在康奈尔的ArXiv的论文，从梵高(Van Gogh)的印刷《星夜》(Starry Night)到贝多芬(Beethoven)的《第五音乐交响曲》(music Symphony no. 5)，从第一门编程语言普兰卡克尔到最先进的机器学习算法。无论采用何种格式，一切都必须遵循信息论的规则。利用信息论，我们可以测量和比较不同信号中存在多少信息。在本节中，我们将研究信息理论的基本概念以及信息理论在机器学习中的应用。

在开始之前，让我们概述一下机器学习和信息理论之间的关系。机器学习旨在从数据中提取有趣的信号并做出重要的预测。另一方面，信息理论研究信息的编码，解码，传输和操纵。结果，信息理论为讨论机器学习系统中的信息处理提供了基础语言。例如，许多机器学习应用程序使用第3.4节中所述的交叉熵损失。这种损失可以直接从信息理论的考虑中得出。

## 信息

让我们从信息论的“灵魂”开始：信息。信息可以用一种或多种编码格式的特定序列以任何形式编码。假设我们要尝试定义信息的概念。起点可能是什么？

考虑以下思想实验。我们有一个有一副扑克牌的朋友。他们会洗牌，翻一些牌，并告诉我们有关牌的说明。我们将尝试评估每个声明的信息内容。

首先，他们将卡片翻过来并告诉我们，“我看到卡片了。” 这根本没有提供任何信息。我们已经确定情况确实如此，因此我们希望这个信息应该为零。

接下来，他们翻过一张卡片，然后说：“我看到红桃。” 这为我们提供了一些信息，但实际上只有4种不同的诉讼是可能的，每种诉讼的可能性均等，因此我们对这一结果并不感到惊讶。我们希望无论采取何种信息措施，此事件的信息含量都应较低。

接下来，他们翻转一张卡片，然后说：“这是黑桃3。” 这是更多信息。确实有52个可能的结果，而我们的朋友告诉我们是哪一个。这应该是中等数量的信息。

让我们把这个逻辑推向极端。假设最后他们翻过来了牌组中的每张牌，并读出了洗牌后的牌组的整个序列。有52！，同样都有可能，因此我们需要大量信息来知道它是哪一个。

我们开发的任何信息概念都必须符合这种直觉。确实，在下一节中，我们将学习如何计算这些事件分别具有0 bits，2bits，5.7bits和225.6bits的信息。

如果我们阅读了这些思想实验，就会发现一个自然的想法。作为出发点，我们可能会以信息为代表的意外程度或事件的抽象可能性，而不是关心知识。例如，如果我们想描述一个不寻常的事件，我们需要很多信息 对于常见事件，我们可能不需要太多信息。

1948年，克劳德·e·香农发表了《通信数学理论》（A Mathematical Theory of Communication）[Shannon, 1948]，建立了信息理论。在他的书中，香农第一次引入了信息熵的概念。我们将从这里开始我们的旅程。

## 自信息

由于信息体现了事件的抽象可能性，因此我们如何将可能性映射到位数？Shannon引入了术语位作为信息单位，它最初是由John Tukey创建的。那么什么是“bit”，为什么我们要用它来衡量信息呢？ 从历史上看，古董发射机只能发送或接收两种类型的代码：0和1。实际上，二进制编码仍然在所有现代数字计算机上普遍使用。这样，任何信息都由一系列的0和1编码。因此，一系列长度为n的二进制数字包含n位信息。

现在，假设对于任何系列的代码，每个0或1发生的可能性为1/2。因此，具有一系列长度为nn的代码的事件XX发生的概率为12n12n。同时，如前所述，该系列包含nn位信息。那么，我们可以概括为一个数学函数，该函数可以将概率pp转换为位数吗？ 香农通过定义自我信息给出了答案

TODO:MATH

作为我们收到的有关此事件XX的信息。请注意，本节将始终使用以2为底的对数。为了简单起见，本节的其余部分将省略对数表示法中的下标2，即log（。）log⁡（。）始终引用log2（。）log2⁡（。）。例如，代码“ 0010”具有自我信息

我们可以计算如下所示的自我信息。在此之前，让我们首先在本节中导入所有必要的包。

TODO:CODE

## 熵

由于自我信息仅测量单个离散事件的信息，因此，对于离散或连续分布的任何随机变量，我们都需要一种更通用的度量。

### Motivating Entropy

让我们尝试具体说明我们想要的东西。 这将是对香农熵公理的非正式陈述。 事实证明，以下常识性陈述的集合迫使我们对信息进行独特的定义。 这些公理以及其他几种公理的正式版本可以在[Csiszar，2008]中找到。

1. 我们通过观察随机变量获得的信息不取决于我们所说的元素，也不取决于概率为零的其他元素的存在。
1. 我们通过观察两个随机变量获得的信息不超过我们通过分别观察它们获得的信息的总和。 如果它们是独立的，则恰好是总和。
1. 观察（几乎）某些事件时获得的信息为（几乎）零。

尽管证明这一事实超出了本文的讨论范围，但重要的是要知道，这唯一地决定了熵必须采用的形式。 这些唯一允许的歧义是在基本单位的选择上，这通常是通过做出我们之前看到的选择而归一化的，因为单个公平硬币翻转提供的信息只有一点。

### 定义

对于遵循概率密度函数(p.d.f.)或概率质量函数(probability density function)(p.m.f.) p(x)的概率分布的任意随机变量X，我们通过熵(或香农熵)来度量预期信息量。

TODO:MATH

具体来说，如果X是离散的，

TODO:MATH

否则，如果X是连续的，我们也将熵称为微分熵

TODO:MATH

### 解释

你可能会好奇:在熵的定义(18.11.3)中，为什么我们使用一个负对数的期望?以下是一些直觉。

首先，为什么我们使用对数函数log?假设p(x)=f1(x)f2(x)…，fn(x)，其中各分量函数fi(x)fi(x)相互独立。这意味着每个fi(x)对从p(x)获得的总信息都是独立贡献的。如上所述，我们希望熵公式对独立随机变量是可加的。幸运的是，loglog可以很自然地将概率分布的乘积转化为单个项的总和。

接下来，为什么要用否定语呢?直观上，更频繁的事件应该比不太常见的事件包含更少的信息，因为我们通常从不寻常的情况中获得的信息比从普通情况中获得的信息更多。但是，loglog是单调递增的，并且对于[0,1]中的所有值都是负的。我们需要在事件的概率和它们的熵之间建立一个单调递减的关系，理想情况下熵总是正的(因为我们观察到的任何东西都不会迫使我们忘记我们已经知道的东西)。因此，我们在loglog函数前面添加一个负号。

最后，期望函数是从哪里来的?考虑一个随机变量XX。我们可以解释self-information(日志(p)−−日志⁡(p))作为惊喜的数量我们有看到一个特定的结果。事实上，当概率趋近于零时，惊奇就会变成无穷。同样，我们可以将熵解释为观察X时的平均惊讶量。例如，假设一个老虎机系统分别发出统计独立符号s1，…，sks1，…，sk，概率分别为p1，…，pk。那么这个系统的熵等于观察每个输出的平均自我信息，即，

TODO:MATH

### 熵的性质

通过以上示例和解释，我们可以得出以下熵的性质（18.11.3）。 在这里，我们将XX称为事件，将PP称为XX的概率分布。
熵是非负的，即H（X）≥0，∀XH（X）≥0，∀X。
如果X〜PX〜P有p.d.f. 或下午 p（x）p（x），我们尝试通过p.d.f的新概率分布QQ估算PP。 或下午 q（x）
（x），然后
（18.11.7）
H（X）=-Ex〜P [logp（x）]≤-Ex〜P [logq（x）]，当且仅当P = Q时相等。
H（X）=-Ex〜P [log原木⁡p（x）]≤-Ex〜P [log原木⁡q（x）]，当且仅当当且仅当P = Q时，才相等。
或者，H（X）H（X）给出编码从PP提取的符号所需的平均位数的下限。

如果X〜PX〜P，则xx在所有可能的结果之间平均分配信息时，将传达最大的信息量。 具体来说，如果概率分布PP是由kk -class {p1，…，pk} {p1，…，pk}离散的，则

（18.11.8）

H（X）≤log（k），当且仅当pi = 1k，∀xi。

H（X）≤log原木⁡（k），当且仅当当且仅当pi = 1k，∀xi时才相等。

如果PP是一个连续的随机变量，那么故事将变得更加复杂。 但是，如果我们另外强加一个有限间隔（所有值都在00到11之间）支持PP，那么，如果PP是该间隔上的均匀分布，则它具有最高的熵。

## 互信息

与先前的随机变量（X，Y）（X，Y）设置相比，您可能会想：“现在我们知道YY中包含多少信息，但XX中不包含，我们是否可以类似地询问XX之间共享了多少信息？ 和YY吗？” 答案将是（X，Y）（X，Y）的相互信息，我们将其写为I（X，Y）I（X，Y）。

与其直接研究正式的定义，不如让我们实践直觉，首先尝试完全根据我们之前构建的术语为互信息导出表达式。 我们希望找到两个随机变量之间共享的信息。 我们可以尝试执行此操作的一种方法是先将XX和YY中包含的所有信息一起开始，然后将不共享的部分删除。 XX和YY中包含的信息一起记为H（X，Y）H（X，Y）。 我们要从中减去XX中包含但不包含在YY中的信息，以及YY中包含但不包含在XX中的信息。 如上一节所述，这分别由H（X∣Y）H（X∣Y）和H（Y∣X）H（Y∣X）给出。 因此，我们认为相互信息应该是

TODO:MATH

在很多方面，我们可以把互信息(18.11.18)看作是我们在18.6节中看到的相关系数的原则性扩展。这使得我们不仅可以要求变量之间的线性关系，而且可以要求任意类型的两个随机变量之间共享的最大信息量。

现在，让我们从头开始实现互信息。

TODO:CODE

### 互信息的性质

你不需要记住互信息的定义(18.11.18)，只需要记住它的显著性质:

* 互信息是对称的，即。I(X, Y) = I(Y, X)
* 互信息是非负的。I(X, Y)≥0
* 当且仅当X和Y相互独立时，I(X,Y)=0。例如，如果XX和YY是独立的，那么YY不提供XX的任何信息，反之亦然，所以他们的相互信息为零。
* 或者，如果X是Y的可逆函数，则YY和XX共享所有信息I(X, Y) = H (Y) = H (X)。

### 逐点互信息

当我们在本章开始时使用熵时，我们能够提供-log（pX（x））-log⁡（pX（x））的解释，这让我们对特定的结果感到惊讶。 我们可以对互信息中的对数术语进行类似的解释，这通常称为逐点互信息：

pmi（x，y）= log原木⁡pX，Y（x，y）pX（x）pY（y）。

我们可以认为（18.11.20）是衡量结果xx和yy的特定组合与我们对独立随机结果所期望的相比较或多或少的可能性。 如果它很大且为正数，则这两个特定结果的发生频率要比随机机会要高得多（注意：分母为pX（x）pY（y）pX（x）pY（y） 这两个结果是独立的），但是如果它很大且为负数，则表示这两个结果的发生率远远少于我们随机预期的结果。

这使我们能够将相互信息（18.11.18）解释为与两个结果独立出现相比，我们惊讶地看到两个结果同时发生的平均数量。

### 互信息的应用

互信息在纯粹的定义中可能有点抽象，那么它与机器学习有什么关系呢?在自然语言处理中，最困难的问题之一是歧义的解决，或者从上下文来看单词的意思不清楚的问题。例如，最近的新闻标题是“亚马逊着火了”。你可能想知道是亚马逊公司的一栋大楼着火了，还是亚马逊雨林着火了。

在这种情况下，相互信息可以帮助我们解决这种模糊性。我们首先找到与亚马逊公司相互信息相对较大的一组单词，如电子商务、技术和online。其次，我们找到另一组词，每个词都与亚马逊雨林有相对较大的互信息，如rain, forest, tropical。当我们需要消除“Amazon”的歧义时，我们可以比较哪一组在“Amazon”上下文中出现的次数更多。在本例中，本文将继续描述森林，并明确上下文。

## Kullback-Leibler散度

正如我们在2.3节中所讨论的，我们可以使用范数来测量任意维度空间中两点之间的距离。我们希望能够用概率分布做类似的任务。有很多方法可以解决这个问题，但信息理论提供了最好的方法之一。我们现在研究Kullback-Leibler (KL)散度，它提供了一种测量两个分布是否接近的方法。

$K L(p(x) \| q(x))=\int p(x) \ln \frac{p(x)}{q(x)} d x$[2]

### KL散度属性

让我们看一下KL散度（18.11.21）的一些属性。

* KL散度是非对称的，即 DKL（P∥Q）≠DKL（Q∥P），如果P≠Q。
* KL散度是非负的，即DKL（P∥Q）≥0。注意，仅当P = Q时，等式成立。
* 如果存在x，使得p（x）> 0且q（x）= 0，则DKL（P∥Q）=∞。
* KL差异和相互信息之间有着密切的关系。 除了图18.11.1所示的关系外，I（X，Y）I（X，Y）在数值上还等同于以下术语：

1. DKL（P（X，Y）∥P（X）P（Y））DKL（P（X，Y）‖P（X）P（Y））;
2. EY {DKL（P（X∣Y）∥P（X））} EY {DKL（P（X∣Y）‖P（X））};
3. EX {DKL（P（Y∣X）∥P（Y））} EX {DKL（P（Y∣X）‖P（Y））}。

对于第一项，我们将互信息解释为P（X，Y）与P（X）和P（Y）乘积之间的KL散度。 衡量联合分布与独立分布之间有何不同。 对于第二个术语，共同信息告诉我们，通过学习XX分布的值可以使YY不确定性的平均降低。 与第三学期相似。

### 例子

让我们通过一个玩具样例来明确地看到非对称性。

首先，我们生成三个长度为1000010,000张量并进行排序:一个目标张量pp服从正态分布N(0,1)N(0,1)，两个候选张量q1和q2分别服从正态分布N(−1,1)和N(1,1)。

TODO:CODE

因为q1q1和q2q2是关于y轴对称的。(x=0x=0)，我们期望DKL(p∥q1)DKL(p∥q1)与DKL(p∥q2)DKL(p‖q2)的KL散度值相似。如下图所示，DKL(p∥q1)DKL(p∥q1)与DKL(p∥q2)DKL(p‖q2)之间的差价只有不到3%。

TODO:CODE

相比之下，你会发现DKL(q2∥p)和DKL(p∥q2)下降了很多，如下图所示，下降了大约40%。

TODO:CODE

## 交叉熵

如果您对信息理论在深度学习中的应用感到好奇，这里有一个简单的例子。我们用概率分布p(x)p(x)定义真实分布PP，用概率分布q(x)q(x)定义估计分布QQ，我们将在本节的其余部分使用它们。

假设我们需要解决一个基于给定nn数据实例{x1，…，xn}的二分类问题。假设我们分别将11和00编码为正类标签yiyi和负类标签yi，我们的神经网络参数化为。如果我们的目的是寻找一个最佳的最佳肝功供，使y^i=p∣xi)，很自然地，我们可以使用18.7节中看到的最大对数似然方法。具体地说，对于真实标签yi和预测y^i=p∣xi y^i=p∣xi (yi∣xi)，被分类为阳性的概率为:iii_p =p∣xi (yi=1∣xi)因此，对数似然函数是

TODO:MATH

最大化对数似然函数l（θ）与最小化-l（θ）相同，因此我们可以从此处找到最佳θθ。 为了将上述损失概括为任何分布，我们也将−l（θ）称为交叉熵损失CE（y，y ^），其中yy遵循真实分布P和y ^遵循估计的分布Q。

所有这些都是通过从最大似然角度进行研究得出的。 但是，如果仔细观察，我们会发现像log（πi）这样的术语已进入我们的计算，这充分表明我们可以从信息理论的角度理解表达式。

### 正式定义

像KL散度一样，对于一个随机变量X，我们也可以通过交叉熵来测量估计分布Q和真实分布P之间的散度，

利用上面讨论的熵的性质，我们也可以将其解释为熵H(P)与P与Q的KL散度之和，即

TODO:MATH

现在定义两个张量作为标记和预测，并计算它们的交叉熵损失。

TODO:MATH

### 属性

如本节开始所述，交叉熵(18.11.25)可用于定义优化问题中的损失函数。下面是等价的:

1. 最大化Q对分布P的预测概率，(即Ex∼P[log(q(x))]);
1. 最小交叉熵CE(P,Q);
1. 最小KL散度DKL(P∥Q)DKL(P‖Q)。

交叉熵的定义间接证明了目标2和目标3之间的等价关系，只要真实数据H(P)的熵是恒定的。

### 交叉熵作为多分类的目标函数

如果深入研究交叉熵损失CE的分类目标函数，我们会发现最小化CE等价于最大化对数似然函数L。

首先，假设我们有一个带有nn实例的数据集，它可以分为k-类。对于每个数据例i，我们通过一热编码表示任意kk -类标签yi=(yi1，…，yik)。具体来说，如果示例i属于类j，那么我们将j-th条目设置为1，并将所有其他组件设置为0，即，

TODO:MATH

例如,如果一个多类分类问题包含三个AA、BB,依依和CC,那么标签可以编码在{答:(1,0,0);B: (0,1,0); C: (0, 0, 1): (1,0,0); B: (0,1,0); C:(0, 0, 1)}。

假设我们的神经网络参数化为。对于真标号向量依依和预测


可以看出，每个数据示例的标签yiyi遵循kk -类多项分布，其概率为:因此，各数据例依依的joint p.m.f.为:xx =∏kj=1, xx =∏j=1k, xx =1k。因此，对数似然函数是

TODO:MATH

由于在最大似然估计中，我们通过使πj=pθ（yij∣xi）最大化目标函数l（θ）l（θ）。因此，对于任何多类分类，最大化上述对数似然函数l（θ）l（θ）等效于最小化CE损失CE（y，y ^）CE（y，y ^）。

为了测试以上证明，让我们应用内置的度量`NegativeLogLikelihood`。使用与前面的示例相同的标签和前缀，我们将得到与前面的示例相同的数值损失，直到小数点后5位。

TODO:CODE

## 小结

* 信息论是研究编码、解码、传输和操纵信息的学科。
* 熵是衡量有多少信息以不同的信号呈现的单位。
* KL散度也可以测量两个分布之间的散度。
* 交叉熵（Cross Entropy）可以看作是多类分类的目标函数。交叉熵损失的最小化等价于对数似然函数的最大化。

## 练习

1. 验证从第一部分的卡例子确实有声称的熵。
1. 结果表明，对于所有分布pp和qq, KL散度D(p∥q)是非负的。提示:使用Jensen不等式，即。,使用计算lnx−−日志⁡x是一个凸函数。Jensen 不等式令 f 为一个凸函数而 X 为一个随机变量。我们有下列不等式：E[f(X)] \geqslant f(E[X])
1. 让我们从以下几个数据源计算熵:
    * 假设你正在观察一只猴子在打字机前产生的输出。猴子随机地按下打字机的4444个键中的任何一个(可以假定它还没有发现任何特殊键或shift键)。你观察到每个角色有多少位的随机性?
    * 由于对猴子不满，你用一个喝醉的排字工人代替了它。它能够产生词汇，尽管不是连贯的。相反，它从2000 2000个单词中随机抽取一个单词。再假设英语单词的平均长度为4.54.5个字母。你现在观察到多少位随机?
    * 仍然对结果不满意，您将排字机替换为高质量的语言模型。目前，每个角色的困惑度可以低至1515分。perplexity定义为长度归一化概率，即
        PPL (x) = [p (x)] 1 /长度(x)。
        你现在观察到多少bits随机?
1. 直观地解释为什么我(X, Y) = H (X)−H (X | Y)我(X, Y) = H (X)−H (X | Y)。然后，通过将两边都表示为关于联合分布的期望来证明这是真的。
1. 两个高斯分布N(μ1,σ21)和 N(μ2,σ22)之间的KL散度是多少?

[2]: https://kexue.fm/archives/5253
