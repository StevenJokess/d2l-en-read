

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-07 14:29:58
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-07 15:16:45
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1111/chapter_appendix-mathematics-for-deep-learning/single-variable-calculus.html
-->

# 单变量微积分

在第2.4节中，我们了解了微积分的基本要素。本节将深入介绍微积分的基础知识，以及如何在机器学习的背景下理解和应用微积分。

## 微分学

微分学是从根本上研究功能在小变化下的行为的研究。要了解为什么这对于深度学习如此重要，让我们考虑一个示例。

假设我们有一个深度神经网络，为了方便起见，权重被连接到单个向量w =（w1，...，wn）中。给定训练数据集，我们考虑该数据集上的神经网络损失，我们将其写为L（w）。

此功能非常复杂，在此数据集上对给定体系结构的所有可能模型的性能进行编码，因此几乎不可能确定权重w的哪组将使损失最小化。因此，在实践中，我们通常从随机初始化权重开始，然后在使损耗尽可能快地减小的方向上反复采取小步骤。

这样，问题就变得表面上变得不那么容易了：我们如何找到使权重尽快减小的方向？ 为了对此进行深入研究，让我们首先检查一个仅具有单个权重的情况：对于单个实数值x，L（w）= L（x）。

让我们采用x并尝试了解当我们将其少量更改为x + ϵ时会发生什么。如果您想具体一点，请考虑类似ϵ = 0.0000001的数字。为了帮助我们可视化发生的情况，让我们在[0,3]上绘制示例函数f（x）= sin（xx）的图形。

TODO:CODE

在这么大的范围内，函数的行为并不简单。然而，如果我们将范围缩小到更小的值，比如[1.75,2.25][1.75,2.25]，我们可以看到图变得简单多了。

TODO:CODE

从极端的角度来说，如果我们放大到一个很小的部分，行为会变得简单得多:它只是一条直线。

TODO:CODE

这是对单个变量演算的关键观察：熟悉的函数的行为可以在足够小的范围内用一条线建模。这意味着对于大多数函数，可以合理地预期，随着我们将函数的x值稍微移动一点，输出f（x）也将稍微移动一点。我们需要回答的唯一问题是：“与输入的变化相比，输出的变化有多大？ 它大一半吗？ 两倍大？”

TODO:MATH

这已经足够在代码中使用了。例如，已知L(x)=x2+1701(x4)3L(x)=x2+1701(x4)3，那么我们可以看到在x=4x=4这一点上这个值有多大。

TODO:CODE

现在，如果我们细心观察，我们会注意到这个数字的输出非常接近88。事实上,如果我们减少ϵϵ,我们将看到值逐渐接近88。因此，我们可以正确地得出结论，我们寻求的值(输入变化的程度改变了输出)在x=4x=4点应该是88。数学家对这个事实进行编码的方式是

TODO:CODE



因此，对于函数输入的微小变化，我们可以考虑函数输出的变化比率。我们可以这样写

每行使用以下规则：链规则和对数导数。
    求和规则。
    常数，链式规则和幂规则的导数。
    和规则，线性函数的导数，常数的导数。
    在执行此示例后，应该清楚两件事：我们可以使用和，乘积，常数，幂，指数和对数写下的任何函数都可以通过遵循这些规则来机械地推导。
    让人类遵循这些规则可能是乏味且容易出错的！
    值得庆幸的是，这两个事实共同暗示了前进的方向：这是机械化的理想之选！ 确实，我们将在本节稍后部分回顾的反向传播正是这样。


TODO:CODE


TODO:CODE

最后一个方程值得明确指出。它告诉我们，如果你对任意一个函数的输入做少量的改变，那么输出也会随着这一小部分的改变而改变。通过这种方式，我们可以把导数理解为一个比例因子它告诉我们输入的变化对输出的影响有多大。

总而言之，二阶导数可以解释为描述函数ff曲线的方式。二阶导数为正，曲线向上，二阶导数为负，意味着ff向下，二阶导数为零，意味着ff根本不弯曲。

## 链式法则

现在我们来了解如何计算显式函数的导数。对微积分的完全正式的处理可以从第一原理推导出一切。在这里，我们不会沉溺于这种诱惑，而是提供对常见规则的理解。

如果我们心中有某个原始函数f(x)f(x)，我们可以计算前两个导数，并找到a,ba,b和cc的值，使它们与计算相匹配。类似于上节我们看到的一阶导数用直线给出了最好的近似，这个构造用二次函数给出了最好的近似。假设f(x)=sin(x)f(x)=sin (x)

第二，如果二阶导数是一个负常数，这意味着一阶导数是递减的。这意味着一阶导数可以从正开始，在某一点变为零，然后变为负。因此，函数ff本身增大，变平，然后减小。也就是说，函数ff向下弯曲，并且只有一个最大值，如图18.3.2所示。

TODO:PIC

第三，如果二阶导数总是零，那么一阶导数就不会改变它是常数!这意味着ff以固定的速率增加(或减少)，ff本身是一条直线，如图18.3.3所示。

TODO:PIC

总而言之，二阶导数可以解释为描述函数ff曲线的方式。二阶导数为正，曲线向上，二阶导数为负，意味着ff向下，二阶导数为零，意味着ff根本不弯曲。

让我们更进一步。考虑函数g(x)=ax2+bx+cg(x)=ax2+bx+c。我们可以计算它

如果我们心中有某个原始函数f(x)f(x)，我们可以计算前两个导数，并找到a,ba,b和cc的值，使它们与计算相匹配。类似于上节我们看到的一阶导数用直线给出了最好的近似，这个构造用二次函数给出了最好的近似。假设f(x)=sin(x)f(x)=sin (x)

TODO:CODE

在下一节中，我们将把这个概念扩展到泰勒级数的概念。

### 泰勒级数

泰勒级数提供了一种近似函数f(x)f(x)的方法，如果我们已知在x0x0点的第一阶nn导数的值，即。,{(x0), f (1) (x0), f (2) (x0), f (n) (x0)} {(x0), f (1) (x0), f (2) (x0), f (n) (x0)}。其思想是找到一个度nn多项式，它匹配x0x0处所有给定的导数。

我们在前一节看到了n=2n=2的情况通过一些代数运算就可以证明这一点

TODO:MATH

正如我们在上面看到的，分母22用来抵消我们对x2x2求导得到的22，而其他项都是零。同样的逻辑也适用于一阶导数和值本身。

如果我们把这个逻辑进一步推到n=3n=3，我们就会得出这个结论

TODO:MATH

其中6=3 2=3!6 = 3 2 = 3 !来自于前面的常数如果我们对x3x3求导。进一步，我们可以得到一个次神经网络多项式

符号


实际上，Pn(x)Pn(x)可以看作是我们的函数f(x)f(x)的最佳nn次多项式逼近。虽然我们不打算深入研究上述近似的误差，但有必要提一下无限极限。在这种情况下，对于行为良好的函数(称为实解析函数)，如cos(x)cos (x)或exex，我们可以写出无限项并近似相同的函数



让我们看看这在代码中是如何工作的，并观察增加泰勒近似度是如何使我们更接近所需的函数exex的。

TODO:CODE

泰勒级数有两个主要的应用:

1. *理论应用*:通常当我们试图理解一个过于复杂的函数时，使用泰勒级数可以使我们把它变成一个可以直接使用的多项式。
2. *数值应用*:一些函数如exex或cos(x)cos (x)对机器来说是难以计算的。它们可以以固定的精度存储值表(这是经常做的)，但仍然存在一些悬而未决的问题，比如cos(1)cos(1)的第1000位是多少?泰勒级数通常有助于回答这些问题。

## 小结

* 导数可以用来表示当输入发生少量变化时函数的变化。
* 可使用导数规则组合初等导数，以创建任意复杂的导数。
* 导数可以迭代得到二阶或更高阶导数。每一次递增都提供关于函数行为的更细粒度的信息。
* 利用单个数据点的导数信息，我们可以用泰勒级数中得到的多项式来近似表现良好的函数。

## 练习

1. x3−4x + 1的导数是什么？
1. log（1x）的导数是什么？
1. 是非题：如果f'（x）= 0，则f在x处具有最大值或最小值。
1. 当x≥0时f（x）= xlog（x）的最小值在哪里（我们假设f在f（0）处取极限值为0）？
