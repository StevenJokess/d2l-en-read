

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-05 17:06:57
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-05 18:34:28
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1117/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html
-->

# 几何和线性代数运算

在第2.3节中，我们学习了线性代数的基础知识，并了解了如何使用线性代数表示转换数据的常见操作。线性代数是支撑我们深度学习和更广泛的机器学习的许多工作的关键数学支柱之一。尽管2.3部分包含了足够的机制来传达现代深度学习模型的机制，但还有更多的内容。在这一节中，我们将更深入地探讨，重点介绍线性代数运算的几何解释，并介绍一些基本概念，包括特征值和特征矢量。

## 矢量的几何学

首先，我们需要讨论向量的两种常见的几何解释，作为空间中的点或方向。基本上，向量是一个数字列表，如下面的 Python 列表。

TODO:CODE

数学家们经常把它写成一个列或行向量，也就是说，要么写成

TODO:MATH

或

TODO:MATH

这些通常有不同的解释，其中数据点是列向量，用于形成加权和的权重是行向量。不过，灵活一点也是有好处的。矩阵是有用的数据结构: 它们允许我们组织具有不同变化方式的数据。例如，矩阵中的行可能对应不同的 house (数据点) ，而列可能对应不同的属性。如果你曾经使用过电子试算表或者读过2.2节，这听起来应该很熟悉。因此，尽管在表示表格数据集的矩阵中，单个矢量的默认方向是列矢量，但更常规的做法是将每个数据点作为矩阵中的行矢量。并且，正如我们将在后面的章节中看到的那样，这个约定将启用通用的深度学习实践。例如，沿着一个张量的最外层轴，我们可以访问或枚举数据点的小批次，或者如果没有小批次存在，只能枚举数据点。

给定一个矢量，我们首先要解释的是它是空间中的一个点。在两个或三个维度中，我们可以通过使用矢量的分量来定义这些点在空间中的位置，并与一个称为原点的固定参考点进行比较，从而可视化这些点。这可以在图18.1.1中看到。

TODO:FIG

图18.1.1作为平面上的点的可视化向量图。向量的第一个分量给出 x 坐标，第二个分量给出 y 坐标。更高的维度是类似的，尽管更难想象。

这种几何学的观点使我们能够在更抽象的层面上考虑这个问题。不再需要面对一些看似不可克服的问题，比如将图片分类为猫或狗，我们可以开始抽象地将任务视为空间中的点集合，并将任务视为发现如何将两个不同的点集合分开。

与此同时，人们经常把向量看作第二种观点: 空间方向。我们不仅可以把向量 v = [2,3] something v = [2,3] something 想象成位置22单位向右，33单位向上，我们还可以把它想象成方向本身，向右走22步向上走33步。用这种方法，我们把图18.1.2中的所有向量看作是一样的。

张量也可以在代码中灵活地操作。如2.3节所示，我们可以创建如下所示的张量。

TODO:CODE

爱因斯坦求和直接通过np.einsum实现。出现在爱因斯坦求和中的指标可以作为弦传递，后面是受作用的张量。例如，为了实现矩阵乘法，我们可以考虑上面看到的爱因斯坦求和(Av=aijvjAv=aijvj)，并去掉索引本身来实现

TODO:CODE

这是一个非常灵活的表示法。例如，如果我们想计算传统上写为

TODO:MATH

它可以通过爱因斯坦求和实现为

这个符号对人类来说是可读的和有效的，无论出于什么原因，如果我们需要通过程序生成一个张量收缩。由于这个原因，einsum通过为每个张量提供整数指标提供了另一种表示法。例如，同样的张量收缩也可以写成

TODO:CODE

这两种符号都允许在代码中简洁有效地表示张量的收缩。

## 总结

* 向量可以在几何上解释为空间中的点或方向。
* 点积将角度的概念定义为任意高维空间。
* 超平面是线和平面的高维概括。它们可用于定义决策平面，这些决策平面通常用作分类任务的最后一步。
* 矩阵乘法可以在几何上解释为基础坐标的均匀变形。它们代表了一种非常有限但数学上干净的向量转换方法。
* 线性相关性是一种判断向量集合何时处于比我们期望的空间空间小的方法的方法（例如，您有3个向量生活在二维空间中）。矩阵的等级是线性独立的其列的最大子集的大小。
* 定义矩阵的逆矩阵后，矩阵求逆可以让我们找到另一个矩阵，该矩阵可以消除第一个矩阵的作用。矩阵求逆理论上很有用，但由于数值不稳定，在实践中需要谨慎。
* 行列式使我们能够衡量矩阵扩展或缩小空间的程度。非零行列式表示一个可逆的（非奇异）矩阵，零值行列式表示该矩阵是不可逆的（奇异）。
* 张量收缩和爱因斯坦求和为表达机器学习中看到的许多计算提供了一种简洁明了的符号。

## 练习

1. 向量角是多少？TODO:MATH
1. 对错判断：TODO:MATH是彼此的逆矩阵吗？
1. 假设我们在平面上画一个面积为100m2的图形。用矩阵TODO:MATH变换后的面积是多少?
1. 下面哪个向量集是线性无关的?
1. 假设你有一个矩阵写成a =[cd] [ab] a =[cd] [ab]对于a,b,ca,b,c和dd。真或假:该矩阵的行列式始终为00
1. 向量e1=[10]和e2=[01]e2=[01]是正交的。矩阵AA的条件是什么使得Ae1Ae1和Ae2Ae2是正交的
1. 对于任意矩阵AA，如何用爱因斯坦符号写出tr(A4)tr(A4)
