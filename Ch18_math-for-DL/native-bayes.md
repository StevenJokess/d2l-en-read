

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-25 13:44:47
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-08 10:54:32
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_appendix-mathematics-for-deep-learning/naive-bayes.html
-->

# 朴素贝叶斯

在前几节中，我们了解了概率和随机变量的理论。为了使该理论发挥作用，让我们介绍*朴素贝叶斯分类器*。这仅使用概率基本原理允许我们执行数字分类。

学习全都在于假设。如果要对从未见过的新数据点进行分类，则必须对哪些数据点彼此相似做出一些假设。朴素的贝叶斯分类器是一种流行且非常清晰的算法，它假定所有特征彼此独立以简化计算。在本节中，我们将应用此模型来识别图像中的字符。

TODO:CODE

## 光学字符识别

MNIST [LeCun et al。，1998]是广泛使用的数据集之一。它包含60,000张用于训练的图像和10,000张用于验证的图像。每个图像包含一个从0到9的手写数字。任务是将每个图像分类为相应的数字。

Gluon在data.vision模块中提供了MNIST类，以自动从Internet检索数据集。随后，Gluon将使用已经下载的本地副本。我们通过将参数train的值分别设置为True或False来指定是请求训练集还是测试集。每个图像都是宽度和高度均为28且形状为(28,28,11)的灰度图像。我们使用自定义转换来删除最后一个通道尺寸。另外，数据集用无符号的8位整数表示每个像素。我们将它们量化为二进制特征以简化问题。

TODO:CODE

我们可以访问一个包含图像和相应标签的特定示例。

TODO:CODE

我们的示例存储在变量图像中，对应的图像高度和宽度为28像素。

TODO:CODE

我们的代码将每个图像的标签存储为标量。它的类型是32位整数。

TODO:CODE

我们也可以同时访问多个示例。

TODO:CODE

让我们把这些例子可视化。

TODO:CODE

## 分类的概率模型

在分类任务中，我们将示例映射到类别中。此处的示例是28×28灰度图像，类别是数字。（有关详细说明，请参见第3.4节。）表达分类任务的一种自然方法是通过概率问题：给定特征（即图像像素）最可能的标签是什么？ 用x∈Rd表示示例的特征，y∈R表示标签。这里的特征是图像像素，我们可以将2维图像重塑为矢量，从而d = 282 = 784，标签是数字。给定特征的标签的概率为p（y∣x）p（y∣x）。如果我们能够计算出这些概率，在我们的示例中，对于y = 0，...，9，这些概率为p（y∣x），则分类器将输出预测y ^ 由表达式给出：

y ^ = argmaxp（y∣x）。

不幸的是，这要求我们为x = x1，...，xdx = x1，...，xd的每个值估计p（y∣x）p（y∣x）。想象每个功能可以采用2个值之一。例如，功能x1 = 1x1 = 1可能表示在给定文档中出现了单词apple，而x1 = 0x1 = 0则表明没有出现。如果我们有3030个这样的二进制特征，那意味着我们需要准备对输入向量xx的230230个可能值（超过10亿个！）中的任何一个进行分类。

此外，学习在哪里？ 如果我们需要查看每个可能的示例以预测相应的标签，那么我们并不是在真正学习模式，而只是记住数据集。

## 朴素贝叶斯分类器

幸运的是，通过对条件独立性做一些假设，我们可以引入一些归纳偏差，并构建一个能够从相对有限的训练示例选择中进行推广的模型。首先，我们使用贝叶斯定理，来表示分类器为

TODO:MATH

注意，分母是归一化项p（x）p（x），它不取决于标签yy的值。结果，我们只需要担心比较不同yy值之间的分子。即使计算分母是很棘手的，只要我们能够评估分子，就可以忽略它。幸运的是，即使我们想恢复归一化常数，我们也可以。由于∑yp（y∣x）= 1∑yp（y∣x）= 1，我们总是可以恢复归一化项。

现在，让我们关注p（x∣y）p（x∣y）。使用概率链规则，我们可以将项p（x∣y）表示为

TODO:MATH

就其本身而言，这种表达方式并不能使我们走得更远。我们仍然必须估计大约2d2d参数。但是，如果我们假设要素在条件上彼此独立（给定标签），那么我们突然会处于更好的状态，因为该术语简化为∏ip（xi∣y）∏ip（xi∣y），这给了我们 预测变量

TODO:MATH

如果对每个i和y估计∏ip(xi=1∣y)，并保存其值为Pxy[i,y]，则Pxy为d×n矩阵，n为类数，y∈{1，…，n}。此外，我们对每个y估计p(y)，并将其保存在Py[y]中，PyPy是一个n长度向量。对于任何新的例子x，我们都可以计算

TODO:MATH

对于任何y。因此，我们的条件独立性假设使得我们的模型复杂度从特征数量的指数依赖性O(2dn)变为线性依赖性O(dn)。

## 训练

现在的问题是我们不知道Pxy和Py。因此，我们需要首先根据一些训练数据来估计它们的值。这是训练模型。估计PyPy并不难。由于我们仅处理1010个类，因此我们可以计算每个数字出现的次数nyny，然后将其除以数据总量nn。例如，如果数字8出现n8 = 5,800n8 = 5,800次，而我们总共有n = 60,000n = 60,000张图像，则概率估计为p（y = 8）= 0.0967。

TODO:CODE

现在开始讨论一些比较困难的事情Pxy。由于我们选择了黑白图像，因此p（xi∣y）表示像素i对于类y开启的可能性。就像之前我们可以计算出niy发生事件的次数并将其除以y发生的总数（即ny）一样。但有些问题令人困扰：某些像素可能永远不会是黑色的（例如，对于裁剪良好的图像，角落像素可能始终是白色的）。统计人员处理此问题的简便方法是将伪计数添加到所有出现的事件。因此，我们使用niy + 1而不是niy，而使用ny + 1代替ny。这也称为拉普拉斯平滑。它似乎是临时的，但是从贝叶斯的角度来看可能是出于动机。

TODO:CODE

通过可视化这些10×28×28的概率(对于每个类的每个像素)，我们可以得到一些看起来平均的数字。

现在我们可以使用(18.9.5)来预测新图像。给定xx，以下函数为每y年计算p(x∣y)p(y)。

TODO:CODE

这太糟糕了！ 为了找出原因，让我们看一下每个像素的概率。它们通常是介于0.001和1之间的数字。我们正在乘以784。值得一提的是，我们正在计算机上计算这些数字，因此指数的范围是固定的。发生的是我们遇到了数字下溢，即将所有较小的数字相乘会导致结果更小，直到将其四舍五入为零为止。我们在第18.7节中将其作为理论问题进行了讨论，但是在实践中我们清楚地看到了这一现象。

如该部分所述，我们通过使用logab = loga +logb的事实来解决此问题，即，我们切换到对数求和。即使a和b均为小数，对数值也应在适当的范围内。

因为对数是递增函数，所以18。9。5可以写成

y ^ = argmaxy∑i = 1 dlogpxy [xi, y] + logPy [y]。

我们可以实现以下稳定版本:

TODO:CODE

我们现在可以检验一下这个预测是否正确。

TODO:CODE

如果现在我们预测一些验证示例，则可以看到贝叶斯分类器运行良好。

TODO:CODE

最后，让我们计算分类器的整体准确性。

TODO:CODE

现代深度网络的错误率小于0.01。相对较差的性能归因于我们在模型中做出的错误统计假设：我们假设每个像素都是独立生成的，仅取决于标签。显然，这不是人类如何写数字，而且这种错误的假设导致我们过于幼稚的贝叶斯（Bayes）分类器的失败。

## 小结

* 使用贝叶斯规则，可以通过假设所有观察到的特征都是独立的来进行分类。
* 通过计算标签和像素值组合的出现次数，可以在数据集上训练该分类器。
* 数十年来，该分类器一直是垃圾邮件检测等任务的黄金标准。

## 练习

1. 考虑数据集[[0,0]，[0,1]，[1,0]，[1,1]] ，其标签由两个元素[0,1,1,0]的XOR给出。基于此数据集的朴素贝叶斯分类器的概率是多少？ 是否成功分类了我们的观点？ 如果没有，违反了哪些假设？
1. 假设我们在估计概率时未使用拉普拉斯平滑，并且在测试时到达的数据点包含在训练中从未观察到的值。该模型将输出什么？
1. 朴素的贝叶斯分类器是贝叶斯网络的一个特定示例，其中随机变量的依存关系使用图结构进行编码。尽管完整的理论已超出本节的范围（有关完整的详细信息，请参见[Koller＆Friedman，2009]），但请解释为什么在XOR模型中允许两个输入变量之间的显式依赖可以创建成功的分类器。
