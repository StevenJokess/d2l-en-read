

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-13 16:30:45
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-13 16:54:54
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1153/chapter_optimization/gd.html
-->

# 梯度下降法

在本节中，我们将介绍梯度下降的基本概念。 这是必要的简短说明。 参见[Boyd＆Vandenberghe，2004]，了解凸优化的详细介绍。 尽管后者很少直接在深度学习中使用，但了解梯度下降是理解随机梯度下降算法的关键。 例如，由于学习率过高，优化问题可能会有所不同。 在梯度下降中已经可以看到这种现象。 同样，预处理是梯度下降中的一种常用技术，并且可以延续到更高级的算法中。 让我们从一个简单的特殊情况开始。

## 一维的梯度下降法

一维梯度下降法是一个很好的例子来解释为什么梯度下降法算法可能会降低目标函数的值。考虑一些连续可微实值函数 f: r → r f: r → r。利用泰勒展开式(18.3节) ，我们得到

TODO:MATH

也就是说，在一次近似下，f (x +) f (x +)是由函数值 f (x) f (x)和 x 上的一阶导数 f ′(x) f ′(x)给出的。假设沿负梯度方向移动的小粒子将减小 f，这是合理的。为了保持简单，我们选择一个固定的步长 > 0 > 0，并选择 =-f ′(x) =-f ′(x)。把它插入到上面我们得到的泰勒展开式中

TODO:MATH

如果导数 f ′(x)≠0 f ′(x)≠0不消失，则 f ′2(x) > 0 f ′2(x) > 0。此外，我们总是可以选择足够小的高阶项变得无关紧要。于是我们到达了

TODO:MATH

这意味着，如果我们使用

TODO:MATH

为了迭代 x x，函数 f (x) f (x)的值可能会下降。因此，在梯度下降法中，我们首先选择一个初始值 x x 和一个常数 > 0 > 0，然后使用它们连续迭代 x x 直到达到停止条件，例如，当梯度 | f ′(x) | f ′(x) | f ′(x) | 达到足够小或迭代次数达到一定值时。

为了简单起见，我们选择目标函数 f (x) = x2f (x) = x2来说明如何实现梯度下降法。虽然我们知道 x = 0 x = 0是使 f (x) f (x)最小化的解决方案，但是我们仍然使用这个简单的函数来观察 x x 如何变化。一如既往，我们首先导入所有必需的模块。

TODO:MATH

接下来，我们使用 x = 10 x = 10作为初始值，并假设 = 0.2 = 0.2。使用梯度下降法迭代 x x 10次，我们可以看到 x x 的值最终接近最优解。

TODO:MATH

优化 x 上的进度可以绘制如下。

TODO:MATH

## 学习率

学习速率可由算法设计者设定。如果我们使用的学习率太小，会导致xx更新非常缓慢，需要更多的迭代才能得到更好的解决方案。为了说明在这种情况下会发生什么，请考虑一下相同的优化问题的进展情况，如:正如我们所看到的，即使经过10步，我们仍然离最佳解决方案很远。

TODO:CODE

相反，如果我们使用过高的学习率，|ηf'（x）||ηf'（x）| 对于一阶泰勒展开式而言，可能太大。 即，：eqref：gd-taylor中的术语O（η2f'2（x））O（η2f'2（x））可能变得有意义。 在这种情况下，我们不能保证xx的迭代将能够降低f（x）f（x）的值。 例如，当我们将学习速率设置为η=1.1η= 1.1时，xx超过了最优解x = 0x = 0并逐渐发散。

TODO:CODE

## 局部最小值

为了说明非凸函数会发生什么，请考虑f（x）=x⋅coscxf（x）=x⋅cos⁡cx的情况。 此函数具有无限多个局部最小值。 根据我们对学习率的选择以及对问题的适应程度，我们可能会得到许多解决方案之一。 下面的示例说明了（不切实际的）高学习率将如何导致较差的局部最小值。

TODO:CODE

## 多变量的梯度下降法

现在我们对单变量情况有了更好的直觉，让我们考虑x∈Rdx∈Rd的情况。 也就是说，目标函数f：Rd→Rf：Rd→R将向量映射为标量。 相应地，其梯度也是多元的。 它是由dd个偏导数组成的向量：

TODO:MATH

梯度中的每个偏导数元素∂f（x）/∂xi∂f（x）/∂xi表示ff在xx处相对于输入xixi的变化率。 和以前一样，在单变量情况下，我们可以对多元函数使用相应的泰勒逼近，以了解应该做什么。 特别是我们有

TODO:MATH

要迭代xx，函数f（x）f（x）的值可能会下降。 因此，在梯度下降中，我们首先选择一个初始值xx和一个常数η>0η> 0，然后使用它们连续迭代xx，直到达到停止条件为止，例如，当梯度的大小| f'（x） || f′（x）| 足够小或迭代次数已达到一定值。

TODO:MATH

换句话说，在最陡下降方向上最多的二阶项由负梯度-∇f（x）-∇f（x）给出。 选择合适的学习率η>0η> 0会产生原型梯度下降算法：

TODO:MATH

为了了解算法在实践中的行为，让我们构造一个目标函数f（x）= x21 + 2x22f（x）= x12 + 2x22，其中二维向量为x = [x1，x2]⊤x= [x1，x2] input作为输入，标量作为输出。 梯度由给出：∇f（x）= [2x1,4x2]⊤∇f（x）= [2x1,4x2] given。 我们将从初始位置[−5，−2] [− 5，−2]通过梯度下降观察xx的轨迹。 我们还需要两个辅助函数。 第一个使用更新功能，并将其应用于初始值2020次。 第二个助手可视化xx的轨迹。

TODO:CODE

接下来，我们针对学习率η=0.1η= 0.1观察优化变量xx的轨迹。 我们可以看到，经过20步后，xx的值接近[0,0] [0,0]的最小值。 尽管进展相当缓慢，但进展还是相当不错的。

TODO:CODE

## 自适应方法

正如我们在11.3.1.1节中所看到的，要使学习率ηη“恰到好处”是很棘手的。 如果选择的太小，我们将毫无进展。 如果选择的太大，则解决方案会振荡，在最坏的情况下甚至可能会发散。 如果我们可以自动确定ηη或完全不必选择步长怎么办？ 在这种情况下，不仅关注物镜的值和梯度而且关注物镜的曲率的二阶方法也可以提供帮助。 尽管这些方法由于计算成本而不能直接应用于深度学习，但是它们为如何设计高级优化算法提供了有用的直觉，这些算法模仿了以下概述的算法的许多理想特性。

为简单起见，我们选择目标函数f（x）= x2f（x）= x2来说明如何实现梯度下降。 尽管我们知道x = 0x = 0是最小化f（x）f（x）的解决方案，但我们仍然使用此简单函数来观察xx的变化。 与往常一样，我们首先导入所有必需的模块。

也就是说，作为优化问题的一部分，我们需要反转Hessian HfHf。

对于f（x）= 12x2f（x）= 12x2，我们有∇f（x）=x∇f（x）= x和Hf = 1Hf = 1。 因此，对于任何xx，我们获得ϵ = -xϵ = -x。 换句话说，一个步骤就足以完全收敛而无需任何调整！ las，自从泰勒展开式很精确以来，我们在这里有点幸运。 让我们看看其他问题会发生什么。

TODO:CODE

现在让我们看看当我们有一个非凸函数时会发生什么，例如f（x）= xcos（cx）f（x）=xcos⁡（cx）。 毕竟，请注意，在牛顿的方法中，我们最终除以黑森州。 这意味着，如果二阶导数为负，我们将朝着增加ff的方向前进。 这是该算法的致命缺陷。 让我们看看实际发生的情况。

TODO:CODE

这出奇地错了。 我们该如何解决？ 一种方法是改为采用绝对值来“修复”黑森州。 另一种策略是恢复学习率。 这似乎未能达到目的，但并非完全如此。 具有二阶信息可以使我们在曲率较大时保持谨慎，而在物镜平坦时可以采取更长的步伐。 让我们看看这是如何以较小的学习率（例如η=0.5η= 0.5）工作的。 如我们所见，我们有一个非常有效的算法。

TODO:CODE

## 凸度分析

用xkxk表示在第kk次迭代中xx的值，并令ek：= xk-x ∗ ek：= xk-x ∗是距最优性的距离。 通过泰勒级数展开式，我们可以将条件f′（x ∗）= 0f′（x ∗）= 0写成

0 = f'（xk-ek）= f'（xk）-ekf''（xk）+ 12e2kf'''（ξk）。

对于ξk∈[xk-ek，xk]ξk∈[xk-ek，xk]成立。 回想一下，我们有更新xk + 1 = xk-f'（xk）/ f''（xk）xk + 1 = xk-f'（xk）/ f''（xk）。 将以上扩展除以f′′（xk）f''（xk）得出

（11.3.10）

ek-f′（xk）/ f′′（xk）= 12e2kf′′′（ξk）/ f′′（xk）。

插入更新方程会导致以下边界ek +1≤e2kf'′′（ξk）/ f′（xk）ek +1≤ek2f‴（ξk）/ f′（xk）。 因此，每当我们在有界f′′′（ξk）/ f′′（xk）≤cf‴（ξk）/ f″（xk）≤c的区域中时，我们都会平方误差ek +1≤ce2kek +1≤cek2。

顺便说一句，优化研究人员将此称为线性收敛，而将ek +1≤αekek+1≤αek之类的条件称为恒定收敛率。 请注意，此分析有很多警告：当我们到达快速收敛的区域时，我们并没有太大的保证。 取而代之的是，我们只知道一旦达到目标，融合就会非常快。 其次，这要求ff具有良好的行为能力，可以胜任高阶导数。 归结为确保ff在如何更改其值方面不具有任何“令人惊讶”的属性。

## 先决条件

毫不奇怪，计算和存储完整的Hessian非常昂贵。 因此，期望找到替代方案。 改善问题的一种方法是避免整体计算Hessian，而仅计算对角线项。 尽管这不像完整的牛顿法那样好，但它仍然比不使用牛顿法要好得多。 此外，对主要对角线元素的估计是驱动随机梯度下降优化算法中某些创新的因素。 这导致形式的更新算法

（11.3.11）

x←x-ηdiag（Hf）-1∇x。

要了解为什么这是一个好主意，请考虑一种情况，其中一个变量表示以毫米为单位的高度，另一个变量表示以公里为单位的高度。 假设对于两种自然尺度，单位均为米，我们在参数化方面都会遇到严重的失配。 使用预处理可以消除这种情况。 有效地使用梯度下降进行预处理等于为每个坐标选择不同的学习率。

## 线搜索的梯度下降

梯度下降的关键问题之一是，我们可能会超额完成目标或进展不充分。 解决此问题的简单方法是将线搜索与梯度下降结合使用。 也就是说，我们使用usef（x）∇f（x）给出的方向，然后执行二进制搜索，以了解步长ηη使f（x-η∇f（x））f（x-η∇f（ X）） 。

该算法快速收敛（有关分析和证明，请参见[Boyd＆Vandenberghe，2004]）。 但是，出于深度学习的目的，这并不是那么可行，因为线搜索的每个步骤都需要我们评估整个数据集上的目标函数。 这太昂贵了，无法完成。

## 小结

* 学习率很重要。 太大了，我们就分歧了，太小了，我们就没有进步。
* 梯度下降可能会卡在局部最小值中。
* 在高维度上，调整学习率很复杂。
* 预处理可以帮助调整刻度。
* 牛顿的方法在凸性问题上开始正常工作后，速度会大大提高。
* 提防使用牛顿方法，而无需对非凸问题进行任何调整。

## 练习

1. 以不同的学习率和目标函数进行实验以进行梯度下降。
2. 实现线搜索以最小化区间[a，b] [a，b]中的凸函数。
   1. 您是否需要用于二分查找的导数，即决定选择[a，（a + b）/ 2]
   2. [a，（a + b）/ 2]还是[（a + b）/ 2，b] [ （a + b）/ 2，b
   3. 该算法的收敛速度有多快？
3. 实现该算法并将其应用于最小log（exp（x）+ exp（-2 * x-3）
4. log⁡（exp⁡（x）+exp⁡（-2 * x-3））。
5. 设计在R2R2上定义的目标函数，其中梯度下降非常慢。 提示：不同标的缩放比例不同。
6. 使用预处理条件来实现牛顿方法的轻量级版本：
    1. 使用对角粗麻布作为前置条件。
    2. 使用该值的绝对值，而不是实际的（可能是带符号的）值。
    3. 将其应用于上面的问题。
7. 将上述算法应用于许多目标函数（是否为凸函数）。 如果将坐标旋转45度会怎样？
