

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-03 15:24:55
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-03 15:43:20
 * @Description:
 * @TODO::MATH
 * @Reference:https://zh.d2l.ai/chapter_optimization/optimization-intro.html
 * http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/optimization-intro.html
-->

# 优化与深度学习

在本节中，我们将讨论优化和深度学习之间的关系，以及在深度学习中使用优化所面临的挑战。对于一个深度学习问题，我们通常首先定义一个损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尽量减少损失。在优化中，损失函数通常被称为优化问题的目标函数。按照传统和惯例，大多数优化算法都与最小化有关。如果我们需要最大化一个目标，有一个简单的解决办法:翻转这个函数的正负号。

虽然优化为深度学习提供了一种最小化损失函数的方法，但本质上，优化和深度学习的目标是完全不同的。前者主要是最小化一个目标，而后者则是在有限的数据量下找到一个合适的模型。在[4.4节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_multilayer-perceptrons/underfit-overfit.html#sec-model-selection)中，我们详细讨论了这两个目标之间的区别。例如，训练误差和泛化误差通常是不同的:由于优化算法的目标函数通常是一个基于训练数据集的损失函数然而，统计推断(以及深度学习)的目标是减少泛化误差。要实现后者，除了使用优化算法减少训练误差外，还需要注意过拟合。我们首先导入几个带有函数的库，以便在图中进行注释。

下图更详细地说明了这个问题。由于我们只有有限的数据量，所以训练误差的最小值可能与期望误差(或测试误差)的最小值在不同的位置。

可能最隐晦的问题是渐变的消失。例如，假设我们想最小化函数f(x)=tanh(x)我们从x=4开始。我们可以看到，f的梯度接近于nil。具体来说，f'(x)=1-tanh^2(x)，因此f^'(4)=0.0013。因此，在我们取得进展之前，优化将停滞很长一段时间。这就是为什么在引入ReLU激活函数之前，训练深度学习模型是相当棘手的原因之一。

正如我们所见，深度学习的优化充满了挑战。幸运的是，有一个健壮的算法范围，表现良好，易于使用，即使是初学者。此外，真的没有必要去寻找最好的解决方案。局部最优解甚至近似解仍然是非常有用的。

TODO:CODE

在图的鞍点位置，目标函数在 x 轴方向上是局部最小值，但在 y 轴方向上是局部最大值。

假设一个函数的输入为 k 维向量，输出为标量，那么它的海森矩阵（Hessian matrix）有 k 个特征值（参见附录中“数学基础”一节）。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点：

- 当函数的海森矩阵在梯度为0的位置上的特征值全为正时，该函数得到局部最小值。
- 当函数的海森矩阵在梯度为0的位置上的特征值全为负时，该函数得到局部最大值。
- 当函数的海森矩阵在梯度为0的位置上的特征值有正有负时，该函数得到鞍点。

对于高维问题，至少某些特征值是负的可能性相当高。这使得鞍点比局部极小点更有可能出现。我们将在下一节介绍凸性时讨论这种情况的一些例外情况。简而言之，凸函数就是那些Hessian的特征值不为负的函数。遗憾的是，大多数深度学习问题并不属于这一类。尽管如此，它是一个伟大的工具来研究优化算法。

## 小结

- 最小化训练误差并不能保证我们找到最小化期望误差的最佳参数集。
- 优化问题可能有许多局部极小值。
- 这个问题可能有更多的鞍点，因为通常这些问题不是凸的。消失梯度会导致优化失速。通常问题的重新参数化会有所帮助。
- 良好的参数初始化也是有益的。

## 练习

1. 考虑一个简单的多层感知器，它只有一个隐含层，比如隐含层中有 $d$ 维，并且只有一个输出。证明对于任何局部最小值都至少有$d!$ 行为相同的等价解。
1. 假设我们有一个对称随机矩阵MM，其中条目$M_ij=M_ji$都是从某个概率分布$p_ij$中抽取的。进一步假设pij(x)=pij(x)，即分布是对称的(例如，[Wigner, 1958])。证明特征值的分布也是对称的。即，对于任意特征向量vv，关联特征值的特征元个数的特征元个数的特征元个数的特征元个数的个数的特征元个数的个数的个数的个数的个数的个数的个数的个数的个数的个数的个数。为什么上面的不意味着P(生长率>)=0.5P(生长率>0)=0.5
1. 对于深度学习中的优化问题，你还能想到哪些其他的挑战？
1. 假设你想在（真的）马鞍上平衡一个（真实的）球。为什么这么难?你能利用这种效应来优化算法吗


