

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-07 13:17:16
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-07 13:24:17
 * @Description:
 * @TODO::
 * @Reference:
-->

特征值通常是我们在学习线性代数时会遇到的最有用的概念之一，然而，作为初学者，很容易忽略它们的重要性。下面，我们介绍特征分解，并试图传达一些为什么它如此重要的意义。假设我们有一个矩阵AA它有以下元素

如果我们将AA应用于任意向量v=[x,y] v=[x,y]，我们得到一个向量vA=[2x, y] vA=[2x, y]。这有一个直观的解释:在xx -方向上将矢量拉伸为两倍宽，然后在yy -方向上翻转。然而，有些向量的某些东西是不变的。即[1,0][1,0]被发送到[2,0][2,0]，[0,1][0,1]被发送到[0,1][0,1]。这些向量仍然在同一条直线上，唯一的改变是矩阵将它们分别拉伸了22倍和11倍。我们称这些向量为特征向量，它们的因子为stret

一般来说，如果我们能找到一个数倍的倍比和一个向量v，使(18.2.2)Av=倍比v。Av =λv。我们说，vv是AA的一个特征向量，而二比二是一个特征值。

## 总结

现在，我们可以确切地看到我们所希望的！ 用原理特征值对矩阵进行归一化后，我们看到随机数据不会像以前那样爆炸，而是最终平衡为一个特定值。 能够从第一性原理开始做这些事情将是一件很高兴的事，结果发现，如果我们深入研究它的数学运算，我们可以看到，一个独立均值为零，方差为一个高斯的大型随机矩阵的最大特征值 由于一个被称为循环法则的引人入胜的事实[Ginibre，1965]，平均条目平均约为n-√，在本例中为5-√≈2.2。 如[Pennington等人，2017]和后续工作中所讨论的，随机矩阵的特征值（和一个称为奇异值的相关对象）之间的关系已证明与神经网络的正确初始化有着深厚的联系。

## 小结

* 特征向量是由一个矩阵拉伸而不改变方向的向量。
* 特征值是特征向量被应用矩阵拉伸的量。
* 矩阵的特征分解可以使许多运算简化为对特征值的运算。
* 格尔什戈林圆定理可以给出矩阵特征值的近似值。
* 迭代矩阵幂的性质主要取决于最大特征值的大小。这种理解在神经网络初始化理论中有很多应用。

## 练习

1. 特征值和特征向量是什么
1. 下面矩阵的特征值和特征向量是什么，这个例子和之前的例子相比有什么奇怪的地方
1. 不计算特征值，下面矩阵的最小特征值是否可能小于0。50.5 ?注意:这个问题可以心算。
