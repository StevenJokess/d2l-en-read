

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-07 12:57:34
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-07 13:08:21
 * @Description:
 * @TODO::
 * @Reference:
-->

深度学习训练通常需要大量的计算。目前，gpu是深度学习中性价比最高的硬件加速器。特别是，与cpu相比，gpu更便宜，提供更高的性能，通常超过一个数量级。此外，一台服务器可以支持多个gpu，对于高端服务器，最多支持8个gpu。对于一个工程工作站，更典型的数字是高达4个gpu，因为热、冷却和电力需求迅速上升，超出了办公大楼所能支持的范围。对于更大的部署，云计算，如Amazon s P3和G4实例是一个更实用的解决方案。

通常不需要购买具有许多线程的高端cpu，因为大部分计算是在gpu上进行的。也就是说，由于Python中的全局解释器锁(GIL)，在我们有4-8个gpu的情况下，CPU的单线程性能会很重要。在所有条件相同的情况下，这意味着具有较少核心数量但较高时钟频率的cpu可能是更经济的选择。例如，当在6核4 GHz CPU和8核3.5 GHz CPU之间选择时，前者更可取，尽管它的聚合速度较低。

一个重要的考虑是gpu使用大量的能量，因此会消耗大量的热量。这需要非常好的冷却和足够大的机箱来使用gpu。如果可能的话，请遵循下面的指导方针

1. **电源供应**。  GPU消耗大量电量。 每台设备最高350W的预算（检查图形卡的峰值需求而不是典型需求，因为高效代码会消耗大量能量）。 如果您的电源不符合需求，您会发现系统变得不稳定。
2. **底盘尺寸**。  GPU很大，辅助电源连接器通常需要额外的空间。 而且，大型机箱更易于冷却。
3. **GPU散热**。 如果您拥有大量GPU，则可能需要投资于水冷。 同样，即使参考文献的风扇数量较少，也要针对参考设计，因为它们的厚度足以允许设备之间的进气。 如果您购买的是多风扇GPU，则可能太厚了，无法在安装多个GPU时获得足够的空气，并且会遇到热调节的问题。
4. **PCIe插槽**。 在GPU之间来回移动数据（以及在GPU之间交换数据）需要大量带宽。 我们建议使用16通道的PCIe 3.0插槽。 如果您安装了多个GPU，请确保仔细阅读主板说明，以确保同时使用多个GPU时16x带宽仍然可用，并且附加插槽的PCIe 2.0与PCIe 2.0相对。 一些主板在安装了多个GPU后降级为8倍甚至4倍带宽。 部分原因是CPU提供的PCIe通道数量。

简而言之，以下是构建深度学习服务器的一些建议：

初学者。 购买低功耗的低端GPU（适合深度学习的廉价游戏GPU 150-200W）。 如果幸运的话，您当前的计算机将支持它。
1个GPU。 具有4核的低端CPU就足够了，大多数主板就足够了。 瞄准至少32 GB的DRAM并投资用于本地数据访问的SSD。  600W的电源应足够。 购买拥有众多粉丝的GPU。
2个GPU。 具有4-6个内核的低端CPU就足够了。 瞄准64 GB DRAM并投资SSD。 两个高端GPU大约需要1000W。 在主板方面，请确保它们具有两个PCIe 3.0 x16插槽。 如果可以，请在PCIe 3.0 x16插槽之间获得一个在主板上有两个自由空间（60mm间距）的主板，以提供额外的空气。 在这种情况下，请购买两个拥有大量风扇的GPU。

4个GPU。 确保您购买的CPU具有相对较快的单线程速度（即较高的时钟频率）。 您可能需要具有更多PCIe通道的CPU，例如AMD Threadripper。 您可能需要相对昂贵的主板才能获得4个PCIe 3.0 x16插槽，因为它们可能需要PLX来多路复用PCIe通道。 购买具有参考设计的GPU，这些参考设计要窄一些，并且要在GPU之间畅通无阻。 您需要1600-2000W的电源，而办公室的电源插座可能不支持该电源。 该服务器可能会大声运行。 您不希望它在您的桌子下面。 建议使用128 GB的DRAM。 获取用于本地存储的SSD（1-2 TB NVMe）和RAID配置中的一堆硬盘以存储数据。

8个GPU。 您需要购买带有多个冗余电源的专用多GPU服务器机箱（例如，每个电源1600W的2 + 1）。 这将需要双插槽服务器CPU，256 GB ECC DRAM，快速网卡（建议10 GBE），并且您将需要检查服务器是否支持GPU的物理尺寸。 消费类和服务器GPU之间的气流和布线位置差异很大（例如RTX 2080与Tesla V100）。 这意味着您可能由于电源线的间隙不足或缺少合适的线束而无法在服务器中安装消费类GPU（这是合著者之一的痛苦发现）。

## 选择GPU

目前，AMD和NVIDIA是专用gpu的两大主要制造商。NVIDIA是第一个进入深度学习领域的公司，通过CUDA为深度学习框架提供了更好的支持。因此，大多数买家选择NVIDIA gpu。

NVIDIA提供两种类型的gpu，针对个人用户(例如，通过GTX和RTX系列)和企业用户(通过特斯拉系列)。这两种类型的gpu提供了相当的计算能力。然而，企业用户gpu通常使用(被动)强制冷却、更多内存和ECC(错误纠正)内存。这些gpu更适合数据中心，通常比消费者gpu贵十倍。

如果你是一个拥有超过100台服务器的大公司，你应该考虑NVIDIA Tesla系列或者使用云计算中的GPU服务器。对于拥有10台以上服务器的实验室或中小型公司来说，NVIDIA RTX系列可能是最划算的。你可以购买预配置的服务器，配有Supermicro或华硕机壳，有效地容纳4-8个gpu。

GPU供应商通常每1-2年发布新一代GPU，比如2017年发布的GTX 1000 (Pascal)系列和2019年发布的RTX 2000 (Turing)系列。每个系列都提供了几种不同的模型，它们提供不同的性能级别。GPU性能主要是以下三个参数的组合：

1. **计算能力**。通常我们寻找32位浮点计算能力。16位浮点训练(FP16)也正在进入主流。如果您只对预测感兴趣，也可以使用8位整数。最新一代的图灵gpu提供4位加速。遗憾的是，目前用于训练低精度网络的算法还没有得到广泛应用。
2. **内存大小**。当你的模型变大或训练期间使用的批变大时，你将需要更多的GPU内存。检查HBM2(高带宽内存)和GDDR6(图形DDR)内存。HBM2更快，但更贵。
3. **内存带宽**。只有当你有足够的内存带宽时，你才能充分利用你的计算能力。如果使用GDDR6，请查找宽内存总线。

对于大多数用户来说，查看计算能力就足够了。注意，许多gpu提供不同类型的加速。例如，英伟达的TensorCores可将操作符子集的速度提高5倍。确保您的库支持这一点。GPU内存应该不少于4 GB (8 GB更好)。尽量避免使用GPU来显示GUI(使用内置图形代替)。如果你不能避免它，增加额外的2gb内存的安全。

图19.5.1比较了各种GTX 900、GTX 1000和RTX 2000系列型号的32位浮点计算能力和价格。价格是维基百科上的建议价格。

我们可以看到很多东西:

1. 在每个系列中，价格和性能大致成比例。泰坦模型的GPU内存容量更大。然而，较新的模式提供更好的成本效益，可以通过比较980ti和1080ti看到。RTX 2000系列的价格似乎没有提高多少。然而，这是由于事实，他们提供了优越的低精度性能(FP16, INT8和INT4)。
2. GTX 1000系列的性价比大约是900系列的两倍。
3. 对于rtx2000系列，价格是价格的仿射函数。

图19.5.2显示了能耗是如何随着计算量呈线性变化的。第二，后代的效率更高。这似乎与rtx2000系列所对应的图形相矛盾。然而，这是由于腱核吸收了不成比例的能量。

在构建服务器时，要注意电源、PCIe总线、CPU单线程速度和冷却。如果可能的话，你应该购买最新一代的GPU。对大型部署使用云。高密度服务器可能不兼容所有gpu。在购买前检查机械和冷却的规格。使用FP16或更低的精度来提高效率。
