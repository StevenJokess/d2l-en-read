{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "movielens-pt.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8VBAHjom5QB",
        "outputId": "ff7b01d9-77f5-48ce-d698-81fb8fd791ab"
      },
      "source": [
        "!pip install -U git+https://github.com/d2l-ai/d2l-en.git@master"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/d2l-ai/d2l-en.git@master\n",
            "  Cloning https://github.com/d2l-ai/d2l-en.git (to revision master) to /tmp/pip-req-build-w1h4fxui\n",
            "  Running command git clone -q https://github.com/d2l-ai/d2l-en.git /tmp/pip-req-build-w1h4fxui\n",
            "Requirement already satisfied, skipping upgrade: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.19.4)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.1.5)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.0.1)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (7.5.1)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.1) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.7.0)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (20.0.0)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.0.8)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.1) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->d2l==0.15.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->d2l==0.15.1) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->d2l==0.15.1) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->d2l==0.15.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (20.8)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (51.0.0)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.15.1) (0.2.5)\n",
            "Building wheels for collected packages: d2l\n",
            "  Building wheel for d2l (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d2l: filename=d2l-0.15.1-cp36-none-any.whl size=72690 sha256=01ebeaf6771e2a46f6e1534448c24e6e6cba206c8f0d8b80fb2b93bf616931da\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h4ptwd8v/wheels/0f/41/8f/72ece70ede8a0e37eec72c03087eb4604925ba212b804f8cad\n",
            "Successfully built d2l\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MPhePUTmdmF"
      },
      "source": [
        "from d2l import torch as d2l\r\n",
        "import torch\r\n",
        "from torch import nn\r\n",
        "import os\r\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPjIuex7mpuJ"
      },
      "source": [
        "#@save\r\n",
        "d2l.DATA_HUB['ml-100k'] = (\r\n",
        "    'http://files.grouplens.org/datasets/movielens/ml-100k.zip',\r\n",
        "    'cd4dcac4241c8a4ad7badc7ca635da8a69dddb83')\r\n",
        "\r\n",
        "#@save\r\n",
        "def read_data_ml100k():\r\n",
        "    data_dir = d2l.download_extract('ml-100k')\r\n",
        "    names = ['user_id', 'item_id', 'rating', 'timestamp']\r\n",
        "    data = pd.read_csv(os.path.join(data_dir, 'u.data'), '\\t', names=names,\r\n",
        "                       engine='python')\r\n",
        "    num_users = data.user_id.unique().shape[0]\r\n",
        "    num_items = data.item_id.unique().shape[0]\r\n",
        "    return data, num_users, num_items"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIcZ_HSjncPz",
        "outputId": "4546c0c3-662f-4910-a3f3-7dde639e3d32"
      },
      "source": [
        "data, num_users, num_items = read_data_ml100k()\r\n",
        "sparsity = 1 - len(data) / (num_users * num_items)\r\n",
        "print(f'number of users: {num_users}, number of items: {num_items}')\r\n",
        "print(f'matrix sparsity: {sparsity:f}')\r\n",
        "print(data.head(5))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/ml-100k.zip from http://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "number of users: 943, number of items: 1682\n",
            "matrix sparsity: 0.936953\n",
            "   user_id  item_id  rating  timestamp\n",
            "0      196      242       3  881250949\n",
            "1      186      302       3  891717742\n",
            "2       22      377       1  878887116\n",
            "3      244       51       2  880606923\n",
            "4      166      346       1  886397596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "cR8a--B1ngpd",
        "outputId": "2690101a-a428-4376-c617-da4da94510f8"
      },
      "source": [
        "d2l.plt.hist(data['rating'], bins=5, ec='black')\r\n",
        "d2l.plt.xlabel('Rating')\r\n",
        "d2l.plt.ylabel('Count')\r\n",
        "d2l.plt.title('Distribution of Ratings in MovieLens 100K')\r\n",
        "d2l.plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8_KseZnkI1"
      },
      "source": [
        "#@save\r\n",
        "def split_data_ml100k(data, num_users, num_items,\r\n",
        "                      split_mode='random', test_ratio=0.1):\r\n",
        "    \"\"\"Split the dataset in random mode or seq-aware mode.\"\"\"\r\n",
        "    if split_mode == 'seq-aware':\r\n",
        "        train_items, test_items, train_list = {}, {}, []\r\n",
        "        for line in data.itertuples():\r\n",
        "            u, i, rating, time = line[1], line[2], line[3], line[4]\r\n",
        "            train_items.setdefault(u, []).append((u, i, rating, time))\r\n",
        "            if u not in test_items or test_items[u][-1] < time:\r\n",
        "                test_items[u] = (i, rating, time)\r\n",
        "        for u in range(1, num_users + 1):\r\n",
        "            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))\r\n",
        "        test_data = [(key, *value) for key, value in test_items.items()]\r\n",
        "        train_data = [item for item in train_list if item not in test_data]\r\n",
        "        train_data = pd.DataFrame(train_data)\r\n",
        "        test_data = pd.DataFrame(test_data)\r\n",
        "    else:\r\n",
        "        mask = [True if x == 1 else False for x in np.random.uniform(\r\n",
        "            0, 1, (len(data))) < 1 - test_ratio]\r\n",
        "        neg_mask = [not x for x in mask]\r\n",
        "        train_data, test_data = data[mask], data[neg_mask]\r\n",
        "    return train_data, test_data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWOUObuBqHWi"
      },
      "source": [
        "https://discuss.pytorch.org/t/generating-random-tensors-according-to-the-uniform-distribution-pytorch/53030\r\n",
        "\r\n",
        "toch.rand returns a tensor samples uniformly in [0, 1). Scaling it as shown in your example should work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCGSIJGXnrvg"
      },
      "source": [
        "#@save\r\n",
        "def split_data_ml100k(data, num_users, num_items,\r\n",
        "                      split_mode='random', test_ratio=0.1):\r\n",
        "    \"\"\"Split the dataset in random mode or seq-aware mode.\"\"\"\r\n",
        "    if split_mode == 'seq-aware':\r\n",
        "        train_items, test_items, train_list = {}, {}, []\r\n",
        "        for line in data.itertuples():\r\n",
        "            u, i, rating, time = line[1], line[2], line[3], line[4]\r\n",
        "            train_items.setdefault(u, []).append((u, i, rating, time))\r\n",
        "            if u not in test_items or test_items[u][-1] < time:\r\n",
        "                test_items[u] = (i, rating, time)\r\n",
        "        for u in range(1, num_users + 1):\r\n",
        "            train_list.extend(sorted(train_items[u], key=lambda k: k[3]))\r\n",
        "        test_data = [(key, *value) for key, value in test_items.items()]\r\n",
        "        train_data = [item for item in train_list if item not in test_data]\r\n",
        "        train_data = pd.DataFrame(train_data)\r\n",
        "        test_data = pd.DataFrame(test_data)\r\n",
        "    else:\r\n",
        "        mask = [True if x == 1 else False for x in (len(data))*torch.rand()< 1 - test_ratio]\r\n",
        "        neg_mask = [not x for x in mask]\r\n",
        "        train_data, test_data = data[mask], data[neg_mask]\r\n",
        "    return train_data, test_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gxBSxaDqwFQ"
      },
      "source": [
        "#@save\r\n",
        "def load_data_ml100k(data, num_users, num_items, feedback='explicit'):\r\n",
        "    users, items, scores = [], [], []\r\n",
        "    inter = torch.zeros((num_items, num_users)) if feedback == 'explicit' else {}\r\n",
        "    for line in data.itertuples():\r\n",
        "        user_index, item_index = int(line[1] - 1), int(line[2] - 1)\r\n",
        "        score = int(line[3]) if feedback == 'explicit' else 1\r\n",
        "        users.append(user_index)\r\n",
        "        items.append(item_index)\r\n",
        "        scores.append(score)\r\n",
        "        if feedback == 'implicit':\r\n",
        "            inter.setdefault(user_index, []).append(item_index)\r\n",
        "        else:\r\n",
        "            inter[item_index, user_index] = score\r\n",
        "    return users, items, scores, inter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACw_odNVrE0v"
      },
      "source": [
        "from torch.utils import data"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdsEaFozq05z"
      },
      "source": [
        "#@save\r\n",
        "def split_and_load_ml100k(split_mode='seq-aware', feedback='explicit',\r\n",
        "                          test_ratio=0.1, batch_size=256):\r\n",
        "    data, num_users, num_items = read_data_ml100k()\r\n",
        "    train_data, test_data = split_data_ml100k(\r\n",
        "        data, num_users, num_items, split_mode, test_ratio)\r\n",
        "    train_u, train_i, train_r, _ = load_data_ml100k(\r\n",
        "        train_data, num_users, num_items, feedback)\r\n",
        "    test_u, test_i, test_r, _ = load_data_ml100k(\r\n",
        "        test_data, num_users, num_items, feedback)\r\n",
        "    train_set = data.TensorDataset(\r\n",
        "        torch.tensor(train_u), torch.tensor(train_i), torch.tensor(train_r))\r\n",
        "    test_set = data.ArrayDataset(\r\n",
        "        torch.tensor(test_u), torch.tensor(test_i), torch.tensor(test_r))\r\n",
        "    train_iter = data.DataLoader(\r\n",
        "        train_set, shuffle=True, \r\n",
        "        batch_size=batch_size)\r\n",
        "    test_iter = data.DataLoader(\r\n",
        "        test_set, batch_size=batch_size)\r\n",
        "    return num_users, num_items, train_iter, test_iter"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsFafnY44qI9"
      },
      "source": [
        "last_batch='rollover'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7f4jE7f5lMa"
      },
      "source": [
        "class MF(nn.Block):\r\n",
        "    def __init__(self, num_factors, num_users, num_items, **kwargs):\r\n",
        "        super(MF, self).__init__(**kwargs)\r\n",
        "        self.P = nn.Embedding(input_dim=num_users, output_dim=num_factors)\r\n",
        "        self.Q = nn.Embedding(input_dim=num_items, output_dim=num_factors)\r\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\r\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\r\n",
        "\r\n",
        "    def forward(self, user_id, item_id):\r\n",
        "        P_u = self.P(user_id)\r\n",
        "        Q_i = self.Q(item_id)\r\n",
        "        b_u = self.user_bias(user_id)\r\n",
        "        b_i = self.item_bias(item_id)\r\n",
        "        outputs = (P_u * Q_i).sum(axis=1) + np.squeeze(b_u) + np.squeeze(b_i)\r\n",
        "        return outputs.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLdRIZa-77a4"
      },
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.squeeze.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "hJ6hoCR35NBt",
        "outputId": "21e8ee8f-69b9-4313-ee4e-a2a68ec61f5f"
      },
      "source": [
        "class MF(nn.Block):\r\n",
        "    def __init__(self, num_factors, num_users, num_items, **kwargs):\r\n",
        "        super(MF, self).__init__(**kwargs)\r\n",
        "        self.P = nn.Embedding(num_embeddings=num_users, embedding_dim=num_factors)\r\n",
        "        self.Q = nn.Embedding(num_embeddings=num_items, embedding_dim=num_factors)\r\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\r\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\r\n",
        "\r\n",
        "    def forward(self, user_id, item_id):\r\n",
        "        P_u = self.P(user_id)\r\n",
        "        Q_i = self.Q(item_id)\r\n",
        "        b_u = self.user_bias(user_id)\r\n",
        "        b_i = self.item_bias(item_id)\r\n",
        "        outputs = (P_u * Q_i).sum(axis=1) + torch.squeeze(b_u) + torch.squeeze(b_i)\r\n",
        "        return outputs.flatten()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-adc74b9d85f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_factors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_factors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'Block'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk1A556k53wn"
      },
      "source": [
        "class MF(nn.Module):\r\n",
        "    def __init__(self, num_factors, num_users, num_items, **kwargs):\r\n",
        "        super(MF, self).__init__(**kwargs)\r\n",
        "        self.P = nn.Embedding(num_embeddings=num_users, embedding_dim=num_factors)\r\n",
        "        self.Q = nn.Embedding(num_embeddings=num_items, embedding_dim=num_factors)\r\n",
        "        self.user_bias = nn.Embedding(num_users, 1)\r\n",
        "        self.item_bias = nn.Embedding(num_items, 1)\r\n",
        "\r\n",
        "    def forward(self, user_id, item_id):\r\n",
        "        P_u = self.P(user_id)\r\n",
        "        Q_i = self.Q(item_id)\r\n",
        "        b_u = self.user_bias(user_id)\r\n",
        "        b_i = self.item_bias(item_id)\r\n",
        "        outputs = (P_u * Q_i).sum(axis=1) + torch.squeeze(b_u) + torch.squeeze(b_i)\r\n",
        "        return outputs.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7zYtaTn9gn4"
      },
      "source": [
        "https://pytorch-lightning.readthedocs.io/en/0.8.3/api/pytorch_lightning.metrics.regression.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DC3lrBTEcwb"
      },
      "source": [
        "def evaluator(net, test_iter, devices):\r\n",
        "    rmse = mx.metric.RMSE()  # Get the RMSE\r\n",
        "    rmse_list = []\r\n",
        "    for idx, (users, items, ratings) in enumerate(test_iter):\r\n",
        "        u = gluon.utils.split_and_load(users, devices, even_split=False)\r\n",
        "        i = gluon.utils.split_and_load(items, devices, even_split=False)\r\n",
        "        r_ui = gluon.utils.split_and_load(ratings, devices, even_split=False)\r\n",
        "        r_hat = [net(u, i) for u, i in zip(u, i)]\r\n",
        "        rmse.update(labels=r_ui, preds=r_hat)\r\n",
        "        rmse_list.append(rmse.get()[1])\r\n",
        "    return float(np.mean(np.array(rmse_list)))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRgUqn_QGO2g"
      },
      "source": [
        "def evaluator(net, test_iter, devices):\r\n",
        "    rmse = pytorch_lightning.metrics.RMSE()  # Get the RMSE\r\n",
        "    rmse_list = []\r\n",
        "    for idx, (users, items, ratings) in enumerate(test_iter):\r\n",
        "        u = gluon.utils.split_and_load(users, devices, even_split=False)\r\n",
        "        i = gluon.utils.split_and_load(items, devices, even_split=False)\r\n",
        "        r_ui = gluon.utils.split_and_load(ratings, devices, even_split=False)\r\n",
        "        r_hat = [net(u, i) for u, i in zip(u, i)]\r\n",
        "        rmse.update(labels=r_ui, preds=r_hat)\r\n",
        "        rmse_list.append(rmse.get()[1])\r\n",
        "    return float(np.mean(np.array(rmse_list)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDcKxHpjBYCe"
      },
      "source": [
        "https://discuss.pytorch.org/t/l2-loss-in-pytorch/13684\r\n",
        "l2loss\r\n",
        "MSELoss:https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWcfwo7wEgO2"
      },
      "source": [
        "#@save\r\n",
        "def train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,\r\n",
        "                        devices=d2l.try_all_gpus(), evaluator=None,\r\n",
        "                        **kwargs):\r\n",
        "    timer = d2l.Timer()\r\n",
        "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0, 2],\r\n",
        "                            legend=['train loss', 'test RMSE'])\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        metric, l = d2l.Accumulator(3), 0.\r\n",
        "        for i, values in enumerate(train_iter):\r\n",
        "            timer.start()\r\n",
        "            input_data = []\r\n",
        "            values = values if isinstance(values, list) else [values]\r\n",
        "            for v in values:\r\n",
        "                input_data.append(gluon.utils.split_and_load(v, devices))\r\n",
        "            train_feat = input_data[0:-1] if len(values) > 1 else input_data\r\n",
        "            train_label = input_data[-1]\r\n",
        "            with autograd.record():\r\n",
        "                preds = [net(*t) for t in zip(*train_feat)]\r\n",
        "                ls = [loss(p, s) for p, s in zip(preds, train_label)]\r\n",
        "            [l.backward() for l in ls]\r\n",
        "            l += sum([l.asnumpy() for l in ls]).mean() / len(devices)\r\n",
        "            trainer.step(values[0].shape[0])\r\n",
        "            metric.add(l, values[0].shape[0], values[0].size)\r\n",
        "            timer.stop()\r\n",
        "        if len(kwargs) > 0:  # It will be used in section AutoRec\r\n",
        "            test_rmse = evaluator(net, test_iter, kwargs['inter_mat'],\r\n",
        "                                  devices)\r\n",
        "        else:\r\n",
        "            test_rmse = evaluator(net, test_iter, devices)\r\n",
        "        train_l = l / (i + 1)\r\n",
        "        animator.add(epoch + 1, (train_l, test_rmse))\r\n",
        "    print(f'train loss {metric[0] / metric[1]:.3f}, '\r\n",
        "          f'test RMSE {test_rmse:.3f}')\r\n",
        "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\r\n",
        "          f'on {str(devices)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RlBa8d9lBxvq"
      },
      "source": [
        "devices = d2l.try_all_gpus()\r\n",
        "num_users, num_items, train_iter, test_iter = d2l.split_and_load_ml100k(\r\n",
        "    test_ratio=0.1, batch_size=512)\r\n",
        "net = MF(30, num_users, num_items)\r\n",
        "\r\n",
        "lr, num_epochs, wd = 0.002, 20, 1e-5\r\n",
        "loss = nn.MSELoss(reduction = 'sum')\r\n",
        "trainer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\r\n",
        "TODO:train_recsys_rating(net, train_iter, test_iter, loss, trainer, num_epochs,\r\n",
        "                    devices, evaluator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGbm58wiD6VF"
      },
      "source": [
        "scores = net(torch.tensor([20], dtype='int', device=devices[0]),\r\n",
        "             torch.tensor([30], dtype='int', device=devices[0]))\r\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}