

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 14:46:13
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-09 14:46:15
 * @Description:
 * @TODO::
 * @Reference:https://my.oschina.net/u/4591990/blog/4504545
-->

 模型中dropout在训练和测试的区别？
Dropout 是在训练过程中以一定的概率的使神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。

Dropout 在训练时采用，是为了减少神经元对部分上层神经元的依赖，类似将多个不同网络结构的模型集成起来，减少过拟合的风险。而在测试时，应该用整个训练好的模型，因此不需要dropout。

如何平衡训练和测试时的差异呢？在训练时以一定的概率使神经元失活，实际上就是让对应神经元的输出为0。假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。因此在训练时还要对第二层的输出数据除以（1-p）之后再传给输出层神经元，作为神经元失活的补偿，以使得在训练时和测试时每一层输入有大致相同的期望。

这里我回答错误了，因为我回答成了是在测试的时候，对输出数据乘上p保证训练和输出有大致的期望。其实是在训练的时候除以（1-p）作为补偿，而测试阶段不做处理，相当于去掉dropout层
