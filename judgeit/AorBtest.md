

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 14:03:13
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-09 14:03:29
 * @Description:
 * @TODO::
 * @Reference:https://www.cnblogs.com/nxf-rabbit75/p/10152099.html
-->

.A/B测试的陷阱
问题1：在对模型进行充分的离线评估之后，为什么还要进行在线A/B测试？
原因：（1）离线评估无法完全消除模型过拟合的影响；

（2）离线评估无法完全还原线上的工程环境；

（3）线上系统的某些商业指标在离线评估中无法计算。



问题2：如何进行线上A/B测试？
进行用户分桶，即将用户分成实验组和对照组，对实验组的用户施以新模型，对对照组的用户施以旧模型，在分桶的过程中，要注意样本的独立性和采样方式的无偏性，确保同一个用户每次只能分到同一个桶中，在分桶过程中所选取的user_id需要是一个随机数，这样才能保证桶中的样本是无偏的。



问题3：如何划分实验组和对照组？
例子：H公司的算法工程师们最近针对系统中的“美国用户”研发了一套全新的视频推荐模型A，而目前正在使用的针对全体用户的推荐模型是B。在正式上线之前，工程师们希望通过A/B测试来验证新推荐模型效果。
正确的做法是将所有美国用户根据user_id个位数划分为试验组合对照组，分别施以模型A和B，才能验证模型A的效果。
