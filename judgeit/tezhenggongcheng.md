什么是特征工程？为什么特征工程对机器学习很重要？

特征工程指的是使用专业知识和技巧来处理数据，使得特征在机器学习算法上发挥更好的作用的过程。这个过程包含了数据预处理，特征构建，特征筛选等。特征工程的目的就是筛选出好的特征，得到更好的训练数据，使模型达到更好的效果。
从数据中提取出来的特征好坏会直接影响到模型的效果，有的时候，如果特征工程做得好，仅使用一些简单的机器学习算法，也能达到很好的效果。由此可见特征工程在实际的机器学习中的重要性。


Q：特征工程的一般步骤是什么？什么是特征工程的迭代？

特征工程常规步骤：

数据获取，数据的可用性评估（覆盖率，准确率，获取难度）
探索性数据分析，对数据和特征有一个大致的了解，同时进行数据的质量检验，包括缺失值，异常值，重复值，一致性，正确性等。
特征处理，包括数据预处理和特征转换两部分，数据预处理主要做清洗工作（缺失值，异常值，错误值，数据格式），特征转换即对连续特征，离散特征，时间序列特征进行转换，便于入模。
特征构建，特征构建的目的是找寻与目标变量相关且区分度较好的特征。常用的方法有特征交叉，四则运算，基于业务理解进行头脑风暴构建特征等。
特征筛选，大量的特征中选择少量的有用特征，也叫作特征降维，常用的方法有过滤法，包装法，嵌入法。

特征工程的迭代:

选择特征：具体问题具体分析，通过查看大量的数据和基于对业务的理解，从数据中查找可以提出出数据的关键。
设计特征：可以自动进行特征提取工作，也可以手工进行特征的构建。
选择特征：使用不同的特征构造方法，从多个角度来评判这个特征是否适合放入模型中。
计算模型：计算模型在该特征上所提升的准确率。
上线测试：通过在线测试的效果来评估特征是否有效。


Q：常用的特征工程方法有哪些？

特征处理：数据的预处理包括异常值和缺失值，要根据实际的情况来处理。特征转换主要有标准化，归一化，区间缩放，二值化等，根据特征类型的不同选择合适的转换方法。
特征构建：特征之间的四则运算（有业务含义）,基于业务理解构造特征，分解类别特征，特征交叉组合等。
特征筛选：过滤法，封装法，嵌入法。


Q：在实际的风控建模中怎么做好特征工程？

本人工作中的一些经验总结：

因为做风控模型大部分的数据源来自第三方，所以第三方数据的可用性评估非常重要，一方面需要了解这些特征底层的衍生逻辑，判断是否与目标变量相关。另一方面考察数据的覆盖率和真实性，覆盖率较低和真实性存疑的特征都不能使用在模型中。
基于金融的数据特点，在特征筛选这个步骤上考量的因素主要有：一个是时间序列上的稳定性，衡量的指标可以是PSI，方差或者IV。一个是特征在样本上覆盖率，也就是特征的缺失率不能太高。另外就是特征的可解释性，特征与目标变量的关系要在业务上要解释的通。
如果第三方返回有用户的原始底层数据，例如社保的缴纳记录，运营商的通话/短信记录，则需要在特征衍生上多下功夫，基于自身对数据的敏感性和业务的理解，构建具有金融，风险属性的特征，也可以与业务部门进行沟通找寻与业务相关的特征。


Q：实际项目中原始数据通常有哪些问题？你是如何解决的？

一些特征的底层逻辑不清晰，字面上的意思可能与实际的衍生逻辑相悖，这个需要与第三方数据供应商进行沟通，了解清楚特征的衍生逻辑。
数据的真实性可能存在问题。比如一个特征是历史总计，但第三方只是爬取了用户近2年的数据，这样的特征就不符合用户的真实情况。所以对数据的真实性校验显得非常重要。
有缺失的特征占的比例较高。在进行缺失值处理前先分析缺失的原因，而不是盲目的进行填充，删除等工作。另外也要分析缺失是否有风险属性，例如芝麻分缺失的用户相对来说风险会较高，那么缺失可以当做一个类别来处理。
大量多类特征如何使用。例如位置信息，设备信息这些特征类别数较多，如果做亚编码处理会造成维度灾难，目前常用的方法一个是降基处理，减少类别数，另一个是用xgboost来对类别数做重要性排序，筛选重要性较高的类别再做亚编码处理。


Q：在做评分卡或其他模型中，怎么衡量特征(数据)的有用性？

特征具有金融风险属性，且与目标变量的关系在业务上有良好的可解释性。
特征与目标变量是高度相关的，衡量的指标主要是IV。
特征的准确率，这个需要了解特征的衍生逻辑，并与实际一般的情况相比较是否有异常。
特征的覆盖率，一般来说覆盖率要达到70%以上。
特征的稳定性，特征的覆盖率，分布，区分效果在时间序列上的表现比较稳定。
特征的及时性，最好是能代表用户最近的信用风险情况。


Q：为什么探索性数据分析（EDA）在机器学习中非常重要？

EDA不单是看看数据的分布，而是对数据整体有一个大概的了解。通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律。从中发现关键性的价值信息，这些信息对于后续建模及对模型的正确理解有很重要的意义。
通过EDA可以发现数据的异常，可以分析每个特征与目标变量之间的关系，特征与特征之间的关系，为特征构建和特征筛选提供有价值的信息。
EDA分析可以验证数据是不是你认为的那样，实际情况中由于数据和特征量比较大，往往忽视这些数据是如何生成的，数据突出的问题或模型的实施中的错误会被长时间忽视，这可能会导致基于错误信息做出决策。


Q：缺失值的处理方式有哪些？风控建模中该如何合理的处理缺失？

首先要了解缺失产生的原因，因数据获取导致的缺失建议用填充的方式(缺失率比较低的情况下），因用户本身没有这个属性导致的缺失建议把缺失当做一个类别。另外可以分析缺失是否有风险属性，有的话最好当做一个类别来处理。
风控模型对于缺失率的要求比较高，尤其是评分卡。个人认为，缺失率在30%以上的特征建议不要用，缺失率在10%以下的变量可用中位数或随机森林来填充，10%-30%的缺失率建议当做一个类别。对于xgboost和lightgbm这类可以自动处理缺失值的模型可以不做处理。


Q：如何发现数据中的异常值？对异常值是怎么处理的？

一种是基于统计的异常点检测算法例如极差，四分位数间距，均差，标准差等，这种方法适合于挖掘单变量的数值型数据。另一种主要通过距离方法来检测异常点，将数据集中与大多数点之间距离大于某个阈值的点视为异常点，检测的标准有欧式距离，绝对距离。
对于异常值先检查下是不是数据错误导致的，数据错误的异常作删除即可。如果无法判别异常的原因，要根据实际情况而定，像评分卡会做WOE转换，所以异常值的影响不大，可以不做处理。若异常值的数量较多，建议将异常值归为一类，数量较少作删除也可以。


Q：对于时间序列特征，连续特征，离散特征这三类是怎么做特征转换的？

时间序列特征：将时间变量的维度进行分离（年/月/日/时/分/秒），或者与位置变量进行结合衍生成新的特征。
连续型特征：标准化，归一化，区间缩放，离散化。在评分卡中主要用的是离散化，离散化常用的方法有卡房分箱，决策树分箱，等频和等深分箱。
离散型特征：如果类别数不是很多，适合做亚编码处理，对于无序离散变量用独热编码，有序离散变量用顺序编码。如果类别数较多，可用平均数编码的方法。


Q：如何处理样本不平衡的问题？

在风控建模中出现样本不平衡主要是坏样本的数量太少，碰到这个问题不要急着试各种抽样方法，先看一下坏用户的定义是否过于严格，过于严格会导致坏样本数量偏少，中间样本偏多。坏用户的定义一般基于滚动率分析的结果，不过实际业务场景复杂多样，还是得根据情况而定。
确定好坏用户定义是比较合理的之后，先尝试能不能扩大数据集，比如一开始取得是三个月的用户数据，试着将时间线延长来增加数据。因为机器学习是使用现在的数据在整个数据分布上进行估计，因此更多的数据往往能够得到更多的分布信息，以及更好的分布估计。
对数据集进行抽样，一种是进行欠采样，通过减少大类的数据样本来降低数据的不平衡，另一种是进行过采样，通过增加小类数据的样本来降低不平衡，实际工作中常用SMOTE方法来实现过采样。
尝试使用xgboost和lightgbm等对不平衡数据处理效果较好的模型。
尝试从新的角度来理解问题，可以把那些小类样本当做异常点，因此该分类问题转化为异常检测问题或变化趋势检测问题，这种方法笔者很少用到，就不详细说明了。


Q：特征衍生的方法有哪些？说说你平时工作中是怎么做特征衍生的？

常规的特征衍生方法：

基于对业务的深入理解，进行头脑风暴，构造特征。
特征交叉，例如对类别特征进行交叉相乘。
分解类别特征，例如对于有缺失的特征可以分解成是否有这个类别的二值化特征，或者将缺失作为一个类别，再进行亚编码等处理。
重构数值量（单位转换，整数小数拆分，构造阶段性特征）
特征的四则运算，例如取平均/最大/最小，或者特征之间的相乘相除。
平时工作特征衍生的做法：

因为风控模型通常需要好的解释能力，所以在特征衍生时也会考虑到衍生出来的特征是否与目标变量相关。例如拿到运营商的通话记录数据，可以衍生一个"在敏感时间段（深夜）的通话次数占比"，如果占比较高，用户的风险也较大。
平常会将大量的时间和精力花在底层数据的衍生上，这个不仅需要对业务的理解，也需要一定的想象力进行头脑风暴，即使衍生出来的特征90%都效果不佳，但只要剩下的10%是好的特征，那对于模型效果的提升是很显著的。
对于评分卡来说，特征需要好的解释能力，所以一些复杂的衍生方法，像特征交叉，log转换基本不会用到。但如果是xgboost等复杂模型，进行特征交叉等方法或许有比较好的效果。


Q：特征筛选的作用和目的？筛选的特征需要满足什么要求？

作用和目的：

简化模型，增加模型的可解释性， 降低模型过拟合的风险。
缩短模型的训练时间。
避免维度灾难。
筛选特征满足的要求：

具有良好的区分能力。
可解释性好，与目标变量的关系在业务上能解释的通。
在时间序列上有比较好的稳定性。
特征的用户覆盖率符合要求。


Q：特征筛选的方法有哪些？每种方法的优缺点？实际工作中用到了哪些方法？

Filter（过滤法）：按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。

相关系数，方差（适用于连续型变量），卡方检验（适用于类别型变量），信息熵，IV。实际工作中主要基于IV和相关性系数（皮尔逊系数）。
优点：算法的通用性强；省去了分类器的训练步骤，算法复杂性低，因而适用于大规模数据集；可以快速去除大量不相关的特征，作为特征的预筛选器非常合适。
缺点：由于算法的评价标准独立于特定的学习算法，所选的特征子集在分类准确率方面通常低于Wrapper方法。
Wrapper（封装法）：封装式特征选择是利用学习算法的性能评价特征子集的优劣。因此，对于一个待评价的特征子集，Wrapper方法需要训练一个分类器，根据分类器的性能对该特征子集进行评价。

方法有完全搜索（递归消除法），启发式搜索（前向/后向选择法，逐步选择法），随机搜索（训练不同的特征子集）。实际工作中主要用到启发式搜索，例如评分卡的逐步逻辑回归。
优点：相对于Filter方法，Wrapper方法找到的特征子集分类性能通常更好。
缺点：Wrapper方法选出的特征通用性不强，当改变学习算法时，需要针对该学习算法重新进行特征选择；由于每次对子集的评价都要进行分类器的训练和测试，所以算法计算复杂度很高，尤其对于大规模数据集来说，算法的执行时间很长。
Embedded（嵌入法）：先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

一种是基于惩罚项，例如岭回归，lasso回归，L1/L2正则化。另一种是基于树模型输出的特征重要性，在实际工作中较为常用，可选择的模型有随机森林，xgboost，lightgbm。
优点：效果最好速度最快，模式单调，快速并且效果明显。
缺点：如何参数设置， 需要对模型的算法原理有较好的理解。
