

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 16:32:52
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-09 18:01:09
 * @Description:
 * @TODO::
 * @Reference:
-->

关于dropout说法错误的是
 阻止了某些特征仅仅在其它特定特征下才有效果的情况
 预测的时候，和训练时一样，按照一定的概率将经网络单元从网络中丢弃 X
 dropout的训练时间比没有dropout的要长
 对参数w的训练进行球形限制(max-normalization)，对dropout的训练非常有用

 加了dropout训练时间会大大加长，加bn训练时间会缩短

 有一个游戏，玩法如下：一个方块一开始放置在原点，每次投掷一个均匀的六面骰子，掷出几点，将方块往前移动几点；问：当游戏一直进行下去，方块曾经落在2019的概率为
 1
 1/6
 1/3
 2/7

月神特别喜欢吃月饼，中秋节时快手发了10个月饼，已知月神一天至少吃一个月饼；请问，月神在3天内将10个月饼全部吃完的概率为：
 1/4
 23/256
 13/128
 3/32

采用插板法

第一天是很好算是C09，第二天吃完的次数是C19，第三天吃完的次数是C29。
现在再来算总次数，第一天是很好算是C09，第二天吃完的次数是C19，第三天吃完的次数是C29，第四天是C39，
依次类推，直到第十天是C99
因此3天内吃完月饼的概率就是（C09 +C19+C29）/（C09 +C19+C29 +C39+C49+C59+C69+C79+C89+C99）=46/29  =46/512

 关于神经网络中经典使用的优化器，以下说法正确的是
 Adam的收敛速度比RMSprop慢
 相比于SGD或RMSprop等优化器，Adam的收敛效果是最好的
 对于轻量级神经网络，使用Adam比使用RMSprop更合适
 相比于Adam或RMSprop等优化器，SGD的收敛效果是最好的

相比于Adam或RMSprop等优化器，SGD的收敛效果是最好的

Adam
相当于 RMSprop + Momentum

 以下说法错误的是
 使用ReLU做为激活函数，可有效地防止梯度爆炸
 使用Sigmoid做为激活函数，较容易出现梯度消失
 使用Batch Normalization层，可有效的防止梯度爆炸
 使用参数weight decay，在一程度上可防止模型过拟合

 更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合更好（这个法则也叫做奥卡姆剃刀），而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。

 weight decay/ l2 norm做的事说白了就是使求得解的weight变小。而为什么小的weight能够防止过拟合可以反过来看：数据集中“独特”的点往往需要较大的weight值来拟合，直观一点，就是weight越大model的曲线越“陡峭”，因而能够去拟合到那些位置较偏的点，这就会导致了过拟合。使用weight decay/ l2 norm就是抑制model的expressiveness从而防止了过拟合。

 前几个月放映的头号玩家简直火得不能再火了，作为一个探索终极AI的研究人员，月神自然去看了此神剧。
由于太过兴奋，晚上月神做了一个奇怪的梦，月神梦见自己掉入了一个被施放了魔法的深渊，月神想要爬上此深渊。

已知深渊有N层台阶构成（1 <= N <= 1000)，并且每次月神仅可往上爬2的整数次幂个台阶(1、2、4、....)，请你编程告诉月神，月神有多少种方法爬出深渊

输入描述:
输入共有M行，(1<=M<=1000)

第一行输入一个数M表示有多少组测试数据，

接着有M行，每一行都输入一个N表示深渊的台阶数

输出描述:
输出可能的爬出深渊的方式

分类任务中经常使用的损失函数是哪一个？
 Cross-Entropy Loss
 L1 Loss
 L2 Loss
 Triplet Loss

度学习中，不经常使用的初始化参数W（权重矩阵）的方法是哪种？
 高斯分布初始化
 常量初始化
 Xavier初始化
 MSRA初始化

关于CNN，以下说法错误的是
 CNN可用于解决图像的分类及回归问题
 CNN最初是由Hinton教授提出的
 CNN是一种判别模型
 第一个经典CNN模型是LeNet

https://www.nowcoder.com/questionTerminal/824af5cb05794606b56657bb3fa91f49?f=discussion

牛顿法描述错误的是

 牛顿法是一种常见的解决带约束最优化问题的方法
 牛顿法使用函数f(x)的泰勒级数的前面几项来寻找方程f(x) = 0的根
 牛顿法使用一个二次曲面去拟合当前所处位置的局部曲面
 牛顿法是一种迭代算法，求解过程中需要计算目标函数的Hessian矩阵的逆矩阵

 0.12/（0.12+0.17）≈ 0.41

 几何分布，几何分布的期望等于E(X)=1/p

 在其他条件不变的前提下，以下哪种做法容易引起机器学习中的过拟合问题（）
 增加训练集量
 增加神经网络隐藏层节点数
 增加更多特征
 在模型中引入正则项


C72=21种，相邻有7种，21-7=14

一方面使网络训练时间延长，另一方面，训练容易陷入局部极小点而得不到最优点，

 快手需要对用户年龄进行识别，识别系统中用户被划分为老人，中年，青年，小孩四类，下面哪种方法最适合此种应用需求：（）

 kmeans聚类
 层次聚类
 多分类算法
 关联挖掘

 二叉树T，已知其先根遍历是1 2 4 3 5 7 6（数字为结点的编号，以下同），中根遍历是2 4 1 5 7 3 6，则该二叉树的后根遍历是：

 4 2 5 7 6 3 1

 试卷： 快手2019年秋季校园招聘笔试试卷—算法A试卷

正确题数： 11/23

得分： 22.0

排名： 前32%
