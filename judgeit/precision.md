

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 12:58:54
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-26 19:53:07
 * @Description:
 * @TODO::math
 * @Reference:https://blog.csdn.net/weixin_43172660/article/details/82958534
 * [2]: https://juejin.im/entry/6844903606903111693
 * [3]: https://blog.csdn.net/Orange_Spotty_Cat/article/details/82016928
 * [4]: https://blog.csdn.net/GFDGFHSDS/article/details/104596414
 * [5]: https://zhuanlan.zhihu.com/p/70124635
 * [6]: https://www.jianshu.com/p/cb8f77bd6067#4.%20Lift%20%E5%92%8CGain%E5%9B%BE
 * [7]: https://www.jianshu.com/p/d3739a5499b5
 * [8]: [Lift , Gain](https://blog.csdn.net/zwqjoy/article/details/84859405#%E5%9B%9B%20Lift%20%2C%20Gain)
 * [9]: https://goodgoodstudy.blog.csdn.net/article/details/108697963
 * [10]: https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch18_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E3%80%81%E7%A6%BB%E7%BA%BF%E5%8F%8A%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E7%AC%AC%E5%8D%81%E5%85%AB%E7%AB%A0_%E5%90%8E%E7%AB%AF%E6%9E%B6%E6%9E%84%E9%80%89%E5%9E%8B%E3%80%81%E7%A6%BB%E7%BA%BF%E5%8F%8A%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97.md
-->

# 评价指标[1]

召回率（Recall） 精确率（Precision）准确率（Accuracy）ROC曲线和AUC值 是机器学习中常用的评价指标



假设某个班级有男生80人，女生20人，共100人。目标是找出所有的女生
现在有人挑选出了50个人，其中20个是女生，另外还错误的把30个男生也当作女生挑选了出来。

真正例(True Positive, TP)：被模型预测为正的正样本；
假正例(False Positive, FP)：被模型预测为正的负样本；
假负例(False Negative, FN)：被模型预测为负的正样本；
真负例(True Negative, TN)：被模型预测为负的负样本；

## confusion_matrix 混淆矩阵[5]

实际女             ; 实际男
TP = 20 (女判为女) ;FP =30 (男判为女)   判为女
FN = 0 （女判为男）;TN = 50（男判为男）  判为男

## 准确率（Accuracy）

所有预测结果的准确率
Accuracy=(TP+TN)/(TP+TN+FN+FP)

### 局限

当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确率。

三本，学生都不好。省力但不对！

如何避免由于样本比例不均衡导致准确率失效？
使用更为有效的平均准确率（每个类别下的样本准确率的算术平均值）。

## 召回率（Recall）

又叫查全率，它是针对原样本而言的
实际为正例中，预判为正例的比例

Recall = TP/ (TP+FN)

即为实际为女生的人中，预测为女生的人占比。 这里是20/20+0 =1
即认为预测结果很好 ，能将需要预测的女生都预测出来。



## 精确率（Precision）

又叫查准率，它是针对预测结果而言的

预判为正例， 实际为正例的比例

Precision = TP/ (TP+FP)

一般情况下，要分辨一个模型是好坏 ，需要结合召回率（Recall）和精确率（Precision）两个值，当两个值都高的时候，我们说这个模型很好，但实际情况都是一个高一个低。所以我们有另一个判别标准 ，F1-score 结合了Precision和Recall。

## F-Measure

在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：

$F_{\beta}=\frac{1+\beta^{2}}{\frac{1}{\text { precision }}+\frac{\beta^{2}}{\text { recall }}}=\frac{\left(1+\beta^{2}\right) * \text { precision } * \text { recall }}{\beta^{2} * \text { precision }+\text { recall }}$

其中 Beta 度量了查全率对查准率的相对重要性。Beta大于1时，召回率更重要，在0-1之间时，精确率更重要。常用的Beta值有 2 和 0.5。

### F1-Score

当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。

F1-score= 2PR/(P+R) = 2TP/(2TP + FP + FN)

当F1 -score的值越高，我们说这个模型效果越好。

### E值[10]

E值表示查准率P和查全率R的加权平均值，当其中一个为0时，E值为1，其计算公式：

E=1-\frac{1+b^{2}}{\frac{b^{2}}{P}+\frac{1}{R}}

b越大，表示查准率的权重越大。


### 平均正确率（Average Precision）

平均正确率表示不同查全率的点上的正确率的平均。

## P-R曲线

用P-R曲线，横轴是召回率，纵轴是精确率。对于一个排序模型来说，

其P-R曲线上的一个点代表着，在某一个阈值下，模型将大于该阈值的结果判定为正样本，小于该阈值的结果判定为负样本

不同点是ROC曲线使用了FPR，而PR曲线使用了Precision，因此PR曲线的两个指标都聚焦于正例。类别不平衡问题中由于主要关心正例，所以在此情况下PR曲线被广泛认为优于ROC曲线。

如果想要评估在相同的类别分布下正例的预测情况，则宜选PR曲线。
类别不平衡问题中，ROC曲线通常会给出一个乐观的效果估计，所以大部分时候还是PR曲线更好。[4]

## ROC曲线

### INTRO


接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力。

接着来说ROC曲线 ，我们可以根据两个指标形成坐标轴

是通过遍历所有阈值来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。

改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。

### TPR和FPR

纵坐标为TPR（True positive rate）灵敏度（sensitivity）
横坐标为FPR（False positive rate）特异度（specificity）

真正率 TPR（True positive rate）=TP/ (TP+FN) 也就是召回率
实际为正例中，预判为正例的比例 正样本正确判断

假正率 FPR（False positive rate）=FP/(FP+TN)
实际为负例中，预判为正例的比例 负样本误判
即为实际是男生，被判定为女生的占比，这里FPR = 3/8

### ROC

我们以FPR为横轴，TPR为纵轴，得到如下ROC空间：

而ROC曲线就是基于这样两个指标画出的曲线。
如图黑粗线就是一条ROC曲线，曲线越靠近左上角模型分类效果越好。

### 判断模型性能

负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。

ROC曲线由于兼顾正例与负例，所以适用于评估分类器的整体性能，相比而言PR曲线完全聚焦于正例。

进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。[4]




### 无视样本不平衡

TODO:WHY?

用动态图的形式再次展示一下它是如何工作的。我们发现：无论红蓝色样本比例如何改变，ROC曲线都没有影响。

## AUC值

AUC值为ROC曲线所覆盖（曲线下面）的区域面积，显然，AUC越大，分类器分类效果越好。

AUC = 1，是完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。

0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。

AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。

AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

## KS曲线

（Kolmogorov-Smirnov）缩写

KS曲线是用来衡量分类型模型准确度的工具。KS曲线与ROC曲线非常的类似。其指标的计算方法与混淆矩阵、ROC基本一致。它只是用另一种方式呈现分类模型的准确性。KS值是KS图中两条线之间最大的距离，其能反映出分类器的划分能力。

KS曲线是两条线，其横轴是阈值，纵轴是TPR与FPR。两条曲线之间之间相距最远的地方对应的阈值，就是最能划分模型的阈值。

KS值是MAX(TPR - FPR），即两曲线相距最远的距离。

KS值越大，说明模型能将两类样本区分开的能力越大。

ks值<0.2,一般认为模型没有区分能力。

ks值[0.2,0.3],模型具有一定区分能力，勉强可以接受

ks值[0.3,0.5],模型具有较强的区分能力。

ks值大于0.75，往往表示模型有异常。

计算KS的常见方法是这样的：[9]

step 1. 对变量进行分箱（binning），可以选择等频、等距，或者自定义距离。
step 2. 计算每个分箱区间的好账户数(goods)和坏账户数(bads)。
step 3. 计算每个分箱区间的累计好账户数占总好账户数比率(cum_good_rate)和累计坏账户数占总坏账户数比率(cum_bad_rate)。
step 4. 计算每个分箱区间累计坏账户占比与累计好账户占比差的绝对值，得到KS曲线。也就是： [公式]
step 5. 在这些绝对值中取最大值，得到此变量最终的KS值。

### 优劣

由于ks值能找出模型中差异最大的一个分段，因此适合用于cut_off，像评分卡这种就很适合用ks值来评估。但是ks值只能反映出哪个分段是区分最大的，而不能总体反映出所有分段的效果，因果AUC值更能胜任。

### 阈值（Threshold）

界定“发生”与“不发生”的临界值，就叫做阈值。[3]

这个机器警察只能按照模型预测的分数去抓，我们设定一个分数值，机器警察会把小于等于这个分数的人，都给抓起来。

![分数越低，人越坏]（img\zhua_score.jpg)

确定最好的“截断点”

![KS](img\KS.jpg)


## 混淆矩阵

混淆矩阵（Confusion Matrix）又被称为错误矩阵，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵i行j列的原始是原本是类别i却被分为类别j的样本个数，计算完之后还可以对之进行可视化：

## 多分类问题

对于多分类问题，或者在二分类问题中，我们有时候会有多组混淆矩阵，例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为宏平均（macro-average）和微平均（micro-average）。简单理解，宏平均就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出Fβ或F1，而微平均则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。其它分类指标同理，均可以通过宏平均/微平均计算得出。

$\operatorname{macro}-P=\frac{1}{n} \sum_{i=1}^{n} P_{i}$

$\begin{aligned} \operatorname{macro}-R &=\frac{1}{n} \sum_{i=1}^{n} R_{i} \\ \text { macro-F } 1 &=\frac{2 \times \text { macro- } P \times \text { macro- } R}{\text { macro- } P+\text { macro- } R} \end{aligned}$

需要注意的是，在多分类任务场景中，如果非要用一个综合考量的metric的话，宏平均会比微平均更好一些，因为宏平均受稀有类别影响更大。宏平均平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均平等考虑数据集中的每一个样本，所以它的值受到常见类别的影响比较大。

## 零一分类损失

简单暴力，分类正确为1，否则为0。这个。。。局限性不用说了吧。。。遇到不均衡问题[5]

## GINI系数[6]

AUC养了一个小弟，叫GINI系数[8]

GINI = 2*AUC -1

gini就是roc曲线和y=x的合并行成区域的面积再乘2，所以就很容易得到上面的公式了，三角形面积为0.5

·GINI系数:也是用于模型风险区分能力进行评估。
GINI统计值衡量坏账户数在好账户数上的的累积分布与随机分布曲线之间的面积，好账户与坏账户分布之间的差异越大，GINI指标越高，表明模型的风险区分能力越强。

GINI系数的计算步骤如下：

计算每个评分区间的好坏账户数。
计算每个评分区间的累计好账户数占总好账户数比率（累计good%）和累计坏账户数占总坏账户数比率(累计bad%)。
按照累计好账户占比和累计坏账户占比得出下图所示曲线ADC。
计算出图中阴影部分面积，阴影面积占直角三角形ABC面积的百分比，即为GINI系数

Gini$=\sum_{i} \frac{n_{i}}{N}\left(1-p_{i}^{2}-\left(1-p_{i}^{2}\right)^{2}\right)=2 \sum_{i} \frac{n_{i}}{N}\left(1-p_{i}\right) p_{i}$


，为样本分组后每组的坏样本率；Gini Scoreu越小区分度越强，最大为0.5，分组越细，分数越小；Gini Score对数据集的好坏比比较敏感；Gini Score不能反映分数在好坏人群上的有序性。[7]



## Lift , Gain[8]

夫妻帮里面是夫妻两个，一个Lift曲线（提升图），一个Gain曲线（增益图），两个人不分高低，共用一个横轴。

再次，阶级帮里面就比

前三个指标应用场景更多一些
Lift图衡量的是，与不利用模型相比，模型的预测能力“变好”了多少，lift(提升指数)越大，模型的运行效果越好。
Gain图是描述整体精准度的指标。

## 模型稳定度指标PSI[8]

群体稳定性指标PSI(Population Stability Index)是衡量模型的预测值与实际值偏差大小的指标。

PSI = sum（（实际占比-预期占比）* ln（实际占比/预期占比））

举例：

比如训练一个logistic回归模型，预测时候会有个概率输出p。
测试集上的输出设定为p1吧，将它从小到大排序后10等分，如0-0.1,0.1-0.2,......。
现在用这个模型去对新的样本进行预测，预测结果叫p2，按p1的区间也划分为10等分。
实际占比就是p2上在各区间的用户占比，预期占比就是p1上各区间的用户占比。
意义就是如果模型跟稳定，那么p1和p2上各区间的用户应该是相近的，占比不会变动很大，也就是预测出来的概率不会差距很大。

一般认为PSI小于0.1时候模型稳定性很高，0.1-0.25一般，大于0.25模型稳定性差，建议重做。

PS：除了按概率值大小等距十等分外，还可以对概率排序后按数量十等分，两种方法计算得到的psi可能有所区别但数值相差不大。


## 散度(Divergence) --好坏样本在统计学意义下的“距离”[7]

散度越大，区分能力越强
散度与好坏样本的比例无关，抽样不会有明显影响；
好坏样本分数比较接近正态分布时，最能真实反映真实的区分度
没有参照的阈值，可以用来比较不同模型在同一数据集上的表现，或者同一模型在不同时期样本上的表现。

