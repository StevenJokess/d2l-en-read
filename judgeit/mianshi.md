

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 21:02:03
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-09 21:07:18
 * @Description:
 * @TODO::
 * @Reference:https://www.nowcoder.com/ta/review-ml?query=&asc=true&order=&tagQuery=&page=20
-->
第一轮群面：

参考答案
大概四十分钟。五分钟阅读材料，每人一分钟自我介绍和阐述观点，注意一共只有1分钟。然后是20分钟的无领导小组讨论，得出一个方案，然后选一个人作总结。我个人的建议是，小组讨论中不需要太过抢风头，找准机会发表一次即可。

为什么选择实习，而不是在学校做研究？

参考答案
我实习的部门就是做研发的，还有跟着项目做研究可以把理论用到实际中，还有在公司实习的好处就是有数据，因为数据决定了机器/深度学习结果的上限，算法只是无限接近这个上限（校招王者）。

无领导群面。给30分钟自我介绍，基本是我叫啥，来自哪，我认为什么。。但是这时候应该有人控场才行，让大家都有发言的机会，然后再通过统计学的方式或者有没有异议(其实这也是一个投票，但是明确说了不能投票，所以算是个假投票)。最终全组人员都过了，就是疫苗问题，重点在于不要争吵，要稳定控场。

对公司有哪些了解

---

GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的。另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。生成对抗网络的一个简单解释如下：假设有两个模型，一个是生成模型（Generative Model，下文简写为G），一个是判别模型（Discriminative Model，下文简写为D），判别模型(D)的任务就是判断一个实例是真实的还是由模型生成的，生成模型(G)的任务是生成一个实例来骗过判别模型（D），两个模型互相对抗，发展下去就会达到一个平衡，生成模型生成的实例与真实的没有区别，判别模型无法区分自然的还是模型生成的。以赝品商人为例，赝品商人（生成模型）制作出假的毕加索画作来欺骗行家（判别模型D），赝品商人一直提升他的高仿水平来区分行家，行家也一直学习真的假的毕加索画作来提升自己的辨识能力，两个人一直博弈，最后赝品商人高仿的毕加索画作达到了以假乱真的水平，行家最后也很难区分正品和赝品了。

对于自己的论文，详细介绍一下，自己做了什么工作，有哪些提高和改进。并把自己的实验成果介绍一下。

因为项目要需要更高的效率，所以要用xgboost和lr。

可以看出，第一步内部有一个k层的循环，某种程度上可以认为是因为我们的训练首先要保证判别器足够好然后才能开始训练生成器，否则对应的生成器也没有什么作用，然后第二步求提督时只计算fake image那部分数据，这是因为real image不由生成器产生，因此对应的梯度为0。

---

Xgboost：

优缺点：1）在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。2）xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。3）特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。4）按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。5）xgboost 还考虑了当数据量比较大，内存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率。

适用场景：分类回归问题都可以。

Lr：

优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高

不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。
