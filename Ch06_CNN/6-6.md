

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-16 22:22:39
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-16 23:00:34
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-neural-networks/lenet.html
-->

# 卷积神经网络（LeNet）

现在，我们具备了组装全功能卷积神经网络所需的所有要素。在我们第一次接触图像数据时，我们将多层感知器（第4.2节）应用于Fashion-MNIST数据集中的衣物图片。为了使该数据适合多层感知器，我们首先将28×2828×28矩阵中的每个图像展平为固定长度784784维矢量，然后用完全连接的层对其进行处理。现在，我们在卷积层上有了一个控制柄，我们可以在图像中保留空间结构。作为用卷积层替换密集层的另一个好处，我们将享受更多的简约模型（所需的参数要少得多）。

在本节中，我们将介绍LeNet，这是最早发布的卷积神经网络之一，以其在计算机视觉任务上的性能吸引广泛的关注。该模型是由当时的AT＆T贝尔实验室的研究员Yann Lecun引入（并以其命名）的，目的是识别图像LeNet5中的手写数字。这项工作代表了对该技术进行十年研究的高潮。1989年，LeCun发表了第一个通过反向传播成功训练卷积神经网络的研究。

当时LeNet取得了与支持向量机（SVM）的性能相匹配的出色结果，这是监督学习中的主要方法。LeNet最终适用于识别数字，以便在ATM机中处理存款。直到今天，一些自动取款机仍然运行着Yann和他的同事Leon Bottou在20世纪90年代写的代码！

## LeNet

LeNet总体上由三个部分组成：（i）由两个卷积层组成的卷积编码器； （ii）由三个全连接层组成的密集块； 该体系结构总结在图6.6.1中。

图6.6.1 LeNet 5中的数据流。输入是一个手写数字，输出是概率除以10的可能结果。

每个卷积块中的基本单元是卷积层，S型激活函数和随后的平均池化操作。请注意，尽管ReLU和max-pooling的工作效果更好，但90年代尚未发现这些发现。每个卷积层使用5×55×5内核和S型激活函数。这些图层将空间排列的输入映射到多个2D特征图，通常会增加通道数。第一个卷积层有6个输出通道，而第二个卷积层有16个输出通道。每个2×2合并操作（步长2）通过空间下采样将尺寸降低44倍。卷积块发出一个输出，该输出的大小由（批量大小，通道，高度，宽度）给定。

为了将输出从卷积块传递到全链接块，我们必须展平迷你批处理中的每个示例。换句话说，我们将这个4D输入转换为完全连接的层所期望的2D输入：提醒一下，我们想要的2D表示使用第一个维度为minibatch中的示例建立索引，第二个表示为 每个示例的平面向量表示。LeNet的全连接层模块具有三个全连接层，分别具有120、84和10个输出。因为我们仍在执行分类，所以10维输出层对应于可能的输出类别的数量。

现在我们已经实现了模型，让我们进行一个实验，看看LeNet在Fashion-MNIST上的表现如何。

TODO:CODE

我们对原始模型略加自由，在最后一层中删除了高斯激活。除此之外，该网络与原始LeNet5架构匹配。

通过将单通道（黑白）28×2828×28图像通过网络传递并在每一层上打印输出形状，我们可以检查模型以确保其操作与图6.6.2所期望的一致 。

注意，整个卷积块中每一层的表示的高度和宽度都减小了（与前一层相比）。第一卷积层使用2个像素填充来补偿高度和宽度的减少，否则使用5×5内核会导致高度和宽度的减少。相反，第二卷积层放弃了填充，因此高度和宽度都减少了4个像素。当我们向上堆叠时，通道数从输入的1到第一卷积层之后的6和第二层之后的16逐层增加。但是，每个池化层的高度和宽度减半。最终，每个完全连接的层都降低了维数，最终发出了一个其维数与类数匹配的输出。

## 数据采集​​与训练

现在我们已经实现了模型，让我们进行一个实验，看看LeNet在Fashion-MNIST上的表现如何。

TODO:CODE

尽管卷积网络的参数很少，但它们的计算仍比类似的深层多层感知器昂贵，因为每个参数都参与了更多的乘法运算。如果您可以使用GPU，则可能是将其付诸实践以加快培训速度的好时机。

为了进行评估，我们需要对第3.6节中描述的valuate_accuracy函数进行一些修改。由于完整的数据集存在于CPU中，因此我们需要先将其复制到GPU，然后才能计算模型。

TODO:CODE

我们还需要更新我们的训练功能以处理GPU。与3.6节中定义的train_epoch_ch3不同，我们现在需要在进行前向和后向传递之前将每批数据移动到我们指定的上下文（希望是GPU）。

训练函数train_ch6也类似于3.6节中定义的train_ch3。由于我们将实现多层网络，因此我们将主要依靠高级API。以下训练函数假定使用高级API创建的模型作为输入，并进行了相应的优化。我们在设备指示的设备上初始化模型参数。与MLP一样，我们的损失函数是交叉熵，我们通过小批量随机梯度下降法将其最小化。由于每个纪元需要数十秒钟才能运行，因此我们可以更频繁地看到训练损失。

TODO:CODE

现在让我们训练这个模型。

TODO:CODE

## 小结

* ConvNet是使用卷积层的网络。
* 在ConvNet中，我们交织卷积，非线性和（通常）合并操作。
* 通常将这些卷积块布置成使得它们逐渐减小表示的空间分辨率，同时增加通道的数量。
* 在传统的ConvNet中，由卷积块编码的表示在发射输出之前由一个（或多个）密集层处理。
* LeNet可以说是这种网络的第一个成功部署。

## 练习

1. 用最大池替换平均池。怎么了？
1. 尝试基于LeNet构建更复杂的网络以提高其准确性。
    1. 调整卷积窗口大小。
    1. 调整输出通道数。
    1. 调整激活功能（ReLU？）。
    1. 调整卷积层数。
    1. 调整完全连接的层数。
    1. 调整学习率和其他培训详细信息（初始化，时期等）
1. 在原始MNIST数据集上尝试改进的网络。
1. 显示LeNet的第一层和第二层对不同输入的激活（例如，毛衣，大衣）。
