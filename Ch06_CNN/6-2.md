

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-16 20:21:02
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-06 23:51:56
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1185/chapter_convolutional-neural-networks/conv-layer.html
-->

# 图片的卷积

现在我们已经理解了理论上的卷积层是如何工作的，我们已经准备好看看它们在实践中是如何工作的。基于我们将卷积神经网络作为探索图像数据结构的有效体系结构的动机，我们坚持以图像为例。

## 互相关的操作符

回想一下，严格地说，卷积层是一个(轻微的)用词不当，因为它们表示的操作被更准确地描述为交叉相关性。在卷积层，一个输入阵列和一个相关核心阵列通过一个互相关操作产生一个输出阵列。现在让我们暂时忽略通道，看看它是如何处理二维数据和隐藏表示的。在图6.2.1中，输入是一个高度为3，宽度为3的二维数组。我们将数组的形状标记为3333或(33,33)。核心的高度和宽度都是22。请注意，在深度学习研究社区中，这个对象可以被称为卷积内核、过滤器或者简单地称为层的权重。内核窗口的形状是由内核的高度和宽度给出的(这里是2222)。

图6.2.1二维互相关运算。阴影部分是第一个输出元素以及计算时使用的输入和核心数组元素: 00 + 11 + 32 + 43 = 1900 + 11 + 32 + 43 = 19。

在二维互相关操作中，我们首先将卷积窗口定位在输入数组的左上角，然后将它滑过输入数组，从左到右和从上到下。当卷积窗口滑动到某个位置时，该窗口中包含的输入子阵列和核心阵列相乘(元素相乘) ，得到的数组相加产生一个标量值。这个结果给出了相应位置的输出数组的值。在这里，输出数组的高度为2，宽度为2,4个元素来自于二维互相关操作:

TODO:MATH

请注意，沿着每个轴，输出比输入稍小。由于内核的宽度和高度大于1，我们只能正确地计算内核完全在图像中的位置的互相关，输出大小由输入大小 h w w 减去卷积内核的大小 h w w w w via (h-h + 1)(w-w + 1)(w-w + 1)给出。这是因为我们需要足够的空间在图像上“移动”卷积内核(稍后我们将看到如何通过在图像边界周围填充零来保持图像的大小不变，这样就有足够的空间移动内核)。接下来，我们在 corr2d 函数中实现这个进程，它接受输入数组 x 和内核数组 k，并返回输出数组 y。

TODO:CODE

我们可以从上面的图中构造输入数组 x 和内核数组 k 来验证上面实现的二维互相关操作的输出。

TODO:CODE

## 卷积层

卷积层使输入和内核互相关，并添加标量偏差以产生输出。卷积层的两个参数是内核和标量偏差。当训练基于卷积层的模型时，我们通常会像对完全连接的层一样随机地初始化内核。

现在我们准备基于上面定义的corr2d函数实现二维卷积层。在__init__构造函数中，我们将权重和偏差声明为两个模型参数。正向计算函数调用corr2d函数并添加偏差。与h×w互相关一样，我们也将卷积层称为h×w卷积。

TODO:CODE

## 图片中的目标边缘检测

让我们花一点时间来解析卷积层的一个简单应用程序:通过查找像素变化的位置来检测图像中对象的边缘。首先，我们构建一个6×86×8像素的“图像”。中间四列为black(0)，其余为white(1)。

TODO:CODE

接下来，我们构造一个内核K，其高度为1，宽度为2。当我们对输入执行互相关运算时，如果水平相邻元素相同，则输出为0。否则，输出为非零。

我们准备使用参数X（我们的输入）和K（我们的内核）执行互相关运算。如您所见，我们检测到从白色到黑色的边缘为1，从黑色到白色的边缘为-1。所有其他输出取值为0。

TODO:CODE

现在我们可以将内核应用于转置后的图像。正如预期的那样，它消失了。核K仅检测垂直边缘。

TODO:CODE

## 学习内核

如果我们知道这正是我们所要寻找的，那么通过有限差分[1，-1]设计边缘检测器就很容易。但是，当我们查看较大的内核并考虑连续的卷积层时，可能无法精确指定每个过滤器应手动执行的操作。

现在让我们看看是否可以仅通过查看(input, output) 对来学习从X生成Y的内核。我们首先构造一个卷积层，并将其内核初始化为随机数组 接下来，在每次迭代中，我们将使用平方误差将Y与卷积层的输出进行比较。然后，我们可以计算梯度以更新权重。为了简单起见，在此卷积层中，我们将忽略偏差。

我们之前构造了Conv2D类。但是，由于我们使用了单元素分配，因此autograd很难找到梯度。相反，我们使用内置的`Conv2D`类。

TODO:CODE

请注意，经过10次迭代后，错误已降至很小的值。现在我们来看看我们学到的内核数组。

TODO:CODE

确实，学习到的内核数组与我们前面定义的内核数组K非常接近。

## 互相关和卷积

回顾上一节我们对互相关和卷积运算符之间的对应关系的观察。上图使这种对应关系显而易见。只需从左下角向右上角翻转内核即可。在这种情况下，总和的索引被还原，但是可以获得相同的结果。为了与深度学习文献中的标准术语保持一致，我们将继续将互相关运算称为卷积，即使严格地说，它稍有不同。

## 小结

* 二维卷积层的核心计算是二维互相关运算。以最简单的形式，它对二维输入数据和内核执行互相关运算，然后增加一个偏差。
* 我们可以设计一个内核来检测图像中的边缘。
* 我们可以从数据中了解内核的参数。

## 练习

1. 构造具有对角线边缘的图像X。
    1. 如果对它应用内核K，会发生什么？
    1. 如果换位X会发生什么？
    1. 如果转置K会发生什么？
1. 当您尝试自动找到我们创建的Conv2D类的渐变时，会看到哪种错误消息？
1. 如何通过更改输入数组和内核数组将互相关运算表示为矩阵乘法？
1. 手动设计一些内核。
1. 二阶导数的核形式是什么？
1. Laplace运算符的内核是什么？
1. 积分的内核是什么？
1. 获得d阶导数的内核的最小大小是多少？


<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-16 20:21:02
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-06 23:50:47
 * @Description:MT, some math
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-neural-networks/conv-layer.html
 * https://github.com/d2l-ai/d2l-en/blob/master/chapter_convolutional-neural-networks/conv-layer.md
-->

# 图片的卷积
:label:`sec_conv_layer`

现在我们已经理解了理论上的卷积层是如何工作的，我们已经准备好看看它们在实践中是如何工作的。基于我们将卷积神经网络作为探索图像数据结构的有效体系结构的动机，我们坚持以图像为例。

## 互相关的操作符

回想一下，严格地说，卷积层是一个(轻微的)用词不当，因为它们表示的操作被更准确地描述为交叉相关性。在卷积层，一个输入阵列和一个相关核心阵列通过一个互相关操作产生一个输出阵列。现在让我们暂时忽略通道，看看它是如何处理二维数据和隐藏表示的。在:numref:`fig_correlation`中，输入是一个高度为3，宽度为3的二维数组。我们将数组的形状标记为$3 \times 3$或($3$, $3$)。核心的高度和宽度都是2。请注意，在深度学习研究社区中，这个对象可以被称为卷积内核、过滤器或者简单地称为层的权重。内核窗口的形状是由内核的高度和宽度给出的(这里是 $2 \times 2$)。

[二维互相关运算。阴影部分是第一个输出元素以及计算时使用的输入和核心数组元素:  $0\times0+1\times1+3\times2+4\times3=19$。](../img/correlation.svg)
:label:`fig_correlation`

在二维互相关操作中，我们首先将卷积窗口定位在输入数组的左上角，然后将它滑过输入数组，从左到右和从上到下。当卷积窗口滑动到某个位置时，该窗口中包含的输入子阵列和核心阵列相乘(元素相乘) ，得到的数组相加产生一个标量值。这个结果给出了相应位置的输出数组的值。在这里，输出数组的高度为2，宽度为2,4个元素来自于二维互相关操作:

$$
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.
$$

请注意，沿着每个轴，输出比输入稍小。由于内核的宽度和高度大于1，我们只能正确地计算内核完全在图像中的位置的互相关，输出大小由输入大小 h w w 减去卷积内核的大小 h w w w w via (h-h + 1)(w-w + 1)(w-w + 1)给出。这是因为我们需要足够的空间在图像上“移动”卷积内核(稍后我们将看到如何通过在图像边界周围填充零来保持图像的大小不变，这样就有足够的空间移动内核)。接下来，我们在 corr2d 函数中实现这个进程，它接受输入数组 x 和内核数组 k，并返回输出数组 y。

TODO:CODE

之所以会出现这种情况，是因为我们需要足够的空间来在图像上“移动”卷积核。稍后我们将看到如何通过在图像边界周围填充0来保持大小不变，以便有足够的空间来移动内核。接下来，我们在corr2d函数中实现这个过程，它接受一个输入张量X和一个核张量K，并返回一个输出张量Y。

TODO:CODE

我们可以从上面的图中构造输入数组 x 和内核数组 k 来验证上面实现的二维互相关操作的输出。

TODO:CODE

## 卷积层

卷积层使输入和内核互相关，并添加标量偏差以产生输出。卷积层的两个参数是内核和标量偏差。当训练基于卷积层的模型时，我们通常会像对完全连接的层一样随机地初始化内核。

现在我们准备基于上面定义的corr2d函数实现二维卷积层。在__init__构造函数中，我们将权重和偏差声明为两个模型参数。正向计算函数调用corr2d函数并添加偏差。与h×w互相关一样，我们也将卷积层称为h×w卷积。

TODO:CODE

在h * wh * w卷积或h * wh * w卷积核中，卷积核的高度和宽度分别为hh和ww。我们也将具有h * wh * w卷积核的卷积层简称为h * wh * w卷积层。

## 图片中的目标边缘检测

让我们花一点时间来解析卷积层的一个简单应用程序:通过查找像素变化的位置来检测图像中对象的边缘。首先，我们构建一个6×8像素的“图像”。中间四列为black(0)，其余为white(1)。

TODO:CODE

接下来，我们构造一个内核K，其高度为1，宽度为2。当我们对输入执行互相关运算时，如果水平相邻元素相同，则输出为0。否则，输出为非零。

我们准备使用参数X（我们的输入）和K（我们的内核）执行互相关运算。如您所见，我们检测到从白色到黑色的边缘为1，从黑色到白色的边缘为-1。所有其他输出取值为0。

TODO:CODE

现在我们可以将内核应用于转置后的图像。正如预期的那样，它消失了。核K仅检测垂直边缘。

TODO:CODE

## 学习内核

如果我们知道这正是我们所要寻找的，那么通过有限差分[1，-1]设计边缘检测器就很容易。但是，当我们查看较大的内核并考虑连续的卷积层时，可能无法精确指定每个过滤器应手动执行的操作。

现在让我们看看是否可以仅通过查看(input, output) 对来学习从X生成Y的内核。我们首先构造一个卷积层，并将其内核初始化为随机数组 接下来，在每次迭代中，我们将使用平方误差将Y与卷积层的输出进行比较。然后，我们可以计算梯度以更新权重。为了简单起见，在此卷积层中，我们将忽略偏差。

TODO:CODE

我们之前构造了Conv2D类。但是，由于我们使用了单元素分配，因此autograd很难找到梯度。相反，我们使用内置的`Conv2D`类。

TODO:CODE

请注意，经过10次迭代后，错误已降至很小的值。现在我们来看看我们学到的内核数组。

TODO:CODE

确实，学习到的内核数组与我们前面定义的内核数组K非常接近。

## 互相关和卷积

回顾上一节我们对互相关和卷积运算符之间的对应关系的观察。上图使这种对应关系显而易见。只需从左下角向右上角翻转内核即可。在这种情况下，总和的索引被还原，但是可以获得相同的结果。为了与深度学习文献中的标准术语保持一致，我们将继续将互相关运算称为卷积，即使严格地说，它稍有不同。

值得注意的是，在深度学习中，核是从数据中学习的，所以无论卷积层执行严格的卷积操作还是互相关操作，卷积层的输出都不会受到影响。

为了说明这一点，假设卷积层进行互相关并学习图6.2.1中的核函数，这里用矩阵KK表示。假设其他条件不变，当这一层进行严格卷积时，在水平和垂直翻转K ' K '后，学习核K ' K '与KK相同。也就是说，当卷积层对图6.2.1中的输入与K’K’进行严格卷积时，会得到与图6.2.1中相同的输出(输入与KK的互相关)。

为了与深度学习文献中的标准术语保持一致，我们将继续将交叉相关操作称为卷积，尽管严格地说，它略有不同。此外，我们使用元素这个术语来表示表示层表示或卷积核的任何张量的一个项(或分量)。

## 特征图和接受野

如6.1.4.1节所述，图6.2.1中的卷积层输出有时被称为feature map，因为它可以被看作是学习到后续层的空间维度(如宽度和高度)上的表示(feature)。在cnn中，对于某一层的任意一个元素xx，其接受场是指在正向传播过程中可能影响xx计算的所有元素(来自前几层)。请注意，接受野可能比输入的实际大小大。

让我们继续用图6.2.1来解释感受野。给定2×22×2卷积核，阴影输出元素的接受场(值为1919)为输入阴影部分的四个元素。现在我们将2×22×2的输出表示为YY，考虑一个更深层的CNN，再增加一个2×22×2的卷积层，以YY为输入，输出单个元素zz。在这种情况下，YY上的zz的接受野包含了所有的四个元素，而输入野包含了所有的九个输入元素。因此，当feature map中的任何元素需要更大的接受域来检测更广区域的输入特征时，我们可以构建更深层次的网络。

## 小结

* 二维卷积层的核心计算是二维互相关运算。以最简单的形式，它对二维输入数据和内核执行互相关运算，然后增加一个偏差。
* 我们可以设计一个内核来检测图像中的边缘。
* 我们可以从数据中了解内核的参数。
* 通过从数据中学习内核，卷积层的输出不受影响，无论这些层执行的操作(严格的卷积或互相关)。
* 当特征图中的任何元素需要更大的接受域来检测更广泛的输入特征时，可以考虑更深层次的网络。

## 练习

1. 构造具有对角线边缘的图像X。
    1. 如果对它应用内核K，会发生什么？
    1. 如果换位X会发生什么？
    1. 如果转置K会发生什么？
1. 当您尝试自动找到我们创建的Conv2D类的渐变时，会看到哪种错误消息？
1. 如何通过更改输入数组和内核数组将互相关运算表示为矩阵乘法？
1. 手动设计一些内核。
   1. 二阶导数的核形式是什么？
   2. Laplace运算符的内核是什么？
   3. 积分的内核是什么？
   4. 获得d阶导数的内核的最小大小是多少？
