

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-08 12:55:51
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-16 20:15:24
 * @Description:translate
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1185/chapter_convolutional-neural-networks/why-conv.html
-->


# 6.1. 从密集层到卷积层

到目前为止，当我们处理表格数据时，我们所讨论的模型仍然是直（到今天）合适的选择。所谓表格，是指数据由与示例对应的行和与特征对应的列组成。使用表格数据，我们可能会预期我们寻找的模式可能涉及到特征之间的交互，但是我们不会假设任何关于特征如何交互的先验结构。

有时候，我们确实缺乏知识来指导工匠建筑的建造。在这些情况下，多层感知器可能是我们能做的最好的。然而，对于高维感知数据，这些*无结构*网络可能会变得笨拙。

例如，让我们回到区分猫和狗的流行例子。假设我们在数据收集方面做了彻底的工作，收集了一个带有注解的100万像素的照片数据集。这意味着网络的每个输入都有100万个维度。即使是**暴力缩减**到1000隐藏维度，也需要一个由10^9个参数组成的稠密（完全连接）层。除非我们有大量的gpu、分布式优化的天赋和非凡的耐心，学习这个网络的参数可能是不可行的。

细心的读者可能会反对这个论点，因为100万像素的分辨率可能没有必要。然而，尽管我们可能能够获得100,000像素，但我们1000的隐藏层严重低估了学习图像良好表示所需的隐藏节点的数量，因此一个实际的系统仍然需要数十亿个参数。此外，通过拟合这么多参数来学习分类器可能需要收集一个庞大的数据集。然而今天，人类和电脑都能很好地区分猫和狗，这似乎有些矛盾。

## 6.1.1. 不变性

假设您想要检测图像中的一个对象。我们用来识别物体的任何方法都不应该过分关注物体在图像中的精确位置，这似乎是合理的。理想情况下，我们的系统应该利用这些知识。猪通常不会飞，飞机通常不会游泳。尽管如此，我们仍然应该认识到猪是一个出现在图像的顶部。我们可以从孩子们的游戏‘ Where’s Waldo’(如图6.1.1所示)中获得一些灵感。这个游戏包含了大量充满活力的混乱场面。沃尔多出现在每个地方，通常潜伏在一些不太可能的地方。读者的目标是找到他。尽管他的装备很有特色，但是由于有太多的干扰，这可能会非常困难。然而，Waldo 的外观并不取决于 Waldo 所在的位置。我们可以使用 Waldo 检测器扫描图像，该检测器可以为每个补丁分配一个得分，表明补丁包含 Waldo 的可能性。CNNs 将这种空间不变性的概念系统化，利用它学习几个参数的有用表示。

图6.1.1图片来自 Walker Books

现在，我们可以通过列举一些需求来指导我们设计适合计算机视觉的神经网络结构，从而使这些直觉更加具体:在最早的层中，我们的网络应该对同一个补丁做出类似的响应，而不管它出现在图像的哪个位置(平移不变量)。最早的网络层应该关注局部区域，而不考虑遥远区域(locality)的图像内容。最终，这些局部表示可以聚合在整个图像级别进行预测。

我们现在可以通过列举一些需求来指导我们设计一个适合计算机视觉的神经网络结构，从而使这些直觉更具体:

1. 在最早的图层中，我们的网络应该对同一个补丁做出类似的响应，不管它在图像中出现在哪里(平移不变性)。
2. 网络的最初层应该聚焦于局部区域，而不考虑遥远区域(局部)的图像内容。最终，这些局部表示可以聚合在一起，在整个图像层面上做出预测。

让我们来看看这是如何转化成数学的。

## 6.1.2. 约束MLP

首先，我们可以考虑一个 MLP，其中 h×w 图像作为输入(在数学中表示为矩阵，在代码中表示为2D 数组) ，隐藏表示类似地组织为 h×w 矩阵/2D 数组。让我们沉浸其中，我们现在不仅认为输入，而且认为隐藏的表征具有空间结构。设 x [ i，j ]和 h [ i，j ]分别表示输入图像中的像素位置(i，j)。因此，为了让每个 h×w 隐藏节点接收来自每个 h×w 输入的输入，我们将从使用权重矩阵(正如我们在前面的 MLPs 中所做的)切换到将我们的参数表示为四维权重张量。

我们可以正式表示这一密集层如下:

TODO:MATH

从 w 到 v 的转换完全是装饰性的(就目前而言) ，因为两个张量的系数之间有一个双射。我们简单地重新索引下标(k，l)使得 k = i + a和 l = j + b。换句话说，我们设定 v [ i，j，a，b ] = w [ i，j，i + a，j + b ]。指数 a，b a，b 超过正偏移和负偏移，覆盖整个图像。对于隐藏层 h [ i，j ]中任意给定的位置(i，j)，我们通过对 x 中的像素进行求和，以(i，j)为中心，用 v [ i，j，a，b ]加权来计算它的值。

现在让我们引用上面建立的第一个原则:平移不变性。这意味着输入xx的位移会导致激活hh的位移。这只有在V和u实际上不依赖于(i,j)时才有可能。V[i,j,a,b]=V[a,b]， u是常数。因此，我们可以简化h的定义。

TODO:MATH

对于二维数组，ff的下标为(i,j)(i,j)， gg的下标为(i−a,j−b)(i−a,j−b)。这看起来类似于上面的定义，但有一个主要的区别。我们不是使用(i+a,j+b)(i+a,j+b)，而是使用差值。但是,请注意,这种区别主要是化妆品,因为我们总是可以匹配的符号用V ~ [a, b] = [−a,−b]获得h = x⊛V ~ 。我们最初的定义更恰当地描述了相互关系。我们将在下一节中回到这个问题。

这是一个卷积！我们有效地加权像素(i + a，j + b)在(i，j)附近的系数 v [ a，b ]以获得h [ i，j ]。注意 v [ a，b ]需要的系数比v [ i，j，a，b ]少得多。对于100万像素的图像，它的系数最多不超过100万。这将减少100万个参数，因为它不再依赖于图像中的位置。我们已经取得了重大进展！

现在让我们调用第二个原则-局部性。如上文所述，我们认为不必为了收集相关信息来评估h [i，j]。这意味着在| a |，| b |>Δ的某个范围之外，我们应将V [a，b] = 0设置。等效地，我们可以将h [i，j]重写为

TODO:MATH

简而言之，这是一个卷积层。当本地区域（也称为接收场）较小时，与完全连接的网络相比，差异可能很大。以前，我们可能需要数十亿个参数来表示图像处理网络中的单个图层，但现在通常只需要几百个参数，而无需更改输入或隐藏表示的维数。大幅减少参数所付出的代价是，我们的功能现在平移不变，并且在确定每个隐藏激活的值时，我们的图层只能合并本地信息。所有的学习都取决于施加归纳偏见。当这种偏见与现实相符时，我们将获得样本有效的模型，这些模型可以很好地推广到看不见的数据。但是，当然，如果这些偏差与现实不符，例如，如果图像证明不是平移不变的话，我们的模型甚至可能难以适应我们的训练数据。

## 卷积

在进一步讨论之前，我们应该简要回顾一下为什么上述操作被称为*卷积*。在数学中，两个函数f,g:Rd→Rf,g:Rd→R之间的卷积被定义为

(f⊛g) (x) =∫Rdf (z) g (x−z) dz。

即，当一个函数“翻转”并平移x时，我们测量f与g之间的重叠。当我们有离散的对象时，积分就变成了和。例如，对于定义在L2上的向量，即。索引运行在Z上的可平方和无限维向量的集合，我们得到以下定义。

(f⊛g) (i) =∑af (a) g(我−)。

对于二维数组，我们有一个相应的总和，其中f的索引为(i，j)，g的索引为(i - a，j - b)。这看起来与上面的定义相似，但有一个主要区别。而不是使用(i + a，j + b)，而是使用差异。但是请注意，由于我们可以始终使用V〜[a，b] = V [-a，-b] 以获得h ＝x⊛V〜。我们最初的定义更恰当地描述了互相关。我们将在下一节中回到这一点。

## 6.1.4. 重新审视Waldo

回到我们的Waldo检测器，让我们看看这是什么样子的。卷积层选择给定大小的窗口，并根据掩模V来衡量强度，如图6.1.2所示。我们可以学习一种模型，以便在“waldoness”最高的地方，我们应该在隐藏层激活中找到一个峰值。

Fig. 6.1.2 Find Waldo.

这种方法只有一个问题。到目前为止，我们幸运地忽略了图像由3个通道组成:红、绿、蓝。实际上，图像不是二维物体，而是3阶张量，其特征是高度、宽度和通道，例如形状为1024 1024 31024 1024 3像素。这些轴的前两个涉及空间关系，而第3rd3可以被视为为*每个像素位置*分配了一个多维表示。

因此，我们将x索引为x[i,j,k]。卷积掩码必须相应地进行调整。为了代替V[a,b]，我们现在有了V[a,b,c]。

而且，就像我们的输入是由一个3rd3阶张量组成的一样，用类似的方式来表示我们隐藏的表示，也不失为一个好主意。换句话说，与每个空间位置对应的单个激活不同，我们需要与每个空间位置对应的隐藏激活的整个向量。我们可以把隐藏的表示法看作是由若干个相互堆叠的2D网格组成的。在输入中，这些有时被称为通道。它们有时也被称为特征图，因为每一种都向后续层提供了一组空间化的已学习的特征。直观地，你可以想象在较低的层，一些通道可以专门用来识别边缘，其他的可以识别纹理，等等。为了在输入和隐藏激活中支持多个通道，我们可以在VV中添加第四个坐标V :  V[a,b,c,d]。把所有的东西放在一起，我们有:

TODO:MATH

这就是卷积神经网络层的定义。我们仍有许多运算需要处理。例如，我们需要弄清楚如何将所有激活组合为单个输出（例如，在映像中的*任何位置*是否有Waldo）。我们还需要决定如何高效地计算，如何组合多层，适当的激活函数，以及如何做出合理的设计选择，以产生实际有效的网络。我们将在本章的其余部分讨论这些问题。

## 6.1.5. 总结

* 图像中的平移不变性意味着图像中的所有小块都将以相同的方式处理。
* 局部性是指只使用像素的一个小邻域来计算相应的隐藏激活。
* 输入和输出通道允许我们的模型在每个空间位置捕获图像的多个方面。

## 6.1.6. 练习

1. 假设卷积面具的大小是Δ= 0。说明在这种情况下，卷积掩码独立地为每组通道实现了一个MLP。
2. 为什么平移不变性不是一个好主意呢？什么时候允许猪飞是没有意义的呢？
3. 在决定如何处理与图像边界像素位置相对应的激活时，我们必须处理哪些问题？
4. 为音频描述一个类似的卷积层。
5. 您认为卷积层也适用于文本数据吗？为什么或为什么不？
6. 证明f⊛g=g⊛f。
