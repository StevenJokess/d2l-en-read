

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-16 21:36:11
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-16 21:53:12
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-neural-networks/channels.html
 * https://zh.d2l.ai/chapter_convolutional-neural-networks/channels.html
-->

# 多输入通道和多输出通道

虽然我们已经描述了组成每个图像的多个通道(例如，彩色图像有标准的 RGB 通道来表示红色、绿色和蓝色的数量) ，但到目前为止，我们通过只使用单个输入和单个输出通道简化了所有的数值示例。这使我们可以将输入、卷积内核和输出视为二维数组。

当我们在混合中加入通道时，我们的输入和隐藏表示都变成了三维数组。例如，每个 RGB 输入图像的形状为3hw3hw。我们将这个尺寸为3的轴称为通道尺寸。在本节中，我们将更深入地研究具有多个输入和多个输出通道的卷积核。

## 多输入通道

当输入数据包含多个通道时，我们需要构造一个具有与输入数据相同数量输入通道的卷积核，以便它能够对输入数据执行互相关操作。假设输入数据的通道数为 ci，则卷积核的输入通道数也需为 ci。如果我们的卷积核的窗口形状是 k h k w kh kw，那么当 c i = 1 ci = 1时，我们可以认为我们的卷积核只是一个形状为 k h k w kh kw 的二维数组。

然而，当 c i > 1 ci > 1时，我们需要一个为每个输入通道包含形状为 k h k w kh kw 的数组的内核。将这些 c i ci 阵列连接在一起可以产生一个形状为 c i k k w ci kh kw 的卷积核。由于输入和卷积核都有 c i ci 通道，我们可以对输入的二维阵列和卷积核的二维阵列执行互相关操作，将 c i ci 结果加在一起(在通道上求和)得到一个二维阵列。这是多通道输入数据和多通道卷积核之间的二维互相关的结果。

在图6.4.1中，我们演示了一个具有两个输入通道的二维互相关的例子。阴影部分是第一个输出元素，也是计算中使用的输入和核心数组元素: (11 + 22 + 43 + 54) + (00 + 11 + 32 + 43) = 56(11 + 22 + 43 + 54) + (00 + 11 + 32 + 43) = 56。

图6.4.1有2个输入通道的互相关计算。阴影部分是第一个输出元素，也是计算中使用的输入和核心数组元素: (11 + 22 + 43 + 54) + (00 + 11 + 32 + 43) = 56(11 + 22 + 43 + 54) + (00 + 11 + 32 + 43) = 56。

为了确保我们真正了解这里发生了什么，我们可以自己实现多个输入通道的互相关操作。请注意，我们所做的只是对每个频道执行一个互相关操作，然后将结果加起来。

TODO:CODE

我们可以构造输入数组 x 和核心数组 k，它们对应于上面图表中的值，以验证互相关/值操作的输出。

TODO:CODE

## 多输出通道

不管输入通道的数量多少，到目前为止我们总是以一个输出通道结束。然而，正如我们前面所讨论的那样，在每个层上有多个通道是必不可少的。在最流行的神经网络体系结构中，我们实际上增加了信道维数，因为我们在神经网络中的位置越来越高，通常降低采样来平衡空间分辨率以获得更大的信道深度。直观地说，您可以将每个通道看作是对一些不同的特性集的响应。现实比对这种直觉的最天真的解释要复杂一些，因为表象不是独立学习的，而是为了共同有用而优化的。因此，这可能不是一个单一的通道学习一个边缘检测器，而是一些方向在通道空间对应的检测边缘。

用 c i ci 和 c o 分别表示输入通道和输出通道数，k h kh 和 k w kw 表示核的高度和宽度。为了获得具有多个通道的输出，我们可以为每个输出通道创建一个形状为 c i k k w ci kh kw 的核心阵列。我们将它们连接在输出通道维数上，使卷积核的形状是 c o c i k k w c c c c c h k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k k。在互相关/值操作中，每个输出通道的结果是从与输出通道对应的卷积核计算出来的，并从输入阵列中的所有通道获取输入。

我们实现了互相关函数，以计算多个通道的输出，如下所示。

TODO:CODE

我们通过将核数组K、K+1 (K中每个元素加1)和K+2连接起来，构建一个具有3个输出通道的卷积核。

TODO:CODE

下面，我们用内核数组k对输入数组X进行交叉相关操作。现在输出包含3个通道。第一个通道的结果与前一个输入数组X和多输入通道、单输出通道内核的结果一致。

TODO:CODE


首先，似乎没有1×1卷积，即kh = kw = 1。 毕竟，卷积使相邻像素相关。 1×1卷积显然不是。 尽管如此，它们还是流行的操作，有时会包含在复杂的深度网络的设计中。 让我们详细了解它的实际作用。

因为使用了最小窗口，所以1×1卷积失去了较大的卷积层识别由相邻元素在高度和宽度维度上的相互作用组成的模式的能力。 1×1卷积的唯一计算发生在通道维度上。

图6.4.2显示了使用具有3个输入通道和2个输出通道的1×11×1卷积核进行互相关计算。 请注意，输入和输出具有相同的高度和宽度。 输出中的每个元素都来自输入图像中相同位置的元素的线性组合。 您可以认为1×11×1卷积层构成了应用于每个像素位置的完全连接层，以将cici对应的输入值转换为coco输出值。 由于这仍然是卷积层，因此权重在像素位置上是紧密相关的。 因此，1×11×1卷积层需要co×cico×ci权重（加上偏置项）。

图6.4.2互相关计算采用1×1卷积核，3个输入通道，2个输出通道。输入和输出具有相同的高度和宽度

TODO:CODE

让我们检查一下这在实践中是否有效:我们使用全连接层实现1×1卷积。唯一需要做的是，我们需要在矩阵乘法之前和之后对数据形状进行一些调整。

TODO:CODE

在进行1×1卷积时，上述函数等价于之前实现的互相关函数`corr2d_multi_in_out`。让我们用一些参考数据来验证一下。

TODO:CODE

## 小结

* 多通道可用于扩展卷积层的模型参数。
* 1×1卷积层相当于全连接层，按像素计算。
* 1×1卷积层通常用于调整网络层之间的信道数量和控制模型复杂度。

## 练习

1. 假设我们有两个大小分别为k1和k2的卷积核（之间没有非线性）。
   1. 证明运算结果可以用一次卷积表示。
   2. 等效单卷积的维数是多少？
   3. 反过来是真的吗？
2. 假设输入形状为ci×h×w且卷积核的形状为co×ci×kh×kw，填充为（ph，pw）（ph，pw），且步幅为 的（sh，sw
   1. 正向计算的计算成本（乘法和加法）是多少？
   2. 内存占用量是多少？
   3. 向后计算的内存占用量是多少？
   4. 向后计算的计算成本是多少？
3. 如果将输入通道ci的数量和输出通道co的数量加倍，计算数量会增加多少个因素？ 如果将填充加倍，会发什么？
4. 如果卷积核的高度和宽度为kh = kw = 1，那么正向计算的复杂度是多少？
5. 本节最后一个示例中的变量Y1和Y2是否完全相同？ 为什么？
6. 当卷积窗口不是1×1时，如何使用矩阵乘法实现卷积？
