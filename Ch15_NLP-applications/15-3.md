

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-31 19:32:18
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-07 16:02:33
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-applications/sentiment-analysis-cnn.html
-->

# 情绪分析: 使用卷积神经网络
:label:`sec_sentiment_cnn`

在:numref:`chap_cnn`，我们探讨了如何处理二维图像数据与二维卷积神经网络。在以往的语言模型和文本分类任务中，我们将文本数据视为只有一个维度的时间序列，自然而然地，我们使用了递归神经网络来处理这些数据。事实上，我们也可以把文本看作一维图像，因此我们可以使用一维卷积神经网络来捕捉相邻词之间的联系。如:numref:`fig_nlp-map-sa-cnn`所述，本节描述了将卷积神经网络应用于情绪分析的开创性方法: :cite:`Kim.2014`。

[本部分将预训练的 GloVe 提供给一个基于 CNN 的体系结构，用于情感分析。](../img/nlp-map-sa-cnn.svg)
:label:`fig_nlp-map-sa-cnn`

首先，导入实验所需的软件包和模块。

TODO:CODE

## 一维卷积层

在介绍该模型之前，让我们先解释一维卷积层是如何工作的。就像二维卷积层一样，一维卷积层使用一维互相关操作。在一维互相关操作中，卷积窗口从输入数组的最左侧开始，从左到右依次滑动到输入数组上。当卷积窗口滑动到一定位置时，将窗口和核心阵列中的输入子阵列乘以元素求和，得到输出阵列中对应位置的元素。如图15.3.2所示，输入是一个宽度为7的一维数组，核心数组的宽度为2。我们可以看到，输出宽度为$7-2+1=6$，第一个元素是通过在最左边的输入子阵上对宽度为2的元素进行乘法得到的，然后对结果进行求和。

[一维互相关运算。阴影部分是第一个输出元素以及用于计算的输入和核心数组元素: $0\times1+1\times2=2$。](../img/conv1d.svg)
:label:`fig_conv1d`

接下来，我们在 corr1d 函数中实现一维的互相关。它接受输入数组 x 和内核数组 k，并输出数组 y。

TODO:CODE

现在，我们将再现:numref:`fig_conv1d`中一维互相关运算的结果。

TODO:CODE

多个输入通道的一维互相关操作也类似于多个输入通道的二维互相关操作。在每个通道上，它对内核及其相应的输入执行一维互相关操作，并添加通道的结果以获得输出。:numref:`fig_conv1d_channel`显示了三个输入通道的一维互相关操作。

[三个输入通道的一维互相关运算。阴影部分是第一个输出元素，也是用于计算的输入元素和核阵列元素:0×1+1×2+1×3+2×(−1)+3×(−3)=2。](../img/conv1d-channel.svg)
:label:`fig_conv1d_channel`

现在，我们将多输入通道的一维互相关运算结果再现在:numref:`fig_conv1d_channel`中。

TODO:CODE

![单输入通道的二维互相关操作。突出部分为第一个输出元件和用于计算的输入及核阵列元件。 ](../img/conv1d-2d.svg)
:label:`fig_conv1d_2d`

图15.3.2和图15.3.3的输出都只有一个通道。我们在6.4节中讨论了如何在一个二维卷积层中指定多个输出通道。同样，我们也可以在一维卷积层中指定多个输出通道，以扩展卷积层中的模型参数。

## Max-Over-Time池化层

类似地，我们有一个一维池化层。TextCNN中使用的最大超时池化层实际上对应于一维全局最大池化层。假设输入包含多个通道，每个通道由不同时间步长的值组成，每个通道的输出将是通道中所有时间步长的最大值。因此，最大超时池化层的输入可以在每个通道上有不同的时间步长。

为了提高计算性能，我们经常将不同长度的计时示例合并到一个小批处理中，并通过在较短示例的末尾附加特殊字符(如0)来使批处理中的每个计时示例的长度保持一致。当然，增加的特殊字符没有内在意义。因为max-over-time池化层的主要目的是捕获最重要的计时特性，所以它通常允许模型不受手动添加的字符的影响。

## TextCNN模型

TextCNN主要使用一维卷积层和最大超时池层。假设输入的文本序列由$n$单词组成，每个单词用$d$维的单词向量表示。那么输入示例的宽度为$n$，高度为1，有$d$个输入通道。textCNN的计算主要分为以下几个步骤:

1. 定义多个一维卷积核，并使用它们对输入进行卷积计算。不同宽度的卷积核可以捕捉到不同数量相邻词的相关性。
1. 在所有输出通道上执行max-over-time池化，然后将这些通道的池化输出值连接到一个向量中。
1. 连接的向量通过完全连接的层转换为每个类别的输出。在这个步骤中可以使用一个dropout层来处理过拟合。

![TextCNN设计。](../img/textcnn.svg)
:label:`fig_conv1d_textcnn`

:numref:`fig_conv1d_textcnn`给出了一个示例来说明textCNN。这里的输入是一个包含11个单词的句子，每个单词由一个6维的单词向量表示。因此，输入序列的宽度分别为11和6个输入通道。我们假设有两个宽度为2和4的一维卷积核，输出通道分别为4和5个。因此，经过一维卷积计算，四个输出通道的宽度为11−2+1=10，而其他五个通道的宽度为11−4+1=8。即使每个通道的宽度不同，我们仍然可以为每个通道执行max-overtime池化，并将9个通道的池化输出连接到一个9维向量中。最后，我们使用一个完全连接的层将9维向量转换为2维输出:积极情绪和消极情绪预测。

接下来，我们将实现一个textCNN模型。与上一节相比，除了用一维卷积层替换递归神经网络之外，这里我们使用了两个嵌入层，一个权值固定，另一个参与训练。

TODO:CODE

创建一个TextCNN实例。它有3个卷积层，内核宽度为3,4,5，都有100个输出通道。

TODO:CODE

## 加载预先训练好的单词向量

与上一节一样，加载预先训练好的100维GloVe词向量，初始化嵌入层的`embedding`和`constant_embedding`。在这里，前者参加训练，后者有固定的权重。

TODO:CODE

## 训练和评估模型

现在我们可以训练模型了。

TODO:CODE

下面，我们使用训练好的模型对两个简单句子的情感进行分类。

TODO:CODE

## 小结

* 我们可以使用一维卷积来处理和分析定时数据。
* 具有多个输入通道的一维互相关操作可以看作是具有单个输入通道的二维互相关操作。
* 最大超时池化层的输入可以在每个通道上有不同数量的时间步长。
* TextCNN主要使用一维卷积层和最大超时池层。

## 练习

1. 调整超参数，并使用递归神经网络和卷积神经网络比较两种情感分析方法的准确性和操作效率。
1. 您是否可以通过使用上一节介绍的三种方法来进一步提高测试集上模型的准确性：调整超参数，使用更大的预训练词向量以及使用spaCy词标记工具？
1. 您可以将textCNN用于其他哪些自然语言处理任务？
