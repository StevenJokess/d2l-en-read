{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "natural-language-inference-and-dataset.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THIpA9houUXV",
        "colab_type": "text"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcBsJU5uuUXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf4dcdd4-05ff-4127-e771-c1509274a9ff"
      },
      "source": [
        "!pip install -U mxnet-cu101==1.7.0\n",
        "!pip install d2l==0.14.4\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet-cu101==1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/26/9655677b901537f367c3c473376e4106abc72e01a8fc25b1cb6ed9c37e8c/mxnet_cu101-1.7.0-py2.py3-none-manylinux2014_x86_64.whl (846.0MB)\n",
            "\u001b[K     |███████████████████████████████▌| 834.1MB 1.3MB/s eta 0:00:09tcmalloc: large alloc 1147494400 bytes == 0x65908000 @  0x7fe4be07c615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
            "\u001b[K     |████████████████████████████████| 846.0MB 20kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (1.18.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/53/39/4ab213673844e0c004bed8a0781a0721a3f6bb23eb8854ee75c236428892/graphviz-0.8.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101==1.7.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101==1.7.0) (3.0.4)\n",
            "Installing collected packages: graphviz, mxnet-cu101\n",
            "  Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-cu101-1.7.0\n",
            "Collecting d2l==0.14.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/9b/7fd84dcfa6d4f51f5671ed581e749586beb6a7432804bcf28fde32df4df7/d2l-0.14.4-py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (3.2.2)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.14.4) (1.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.14.4) (2.4.7)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.10.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (4.7.7)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.14.4) (5.2.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.14.4) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->d2l==0.14.4) (1.15.0)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (4.3.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.0.7)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (0.8.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (4.6.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (1.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client>=5.2.0 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (2.11.2)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.14.4) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel->jupyter->d2l==0.14.4) (5.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.8.4)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (2.6.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (1.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.14.4) (3.1.5)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (1.9.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.14.4) (19.0.2)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.14.4) (3.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.14.4) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->jupyter->d2l==0.14.4) (4.4.2)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->d2l==0.14.4) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.14.4) (0.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->d2l==0.14.4) (1.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.14.4) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.14.4) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.14.4) (50.3.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.14.4) (0.7.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.14.4) (20.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.14.4) (0.2.5)\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.14.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "ub1xpCNVuUXf",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Inference and the Dataset\n",
        ":label:`sec_natural-language-inference-and-dataset`\n",
        "\n",
        "In :numref:`sec_sentiment`, we discussed the problem of sentiment analysis.\n",
        "This task aims to classify a single text sequence into predefined categories,\n",
        "such as a set of sentiment polarities.\n",
        "However, when there is a need to decide whether one sentence can be inferred form another, \n",
        "or eliminate redundancy by identifying sentences that are semantically equivalent,\n",
        "knowing how to classify one text sequence is insufficient.\n",
        "Instead, we need to be able to reason over pairs of text sequences.\n",
        "\n",
        "\n",
        "## Natural Language Inference\n",
        "\n",
        "*Natural language inference* studies whether a *hypothesis*\n",
        "can be inferred from a *premise*, where both are a text sequence.\n",
        "In other words, natural language inference determines the logical relationship between a pair of text sequences.\n",
        "Such relationships usually fall into three types:\n",
        "\n",
        "* *Entailment*: the hypothesis can be inferred from the premise.\n",
        "* *Contradiction*: the negation of the hypothesis can be inferred from the premise.\n",
        "* *Neutral*: all the other cases.\n",
        "\n",
        "Natural language inference is also known as the recognizing textual entailment task.\n",
        "For example, the following pair will be labeled as *entailment* because \"showing affection\" in the hypothesis can be inferred from \"hugging one another\" in the premise.\n",
        "\n",
        "> Premise: Two women are hugging each other.\n",
        "\n",
        "> Hypothesis: Two women are showing affection.\n",
        "\n",
        "The following is an example of *contradiction* as \"running the coding example\" indicates \"not sleeping\" rather than \"sleeping\".\n",
        "\n",
        "> Premise: A man is running the coding example from Dive into Deep Learning.\n",
        "\n",
        "> Hypothesis: The man is sleeping.\n",
        "\n",
        "The third example shows a *neutrality* relationship because neither \"famous\" nor \"not famous\" can be inferred from the fact that \"are performing for us\". \n",
        "\n",
        "> Premise: The musicians are performing for us.\n",
        "\n",
        "> Hypothesis: The musicians are famous.\n",
        "\n",
        "Natural language inference has been a central topic for understanding natural language.\n",
        "It enjoys wide applications ranging from\n",
        "information retrieval to open-domain question answering.\n",
        "To study this problem, we will begin by investigating a popular natural language inference benchmark dataset.\n",
        "\n",
        "\n",
        "## The Stanford Natural Language Inference (SNLI) Dataset\n",
        "\n",
        "Stanford Natural Language Inference (SNLI) Corpus is a collection of over $500,000$ labeled English sentence pairs :cite:`Bowman.Angeli.Potts.ea.2015`.\n",
        "We download and store the extracted SNLI dataset in the path `../data/snli_1.0`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "28"
        },
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "dKZ2VlFZuUXg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59a62869-bc3c-4617-b321-a5e5db66d108"
      },
      "source": [
        "import collections\n",
        "from d2l import mxnet as d2l\n",
        "from mxnet import gluon, np, npx\n",
        "import os\n",
        "import re\n",
        "import zipfile\n",
        "\n",
        "npx.set_np()\n",
        "\n",
        "#@save\n",
        "d2l.DATA_HUB['SNLI'] = (\n",
        "    'https://nlp.stanford.edu/projects/snli/snli_1.0.zip',\n",
        "    '9fcde07509c7e87ec61c640c1b2753d9041758e4')\n",
        "\n",
        "data_dir = d2l.download_extract('SNLI')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/snli_1.0.zip from https://nlp.stanford.edu/projects/snli/snli_1.0.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "id": "we91yGxUuUXn",
        "colab_type": "text"
      },
      "source": [
        "### Reading the Dataset\n",
        "\n",
        "The original SNLI dataset contains much richer information than what we really need in our experiments. Thus, we define a function `read_snli` to only extract part of the dataset, then return lists of premises, hypotheses, and their labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "66"
        },
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "Qsy8N-M4uUXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@save\n",
        "def read_snli(data_dir, is_train):\n",
        "    \"\"\"Read the SNLI dataset into premises, hypotheses, and labels.\"\"\"\n",
        "    def extract_text(s):\n",
        "        # Remove information that will not be used by us\n",
        "        s = re.sub('\\\\(', '', s) \n",
        "        s = re.sub('\\\\)', '', s)\n",
        "        # Substitute two or more consecutive whitespace with space\n",
        "        s = re.sub('\\\\s{2,}', ' ', s)\n",
        "        return s.strip()\n",
        "    label_set = {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
        "    file_name = os.path.join(data_dir, 'snli_1.0_train.txt'\n",
        "                             if is_train else 'snli_1.0_test.txt')\n",
        "    with open(file_name, 'r') as f:\n",
        "        rows = [row.split('\\t') for row in f.readlines()[1:]]\n",
        "    premises = [extract_text(row[1]) for row in rows if row[0] in label_set]\n",
        "    hypotheses = [extract_text(row[2]) for row in rows if row[0] in label_set]\n",
        "    labels = [label_set[row[0]] for row in rows if row[0] in label_set]\n",
        "    return premises, hypotheses, labels"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "zZQuRWUhuUXs",
        "colab_type": "text"
      },
      "source": [
        "Now let us print the first $3$ pairs of premise and hypothesis, as well as their labels (\"0\", \"1\", and \"2\" correspond to \"entailment\", \"contradiction\", and \"neutral\", respectively ).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "70"
        },
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "Z6OvG0osuUXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "4fb22caf-a0f7-4e9c-b7e8-c87369ab5a2b"
      },
      "source": [
        "train_data = read_snli(data_dir, is_train=True)\n",
        "for x0, x1, y in zip(train_data[0][:3], train_data[1][:3], train_data[2][:3]):\n",
        "    print('premise:', x0)\n",
        "    print('hypothesis:', x1)\n",
        "    print('label:', y)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is training his horse for a competition .\n",
            "label: 2\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is at a diner , ordering an omelette .\n",
            "label: 1\n",
            "premise: A person on a horse jumps over a broken down airplane .\n",
            "hypothesis: A person is outdoors , on a horse .\n",
            "label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "Csotc_xyuUXy",
        "colab_type": "text"
      },
      "source": [
        "The training set has about $550,000$ pairs,\n",
        "and the testing set has about $10,000$ pairs.\n",
        "The following shows that \n",
        "the three labels \"entailment\", \"contradiction\", and \"neutral\" are balanced in \n",
        "both the training set and the testing set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "g2-g6pdPuUXz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a6faeb15-8784-40c1-c7bd-9c1a4adccb2a"
      },
      "source": [
        "test_data = read_snli(data_dir, is_train=False)\n",
        "for data in [train_data, test_data]:\n",
        "    print([[row for row in data[2]].count(i) for i in range(3)])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[183416, 183187, 182764]\n",
            "[3368, 3237, 3219]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 8,
        "id": "Gx2YrIYEuUX3",
        "colab_type": "text"
      },
      "source": [
        "### Defining a Class for Loading the Dataset\n",
        "\n",
        "Below we define a class for loading the SNLI dataset by inheriting from the `Dataset` class in Gluon. The argument `num_steps` in the class constructor specifies the length of a text sequence so that each minibatch of sequences will have the same shape. \n",
        "In other words,\n",
        "tokens after the first `num_steps` ones in longer sequence are trimmed, while special tokens “&lt;pad&gt;” will be appended to shorter sequences until their length becomes `num_steps`.\n",
        "By implementing the `__getitem__` function, we can arbitrarily access the premise, hypothesis, and label with the index `idx`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "115"
        },
        "origin_pos": 9,
        "tab": [
          "mxnet"
        ],
        "id": "WI6Mqi9ruUX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@save\n",
        "class SNLIDataset(gluon.data.Dataset):\n",
        "    \"\"\"A customized dataset to load the SNLI dataset.\"\"\"\n",
        "    def __init__(self, dataset, num_steps, vocab=None):\n",
        "        self.num_steps = num_steps\n",
        "        all_premise_tokens = d2l.tokenize(dataset[0])\n",
        "        all_hypothesis_tokens = d2l.tokenize(dataset[1])\n",
        "        if vocab is None:\n",
        "            self.vocab = d2l.Vocab(all_premise_tokens + all_hypothesis_tokens,\n",
        "                                   min_freq=5, reserved_tokens=['<pad>'])\n",
        "        else:\n",
        "            self.vocab = vocab\n",
        "        self.premises = self._pad(all_premise_tokens)\n",
        "        self.hypotheses = self._pad(all_hypothesis_tokens)\n",
        "        self.labels = np.array(dataset[2])\n",
        "        print('read ' + str(len(self.premises)) + ' examples')\n",
        "\n",
        "    def _pad(self, lines):\n",
        "        return np.array([d2l.truncate_pad(\n",
        "            self.vocab[line], self.num_steps, self.vocab['<pad>'])\n",
        "                         for line in lines])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (self.premises[idx], self.hypotheses[idx]), self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.premises)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 10,
        "id": "st4cIvdIuUX-",
        "colab_type": "text"
      },
      "source": [
        "### Putting All Things Together\n",
        "\n",
        "Now we can invoke the `read_snli` function and the `SNLIDataset` class to download the SNLI dataset and return `DataLoader` instances for both training and testing sets, together with the vocabulary of the training set.\n",
        "It is noteworthy that we must use the vocabulary constructed from the training set\n",
        "as that of the testing set. \n",
        "As a result, any new token from the testing set will be unknown to the model trained on the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "114"
        },
        "origin_pos": 11,
        "tab": [
          "mxnet"
        ],
        "id": "_NToNdk1uUX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@save\n",
        "def load_data_snli(batch_size, num_steps=50):\n",
        "    \"\"\"Download the SNLI dataset and return data iterators and vocabulary.\"\"\"\n",
        "    num_workers = d2l.get_dataloader_workers()\n",
        "    data_dir = d2l.download_extract('SNLI')\n",
        "    train_data = read_snli(data_dir, True)\n",
        "    test_data = read_snli(data_dir, False)\n",
        "    train_set = SNLIDataset(train_data, num_steps)\n",
        "    test_set = SNLIDataset(test_data, num_steps, train_set.vocab)\n",
        "    train_iter = gluon.data.DataLoader(train_set, batch_size, shuffle=True,\n",
        "                                       num_workers=num_workers)\n",
        "    test_iter = gluon.data.DataLoader(test_set, batch_size, shuffle=False,\n",
        "                                      num_workers=num_workers)\n",
        "    return train_iter, test_iter, train_set.vocab"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 12,
        "id": "oSKDUSH6uUYD",
        "colab_type": "text"
      },
      "source": [
        "Here we set the batch size to $128$ and sequence length to $50$,\n",
        "and invoke the `load_data_snli` function to get the data iterators and vocabulary.\n",
        "Then we print the vocabulary size.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "111"
        },
        "origin_pos": 13,
        "tab": [
          "mxnet"
        ],
        "id": "WkC2H_2VuUYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "ca04223c-8646-4b35-821b-a185f8f97b82"
      },
      "source": [
        "train_iter, test_iter, vocab = load_data_snli(128, 50)\n",
        "len(vocab)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "read 549367 examples\n",
            "read 9824 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18678"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 14,
        "id": "DmeaATaUuUYJ",
        "colab_type": "text"
      },
      "source": [
        "Now we print the shape of the first minibatch.\n",
        "Contrary to sentiment analysis,\n",
        "we have $2$ inputs `X[0]` and `X[1]` representing pairs of premises and hypotheses.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "113"
        },
        "origin_pos": 15,
        "tab": [
          "mxnet"
        ],
        "id": "gNz91GMMuUYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2493e748-3448-4a14-8ac8-5877e8a62c9e"
      },
      "source": [
        "for X, Y in train_iter:\n",
        "    print(X[0].shape)\n",
        "    print(X[1].shape)\n",
        "    print(Y.shape)\n",
        "    break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 50)\n",
            "(128, 50)\n",
            "(128,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 16,
        "id": "q3beyvrnuUYN",
        "colab_type": "text"
      },
      "source": [
        "## Summary\n",
        "\n",
        "* Natural language inference studies whether a hypothesis can be inferred from a premise, where both are a text sequence.\n",
        "* In natural language inference, relationships between premises and hypotheses include entailment, contradiction, and neutral.\n",
        "* Stanford Natural Language Inference (SNLI) Corpus is a popular benchmark dataset of natural language inference.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Machine translation has long been evaluated based on superficial $n$-gram matching between an output translation and a ground-truth translation. Can you design a measure for evaluating machine translation results by using natural language inference?\n",
        "1. How can we change hyperparameters to reduce the vocabulary size?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 17,
        "tab": [
          "mxnet"
        ],
        "id": "eBAsM8ruuUYO",
        "colab_type": "text"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/394)\n"
      ]
    }
  ]
}