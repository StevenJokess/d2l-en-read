

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-31 19:56:39
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-12 19:51:33
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-applications/natural-language-inference-attention.html
-->

# 自然语言推理: 注意力的运用

在15.4节中，我们介绍了自然语言推理任务和 SNLI 数据集。鉴于许多模型是基于复杂和深层的架构，Parikh 等人提出解决自然语言推理与注意机制，并称之为“可分解注意模型”[ Parikh 等人，2016]。这导致了一个没有循环层或卷积层的模型，在 SNLI 数据集上用更少的参数取得了最好的结果。在本节中，我们将描述并实现这种基于注意力的自然语言推理方法(使用 MLPs) ，如图15.5.1所示。

图15.5.1本部分将预先训练的 GloVe 提供给基于注意力和 mlp 的自然语言推理架构。

## 模型

比起在前提和假设中保持单词的顺序更简单的是，我们可以将一个文本序列中的单词与另一个文本序列中的每个单词对齐，反之亦然，然后比较和聚合这些信息来预测前提和假设之间的逻辑关系。类似于机器翻译中源语句和目标语句之间的词语对齐，前提和假设之间的词语对齐可以通过注意力机制巧妙地完成。

图15.5.2使用注意机制的自然语言推理。

图15.5.2描绘了使用注意机制的自然语言推理方法。在高层次上，它包括三个共同培训的步骤: 参加，比较和聚集。我们将在下面一步一步地说明它们。

TODO:CODE

### 参与

第一步是将一个文本序列中的单词与另一个序列中的每个单词对齐。如果：前提是“我确实需要睡眠” ，而假设是“我累了”。由于语义相似性，我们可能希望将假设中的“i”与前提中的“i”对齐，并将假设中的“tired”与前提中的“ sleep”对齐。同样，我们可能希望将前提中的“i”与假设中的“i”对齐，并将前提中的“需要”和“睡眠”与假设中的“累”对齐。注意，这样的对齐方式是柔性的，使用加权平均数，其中理想的大权重与要对齐的单词相关联。为了便于演示，图15.5.2显示了这种硬的方式排列。

现在我们使用注意力机制更详细地描述软对齐。表示 a = (a1，... ，a m) 和 b = (b1，... ，b n)前提和假设，其中 a i，bj ∈ Rd (i = 1，... ，m，j = 1，... ，n)是 d 维嵌入向量。对于软比对，我们计算注意力的权重 eij ∈ r

TODO:MATH

其中函数f是在以下mlp函数中定义的多层感知器。 f的输出维由mlp的num_hiddens参数指定。

需要强调的是，在(15.5.1)中，ff分别接受ai和bj的输入，而不是同时接受它们的一对作为输入。这种分解技巧只导致ff的m+n应用(线性复杂度)，而不是mn应用(二次复杂度)。

将(15.5.1)中的注意权值归一化，计算假设中嵌入的所有单词的加权平均值，得到假设在前提下与i索引的单词软对齐的表示:

TODO:MATH

同样，我们计算假设中jj索引的每个词的前提词软对齐:

TODO:MATH

下面我们定义了Attend类来计算假设(beta)与输入假设A的软对齐以及假设(alpha)与输入假设B的软对齐。

TODO:CODE

### 比较

在下一步中，我们将一个序列中的单词与另一个与该单词软对齐的序列进行比较。请注意，在软对齐中，一个序列中的所有单词(尽管可能具有不同的注意权重)将与另一个序列中的单词进行比较。为了便于演示，图15.5.2将单词与排列整齐的单词进行硬配对。例如，假设参与步骤确定了前提中的“需要”和“睡眠”都与假设中的“累”对齐，则将“累-需要睡眠”对进行比较。

在比较步骤,我们喂连接(操作符(⋅⋅][⋅⋅])的文字从一个序列和对齐的文字从其他序列函数gg(一个多层感知器):

TODO:MATH

在(15.5.4)中，vA,ivA,i是前提中单词ii与所有与单词ii软对齐的假设单词之间的比较;而vB,jvB,j是假设词jj与所有与jj软对齐的前提词之间的比较。下面的比较类定义如比较步骤。

TODO:CODE

### 聚合

有两组比较向量vA,ivA,i (i=1，…，mi=1，…，m)和vB,jvB,j (j=1，…，nj=1，…，n)，在最后一步我们将这些信息进行汇总来推断逻辑关系。我们首先总结这两种情况:

TODO:MATH

接下来我们将两个总结结果的串联输入到hh函数(一个多层感知器)中，得到逻辑关系的分类结果:

TODO:MATH

聚合步骤在以下Aggregate类中定义。


TODO:CODE

### 把所有东西放在一起

通过将参加、比较和聚集三个步骤放在一起，我们定义了可分解注意模型来共同训练这三个步骤。

TODO:CODE

## 训练和评估模型

现在我们将训练和评估SNLI数据集上定义的可分解注意力模型。我们从读取数据集开始。

### 读取数据集

我们使用15.4节中定义的函数下载和读取SNLI数据集。批大小和序列长度分别设置为256和50。

TODO:CODE

### 创建模型

我们使用预先训练好的100100维手套嵌入来表示输入标记。因此，我们将:eqref:eq_nli_e中的向量aiai和bjbj的维数预定义为100100。函数ff in(15.5.1)和gg in:eqref:eq_nli_v_ab的输出维数设置为200200。然后创建一个模型实例，初始化其参数，并加载嵌入的手套来初始化输入令牌的向量。

TODO:CODE

### 训练和评估模型

其中函数 f 是在下面的 mlp 函数中定义的多层感知机。F 的输出维度由 mlp 的 num _ hiddens 参数指定。

与12.5节中的`split_batch`函数接受单个输入(如文本序列(或图像))不同，我们定义了一个`split_batch_multi_input`函数来接受多个输入(如在小批量中接受前提和假设)。

TODO:CODE

现在我们可以在SNLI数据集上训练和评估模型。

TODO:CODE

### 使用模型

最后，定义预测函数，用来输出前提与假设之间的一对逻辑关系。

TODO:CODE

我们可以使用训练过的模型来获得一个句子样本对的自然语言推理结果。

TODO:CODE

## 小结

* 可分解注意模型包括预测前提与假设之间的逻辑关系的三个步骤:注意、比较和聚集。
* 通过注意机制，我们可以将一个文本序列中的单词与另一个文本序列中的每个单词对齐，反之亦然。使用加权平均，这种对齐是软的，在理想情况下，较大的权重与要对齐的单词相关联。
* 在计算注意力权重时，分解技巧可以得到一个比二次复杂度更理想的线性复杂度。
* 我们可以使用预先训练好的字嵌入作为下游自然语言处理任务(如自然语言推理)的输入表示。

## 练习

1. 用其他超参数组合训练模型。你能在测试集上获得更好的精确度吗?
1. 自然语言推理中可分解注意模型的主要缺点是什么?
1. 假设我们想要得到任意一对句子的语义相似程度(例如，00到11之间的连续值)。我们如何收集和标签数据集?你能设计一个有注意机制的模型吗?
