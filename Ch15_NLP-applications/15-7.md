

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-15 01:25:27
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-05 22:52:40
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-applications/natural-language-inference-bert.html
-->

# 自然语言推理: 微调 BERT

在本章的前面部分，我们已经为 SNLI 数据集上的自然语言推理任务设计了一个基于注意力的体系结构(见15.5节)。现在我们通过微调 BERT 来重新讨论这个任务。正如在15.6节中所讨论的，自然语言推理是一个序列级别的文本对分类问题，而微调 BERT 只需要一个额外的基于 mlp 的体系结构，如图15.7.1所示。

图15.7.1这一部分将预先训练的 BERT 提供给基于 mlp 的自然语言推理架构。

在本节中，我们将下载一个预先训练的小版本 BERT，然后对它进行微调，以便在 SNLI 数据集上进行自然语言推断。

TODO:CODE

## 加载预训练的 BERT

我们已经在14.9和14.10节中解释了如何在 WikiText-2数据集上预先训练 BERT (注意，原始的 BERT 模型是在更大的语料库上预先训练的)。正如在14.10节中所讨论的，原始的 BERT 模型有数亿个参数。在下面，我们提供了两个版本的预训练 BERT: “ BERT.base”大约和原始 BERT 基模型一样大，需要大量的计算资源进行微调，而“ BERT.small”是一个小版本，以方便演示。

TODO:CODE

任何一个预训练的 BERT 模型都包含一个定义词汇表集的“ vocab.json”文件和一个预训练参数的“ pretrained.params”文件。我们实现了如下的负载预训练模型函数来加载预训练的误码率测试参数。

TODO:CODE

为了方便在大多数机器上进行演示，我们将在本节中加载并微调预先训练的 BERT 的小版本(“ BERT.small”)。在练习中，我们将展示如何微调更大的“ bert.base” ，以显著提高测试准确性。

TODO:CODE

## 用于精调 BERT 的数据集

对于 SNLI 数据集上的下游任务自然语言推断，我们定义了一个自定义的数据集类 SNLIBERTDataset。在每个例子中，前提和假设形成一对文本序列，并被打包成一个 BERT 输入序列，如图15.6.2所示。回忆14.8.4节，段 id 用于区分 BERT 输入序列中的前提和假设。随着预定义的 BERT 输入序列的最大长度(max _ len) ，输入文本对中较长的最后一个标记将不断被删除，直到满足最大 len 为止。为了加速生成用于微调 BERT 的 SNLI 数据集，我们使用4个工作进程并行地生成训练或测试示例。

TODO:CODE

下载SNLI数据集之后，我们通过实例化SNLIBERTDataset类来生成训练和测试示例。这样的例子将在自然语言推理的训练和测试期间分批读取。

TODO:CODE

## 微调 BERT

如图15.6.2所示，对BERT进行微调以进行自然语言推理仅需要一个额外的MLP，该MLP包含两个完全连接的层（请参见以下BERTClassifier类中的self.hidden和self.output）。该MLP将特殊的“ <cls>”令牌的BERT表示形式转换为自然语言推断的三个输出：蕴涵性，矛盾性和中立性，该特殊的“ <cls>”标记对前提和假设的信息进行编码。

TODO:CODE

下面，将预训练的BERT模型bert馈入BERTClassifier实例网络以用于下游应用程序。在BERT微调的常见实现中，只会从头开始学习其他MLP（net.output）的输出层的参数。将对预训练的BERT编码器（net.encoder）和附加MLP的隐藏层（net.hidden）的所有参数进行微调。

TODO:CODE

回想一下，在14.8节中，MaskLM类和NextSentencePred类在其使用的MLP中均具有参数。这些参数是预训练的BERT模型bert中的参数的一部分，因此是net中的参数的一部分。但是，这些参数仅用于计算预训练期间的掩蔽语言建模损失和下一句预测损失。这两个损失函数与微调下游应用无关，因此，当对BERT进行微调时，MaskLM和NextSentencePred中使用的MLP的参数不会更新（陈旧）。

要允许参数具有陈旧的渐变，请在d2l.train_batch_ch13的步进函数中设置标志ignore_stale_grad = True。我们使用此功能通过SNLI的训练集（train_iter）和测试集（test_iter）训练和评估模型网络。由于计算资源有限，因此可以进一步提高训练和测试的准确性：我们将其讨论留在练习中。

TODO:CODE

## 小结

* 我们可以为下游应用微调预训练的BERT模型，例如SNLI数据集上的自然语言推断。
* 在微调期间，BERT模型成为下游应用程序模型的一部分。仅与预训练损耗有关的参数在微调期间不会更新。

## 练习

1. 如果您的计算资源允许，请微调一个更大的，经过预训练的BERT模型，该模型与原始BERT基本模型一样大。将load_pretrained_model函数中的参数设置为：用“ bert.base”替换“ bert.small”，将num_hiddens = 256，ffn_num_hiddens = 512，num_heads = 4，num_layers = 2的值分别增加到768、3072、12、12。通过增加微调时间（可能还需要调整其他超参数），您能否获得高于0.86的测试精度？
1. 如何根据长度比截断一对序列？ 比较此对截断方法和SNLIBERTDataset类中使用的方法。他们的优缺点是什么？
