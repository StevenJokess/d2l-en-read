

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-31 20:18:11
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-31 20:29:55
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_natural-language-processing-applications/finetuning-bert.html
 * https://easyai.tech/ai-definition/bert/
-->

# 用于序列级和令牌级应用的微调 BERT

在本章的前几节中，我们已经为自然语言处理应用程序设计了不同的模型，例如基于 RNNs、 CNNs、 attention 和 MLPs 的模型。当存在空间或时间限制时，这些模型是有用的，然而，为每个自然语言处理任务制定一个特定的模型实际上是不可行的。在第14.8节中，我们介绍了一个预训练模型 BERT，它要求对大范围的自然语言处理任务进行最小的体系结构更改。一方面，在提出这个建议的时候，BERT 改进了各种自然语言处理任务的先进水平。另一方面，如第14.10节所述，原始 BERT 模型的两个版本有1.1亿个和3.4亿个参数。因此，当有足够的计算资源时，我们可以考虑为下游的自然语言处理应用程序微调 BERT。

在下面，我们将自然语言处理应用程序的子集概括为序列级和令牌级。在序列层次上，介绍了在单文本分类和文本对分类或回归中，如何将文本输入的 BERT 表示转换为输出标签。在令牌层面，我们将简要介绍文本标签和问答等新应用，并阐明 BERT 如何表示输入并将其转换为输出标签。在微调过程中，BERT 跨不同应用程序所要求的“最小体系结构更改”是额外的全连接层。在下游应用程序的监督式学习几个月，额外层的参数从头学习，而预先训练的 BERT 模型中的所有参数都经过微调。

## 单文本分类

单文本分类以单个文本序列作为输入，输出其分类结果。除了我们在本章中研究的情感分析之外，语言可接受性语料库(CoLA)也是一个用于单文本分类的数据集，用于判断一个给定的句子在语法上是否可接受。例如，“I should study.”是可以接受的，但是“I should studying”不是。

图15.6.1用于情感分析和语言可接受性测试等单文本分类应用的微调 BERT。假设输入的单个文本有六个标记。

第14.8节描述了 BERT 的输入表示。BERT 输入序列无歧义地表示单个文本和文本对，其中特殊分类标记“ < cls > ”用于序列分类，特殊分类标记“ < sep > ”用于标记单个文本的末尾或分隔一对文本。如图15.6.1所示，在单个文本分类应用程序中，特殊分类标记“ < cls > ”的 BERT 表示对整个输入文本序列的信息进行编码。作为输入单个文本的表示，它将被反馈到一个由完全连接(密集)层组成的小 MLP 输出所有离散标签值的分布。

## 文本对分类或回归

在本章中，我们还考察了自然语言推理。它属于文本对分类，一种对一对文本进行分类的应用程序。

以一对文本作为输入，但输出一个连续值，语义文本相似度是一个流行的文本对回归任务。这项任务测量句子的语义相似度。例如，在语义文本相似性基准数据集中，一对句子的相似性得分是从0(没有意义重叠)到5(意义等价)的序数尺度[ Cer 等人，2017]。我们的目标是预测这些分数。来自语义文本相似性基准数据集的例子包括(句子1，句子2，相似性评分) :

* “A plane is taking off.”, “An air plane is taking off.”, 5.000;
* “A woman is eating something.”, “A woman is eating meat.”, 3.000;
* “A woman is dancing.”, “A man is talking.”, 0.000

图15.6.2用于文本对分类或回归应用(如自然语言推理和语义文本相似性)的微调 BERT。假设输入文本对具有两个和三个标记。

与图15.6.1中的单文本分类相比，图15.6.2中的文本对分类的微调BERT在输入表示上有所不同。对于文本对回归任务(如语义文本相似度)，可以应用一些细小的更改，比如输出连续的标签值和使用均数平方损失:这在回归中很常见。

## 文本标签

现在让我们考虑标记级任务，比如文本标记，其中每个标记被分配一个标签。在文本标注任务中，词性标注根据单词在句子中的作用给每个单词分配一个词性标注(如形容词和限定词)。例如，根据Penn Treebank II标签集，句子“John Smith ' s car is new”应该被标记为“NNP(名词，专有单数)NNP POS(所有格结尾)NN(名词，单数或质量)VB(动词，基本形式)JJ(形容词)”。

图15.6.3文本标注应用的BERT微调，如词性标注。假设输入的单个文本有六个标记。

用于文本标记应用的BERT微调如图15.6.3所示。与图15.6.1相比，唯一的区别在于文本标记中，输入文本的每个标记的BERT表示被输入到相同的全连接层中，输出标记的标签，例如词性标记。

## 问题回答

作为另一种符号级应用，回答问题反映了阅读理解能力。例如，Stanford Question answer Dataset (SQuAD v1.1)由阅读文章和问题组成，其中每个问题的答案都是来自文章的一段文本(文本跨度)，问题是关于的[Rajpurkar et al.， 2016]。要解释这个问题，请参考下面这段话:“一些专家报告说口罩的功效并不是决定性的。不过，口罩制造商坚称，他们的N95口罩等产品可以预防这种病毒。还有一个问题是“谁说N95口罩可以预防病毒?”答案应该是文中的“mask makers”。因此，在SQuAD v1.1中，我们的目标是通过一对问题和段落来预测文本跨度的开始和结束。

图15.6.4对BERT进行问答微调。假设输入文本对有两个和三个标记。

为了调整BERT的问题回答，在BERT的输入中，问题和段落分别被打包为第一个和第二个文本序列。为了预测文本范围开始的位置，相同的附加全连接层将把任何记号的BERT表示从位置ii转换为标量分数sisi。所有段落令牌的分数进一步由softmax操作转换为概率分布，以便为段落中每个令牌位置ii分配一个作为文本跨度开始的概率pipi。预测文本跨度的结束与上面相同，除了它的附加全连接层中的参数独立于预测开始的参数。在预测结束时，位置ii的任何通道标记由同一全连通层转换为标量分数eiei。:numref:fig_bert-qa描述了为回答问题对BERT进行微调。

对于问题的回答，监督学习的训练目标很简单，就是最大限度地提高ground-truth开始和结束位置的对数概率。在预测跨度时，我们可以计算从位置i到位置j (i≤ji≤j)的有效跨度的分数si+ejsi+ej，并输出得分最高的跨度

## 小结

* BERT对序列级和令牌级自然语言处理应用程序要求最小的体系结构更改（额外的全连接层），例如单个文本分类（例如，情感分析和测试语言可接受性），文本对分类或回归（例如，自然的） 语言推断和语义文本相似性），文本标记（例如词性标记）和问题回答。
* 在下游应用程序的有监督学习期间，从头开始学习额外层的参数，同时对预训练的BERT模型中的所有参数进行微调。

## 练习

1. 让我们为新闻文章设计一个搜索引擎算法。当系统收到一个查询(例如，“冠状病毒爆发期间的石油行业”)时，它应该返回与该查询最相关的新闻文章的排序列表。假设我们有一个巨大的新闻文章池和大量的查询。为了简化问题，假设已经为每个查询标记了最相关的文章。如何在算法设计中应用负抽样(见14.2.1节)和BERT ?
1. 我们如何利用BERT来训练语言模型?
1. 我们能在机器翻译中利用BERT吗?
