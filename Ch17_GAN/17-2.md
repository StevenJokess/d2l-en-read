

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-19 11:28:24
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-14 20:54:59
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/dcgan.html
 * [2]: https://nndl.github.io/ 5.5.1
-->

# 深度卷积生成对抗网络
:label:`sec_dcgan`

在17.1节中，我们介绍了GANs工作原理背后的基本思想。我们展示了它们可以从一些简单的、易于抽样的分布(如均匀分布或正态分布)中抽取样本，并将它们转换为似乎与某些数据集的分布相匹配的样本。虽然我们的匹配二维高斯分布的例子得到了这个点，但它并不是特别令人兴奋。

在本节中，我们将演示如何使用GANs生成逼真的图像。我们的模型将基于[Radford et al., 2015]中介绍的deep convolutional GANs (DCGAN)。我们将借用卷积架构，已经证明如此成功的甄别计算机视觉问题，并展示如何通过GANs，他们可以被利用来生成逼真的图像。

TODO:CODE

## 口袋妖怪数据集

我们将使用的数据集是Pokemon精灵从[pokemondb](https://pokemondb.net/sprites)获得的一个集合。首先下载、提取和加载此数据集。

TODO:CODE

我们将每个图像调整为$64\times 64$。 `ToTensor`变换会将像素值投影到[0,1]中，而我们的生成器将使用tanh函数来获取$[-1, 1]$中的输出。 因此，我们使用0.5平均值和0.5标准偏差对数据进行归一化以匹配值范围。

TODO:CODE

让我们可视化前20张图像。

TODO:CODE

## 生成器

生成器需要将噪声变量$\mathbf z\in\mathbb R^d$(长度-d向量)映射到宽度和高度为64×64的RGB图像。在13.11节中，我们介绍了使用转置卷积层来扩大输入大小的全卷积网络(参见:numref:`sec_transposed_conv`)。(我们将低维特征映射到高维特征的卷积操作称为转置卷积（Transposed Convolution）[Dumoulinet al.,2016]，也称为反卷积（Deconvolution）[Zeiler et al.,2011]．)[2] 生成器的基本块包含一个转置卷积层，然后是批处理归一化和ReLU激活。（步长𝑆 < 1的转置卷积也称为微步卷积（Fractionally-Strided Convolution）[Long et al.,2015]．）[2]

在默认情况下，转置卷积层使用$k_h = k_w = 4$内核，一个$s_h = s_w = 2$步，和一个$p_h = p_w = 1$填充。输入形状$n_h^{'} \times n_w^{'} = 16 \times 16$，发生器块将输入的宽度和高度加倍。

$$
\begin{aligned}
n_h^{'} \times n_w^{'} &= [(n_h k_h - (n_h-1)(k_h-s_h)- 2p_h] \times [(n_w k_w - (n_w-1)(k_w-s_w)- 2p_w]\\
  &= [(k_h + s_h (n_h-1)- 2p_h] \times [(k_w + s_w (n_w-1)- 2p_w]\\
  &= [(4 + 2 \times (16-1)- 2 \times 1] \times [(4 + 2 \times (16-1)- 2 \times 1]\\
  &= 32 \times 32 .\\
\end{aligned}
$$

生成器由四个基本块组成，可以将输入的宽度和高度从1增加到32。同时，先将潜在变量投射到64×8个通道中，然后每次将通道减半。最后，利用转置卷积层生成输出。它进一步加倍宽度和高度，以匹配所需的64×64形状，并减少通道大小为3。tanh激活函数用于将输出值投影到(−1,1)范围。

TODO:CODE

生成一个100维的潜在变量来验证发电机的输出形状。

TODO:CODE

## 鉴别器

该鉴别器是一个正常的卷积网络，只是它使用了一个leaky ReLU作为它的激活函数。给定 $\alpha \in[0, 1]$，其定义为

$$\textrm{leaky ReLU}(x) = \begin{cases}x & \text{if}\ x > 0\\ \alpha x &\text{otherwise}\end{cases}.$$

可以看出，如果a =0，则ReLU正常;如果a =1，则为identity函数。对于mhz∈(0,1)，leaky ReLU是一个非线性函数，对于负的输入给出非零的输出。它旨在解决“死亡的ReLU”问题，即由于ReLU的梯度为0，神经元可能总是输出一个负值，因此无法取得任何进展。

TODO:CODE

鉴别器的基本块是卷积层、批处理归一层和泄漏的ReLU激活。卷积层的超参数与生成器块中的转置卷积层相似。

TODO:CODE

带有默认设置的基本块将把输入的宽度和高度减半，正如我们在:numref:`sec_padding`中演示的那样。例如，给定输入形状$n_h = n_w = 16$，核形状$k_h = k_w = 4$，步幅形状$s_h = s_w = 2$，填充形状$p_h = p_w = 1$，输出形状为:

$$
\begin{aligned}
n_h^{'} \times n_w^{'} &= \lfloor(n_h-k_h+2p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+2p_w+s_w)/s_w\rfloor\\
  &= \lfloor(16-4+2\times 1+2)/2\rfloor \times \lfloor(16-4+2\times 1+2)/2\rfloor\\
  &= 8 \times 8 .\\
\end{aligned}
$$

TODO:CODE

鉴别器是发生器的镜像。

TODO:CODE

它使用输出通道1作为最后一层的卷积层来获得单个预测值。

TODO:CODE

## 训练

与:numref:`sec_basic_gan`中的基本GAN相比，我们对生成器和鉴别器使用相同的学习率，因为它们彼此相似。 另外，我们将Adam (:numref:`sec_adam`)中的β1从0.9更改为0.5。 它会降低动量的平滑度（过去梯度的指数加权移动平均值），以照顾快速变化的梯度，因为生成器和鉴别器会相互竞争。 此外，随机产生的噪声Z是一个4维张量，我们使用GPU来加速计算。

TODO:CODE

我们以少量的epoch训练模型，仅用于演示。 为了获得更好的性能，可以将变量`num_epochs`设置为更大的数字。

TODO:CODE

## 总结

* DCGAN结构有四个卷积层用于鉴别器，四个“分数条纹”卷积层用于发生器。
* 鉴别器是一个4层步幅卷积与批处理规范化(除了它的输入层)和泄漏继电器激活。
* Leaky ReLU是一个非线性函数，在负输入时给出非零输出。它的目标是修复“死亡的ReLU”问题，并帮助梯度更容易地通过架构。

## 练习

1. 如果我们使用标准的ReLU激活而不是leaky ReLU，会发生什么?
1. 将DCGAN应用到Fashion-MNIST上，看看哪个类别比较好，哪个不合适。
