

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-08 17:42:09
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-26 20:58:46
 * @Description:
 * @TODO::
 * @Reference:
-->

# WassersteinGAN with Gradient Penality

WGANæ²¡æœ‰é‚£ä¹ˆå¥½ç”¨ï¼Œä¸»è¦åŸå› åœ¨äºWAGNè¿›è¡Œæ¢¯åº¦æˆªæ–­ã€‚æ¢¯åº¦æˆªæ–­å°†å¯¼è‡´åˆ¤åˆ«ç½‘ç»œè¶‹å‘äºä¸€ä¸ªäºŒå€¼ç½‘ç»œï¼Œé€ æˆæ¨¡å‹å®¹é‡çš„ä¸‹é™ã€‚[9]

åœ¨æå‡ºäº†WGANåï¼Œä½œè€…ç»§ç»­åœ¨WGANä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œåˆç»™å‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ï¼ŒæŠ›å¼ƒweight clippingï¼Œä¹Ÿå°±ä¸å†éœ€è¦ç»éªŒå¸¸æ•°cäº†ï¼Œå–è€Œä»£ä¹‹çš„æ˜¯gradient penalityï¼ˆæ¢¯åº¦æƒ©ç½šï¼‰ï¼Œå› æ­¤å–åä¸ºWGAN_GPï¼Œä¹Ÿå«Improved_WGANã€‚[8]

Gradient penalityæ˜¯æŒ‡ï¼šå¯¹Dçš„æ¯ä¸€ä¸ªè¾“å…¥æ ·æœ¬x, ä½¿å¾— $\left\|\nabla_{\hat{x}} D(\hat{x})\right\|_{2} \leq 1$ ã€‚æ„æ€æ˜¯å¯¹äºä»»æ„ä¸€ä¸ª
è¾“å…¥æ ·æœ¬x, ç”¨Dçš„è¾“å‡ºç»“æœD(x)å¯¹æ±‚æ¢¯åº¦åçš„å€¼çš„ L2èŒƒæ•° ä¸å¤§äº1ã€‚

ä¸Šé¢çš„è§£é‡Šå¯èƒ½æœ‰äº›æ‹—å£ï¼Œæˆ‘ä»¬ä»ä¸€ç»´ç©ºé—´ä¸­çš„å‡½æ•°f(x)æ¥è¿›è¡Œé˜è¿°ï¼šä¸€ç»´å‡½æ•°f(x)ï¼Œå¯¹äºä»»æ„è¾“å…¥ $x,$ è¯¥å‡½æ•°æ»¡è¶³: $\quad\left(\frac{d f(x)}{d x}\right)^{2} \leq 1 ï¼Œ$ å³ä»»æ„ä¸€ç‚¹çš„æ–œç‡çš„å¹³æ–¹ä¸å¤§äº1ï¼Œè¿›è€Œå¯ä»¥æ¨å‡º:
$\left(f\left(x_{1}\right)-f\left(x_{2}\right)\right)^{2} \leq\left(x_{1}-x_{2}\right)^{2},$ å¯æƒ³è€ŒçŸ¥ $f(x)$ çš„å‡½æ•°æ›²çº¿æ˜¯æ¯”è¾ƒå¹³æ»‘çš„, æ‰€ä»¥ç§°ä¸ºæ¢¯åº¦æƒ©ç½šã€‚é‚£ä¸º

ä»€ä¹ˆæ˜¯ L2èŒƒæ•° å‘¢è€Œä¸æ˜¯ L1èŒƒæ•° å‘¢ï¼ŸåŸå› å¾ˆç®€å•, L1èŒƒæ•°ä¼šç ´åä¸€ä¸ªå‡½æ•°çš„å¯å¾®æ€§å‘€, æ‰€ä»¥ L2èŒƒ
æ•° æ˜¯éå¸¸åˆç†çš„!

æ³¨æ„å‰é¢æˆ‘è¯´çš„æ˜¯é’ˆå¯¹äºDçš„æ¯ä¸€ä¸ªè¾“å…¥æ ·æœ¬, éƒ½è®©å®ƒæ»¡è¶³ || $\nabla_{x} D(x) \|_{2} \leq 1 ï¼Œ$ å®é™…ä¸Šè¿™æ˜¯ä¸ç°å®
çš„, æ‰€ä»¥ä½œè€…åˆæƒ³äº†ä¸€ä¸ªåŠæ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼šå‡è®¾ä»çœŸå®æ•°æ®ä¸­é‡‡æ ·å‡ºæ¥çš„ä¸€ä¸ªç‚¹ç§°ä¸ºxï¼ˆè¿™ä¸ªç‚¹
æ˜¯é«˜ç»´ç©ºé—´ä¸­çš„ç‚¹ $ï¼‰,$ Gåˆ©ç”¨é‡‡æ ·å¾—åˆ°çš„å™ªå£°å‘é‡æ‰€ç”Ÿæˆçš„å‡æ•°æ®ç§°ä¸º $\hat{\chi},$ åœ¨è¿™ä¸¤ç‚¹ä¹‹é—´çš„æŸä¸€ä¸ªä½
ç½®é‡‡æ ·ä¸€ä¸ªç‚¹è®°ä¸º, å³å¯¹äºæ¯ä¸€ä¸ª $\hat{\chi},,$ å°½é‡è®© $\left\|\nabla_{\hat{x}} D(\hat{x})\right\|_{2} \leq 1$ ã€‚é‚£ä¹ˆæœ€å¸¸è§çš„æ»¡è¶³ä¸Šè¿°è¦æ±‚
çš„é‡‡æ ·æ–¹æ³•å°±æ˜¯çº¿æ€§é‡‡æ ·æ–¹æ³•äº†ï¼Œå³åœ¨xä¸ $\tilde{x}$ æ‰€å½¢æˆçš„è¶…å¹³é¢ä¸Šä»»æ„é€‰å–ä¸€ä¸ªç‚¹ $\hat{x},$, æ¢å¥è¯è¯´å°±æ˜¯
åœ¨ç”Ÿæˆæ ·æœ¬å’ŒçœŸå®æ ·æœ¬é—´åšä¸€ä¸ªçº¿æ€§æ’å€¼, æ‰€ä»¥å­˜åœ¨ $t \in[0,1],$ ä½¿å¾— $\hat{x}=t x+(1-t) \tilde{x}$ ã€‚
åœ¨æ–°çš„ æŸå¤±å‡½æ•° é—ªäº®ç™»åœºä¹‹å‰, æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªå°å°çš„ä¼˜åŒ–! å› ä¸ºä½œè€…æœ€åå‘ç°, å…¶å®è®©
$\left\|\nabla_{\hat{x}} D(\hat{x})\right\|_{2} \approx 1$ æ˜¯æœ€å¥½çš„æ–¹æ¡ˆ, è€Œä¸æ˜¯æŠŠ1ä½œä¸ºä¸Šã€ä¸‹é™, åˆ«é—®æˆ‘ä¸ºä»€ä¹ˆ, ä½œè€…ä¹Ÿä¸çŸ¥é“ï¼å› ä¸ºæ˜¯é€šè¿‡å¤§é‡çš„å®éªŒæ€»ç»“å‡ºæ¥çš„ã€‚

å³ä½¿å¾—ç”Ÿæˆå™¨çš„åˆ†å¸ƒğ‘ğ‘”ä¸çœŸå®åˆ†å¸ƒğ‘ğ‘Ÿä¹‹é—´çš„ EM è·ç¦»è¶Šå°è¶Šå¥½ã€‚è€ƒè™‘åˆ°ğ”¼ğ’™ğ‘Ÿâˆ¼ğ‘ğ‘Ÿ[ğ·(ğ’™ğ‘Ÿ)]ä¸€é¡¹ ä¸ç”Ÿæˆå™¨æ— å…³ï¼Œå› æ­¤ç”Ÿæˆå™¨çš„è®­ç»ƒç›®æ ‡ç®€å†™ä¸º

$\min _{\phi} \mathcal{L}(G, D)=-\mathbb{E}_{x_{f} \sim p_{g}}\left[D\left(x_{f}\right)\right]$
$\quad=-\mathbb{E}_{\mathbf{z} \sim p_{z}(\cdot)}[D(G(\mathbf{z}))]$



ç›®æ ‡å‡½æ•°ï¼š

$L=\underbrace{\underset{\tilde{\boldsymbol{x}} \sim \mathbb{P}_{g}}{\mathbb{E}}[D(\tilde{\boldsymbol{x}})]-\underset{\boldsymbol{x} \sim \mathbb{P}_{r}}{\mathbb{E}}[D(\boldsymbol{x})]}_{\text {Original critic loss }}+\underbrace{\lambda \underset{\hat{\boldsymbol{x}} \sim \mathbb{P}_{\hat{\boldsymbol{x}}}}{\mathbb{E}}\left[\left(\left\|\nabla_{\hat{\boldsymbol{x}}} D(\hat{\boldsymbol{x}})\right\|_{2}-1\right)^{2}\right]}_{\text {Our gradient penalty }} .$

```py
def d_loss_fn(generator, discriminator, batch_z, batch_x, is_training):
    # è®¡ç®— D çš„æŸå¤±å‡½æ•°     fake_image = generator(batch_z, is_training) # å‡æ ·æœ¬     d_fake_logits = discriminator(fake_image, is_training) # å‡æ ·æœ¬çš„è¾“å‡º     d_real_logits = discriminator(batch_x, is_training) # çœŸæ ·æœ¬çš„è¾“å‡º     # è®¡ç®—æ¢¯åº¦æƒ©ç½šé¡¹     gp = gradient_penalty(discriminator, batch_x, fake_image)
    # WGAN-GP D æŸå¤±å‡½æ•°çš„å®šä¹‰ï¼Œè¿™é‡Œå¹¶ä¸æ˜¯è®¡ç®—äº¤å‰ç†µï¼Œè€Œæ˜¯ç›´æ¥æœ€å¤§åŒ–æ­£æ ·æœ¬çš„è¾“å‡º     # æœ€å°åŒ–å‡æ ·æœ¬çš„è¾“å‡ºå’Œæ¢¯åº¦æƒ©ç½šé¡¹     loss = tf.reduce_mean(d_fake_logits) - tf.reduce_mean(d_real_logits) + 1 0. * gp

    return loss, gp
```

```py
# [5]

def gradient_penalty_loss(y_pred, y_average):
    gradients = tf.gradients(y_pred, y_average)[0]
    gradients_sqr = tf.square(gradients)
    gradients_sqr_sum = tf.reduce_sum(gradients_sqr, axis=np.arange(1, len(gradients_sqr.shape)))
    gradients_l2_norm = tf.sqrt(gradients_sqr_sum)

    gradient_penalty = tf.square(gradients_l2_norm)

    return tf.reduce_mean(gradient_penalty)

def discriminator_loss(real_output, generated_output, validity_interpolated, interpolated_img, lambda_=10):

    real_loss = -tf.reduce_mean(real_output)
    fake_loss = tf.reduce_mean(generated_output)
    gp_loss = gradient_penalty_loss(validity_interpolated, interpolated_img)

    return real_loss + fake_loss + gp_loss * lambda_
```

åŒæ ·æ²¡æœ‰äº¤å‰ç†µçš„è®¡ç®—æ­¥éª¤ã€‚ä»£ç å®ç°å¦‚ä¸‹ï¼š

```py
def g_loss_fn(generator, discriminator, batch_z, is_training):
    # ç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°     fake_image = generator(batch_z, is_training)     d_fake_logits = discriminator(fake_image, is_training)
    # WGAN-GP G æŸå¤±å‡½æ•°ï¼Œæœ€å¤§åŒ–å‡æ ·æœ¬çš„è¾“å‡ºå€¼     loss = - tf.reduce_mean(d_fake_logits)

    return loss
```

```py
# [5]
def generator_loss(generated_output):
    return -tf.reduce_mean(generated_output)
```


its stablity during training.[2]

## Generator

For G I use 4 transposed convolutional layer to rebuild a 3*128*128 RGB image.

```python
class G_net(nn.Module):
    def __init__(self,in_dim=latent_dim):
        super(G_net,self).__init__()
        self.fc1= nn.Sequential(
            nn.Linear(in_dim,units[4]),
            nn.ReLU(),
            nn.Linear(units[4],units[3]*fs[0]*fs[1]),
            nn.ReLU(),
            nn.BatchNorm1d(units[3]*fs[0]*fs[1])
        )
        self.ct1 = nn.Sequential(
            nn.ConvTranspose2d(units[3],units[2],k_size[3],stride=strides[3],padding=padding[3],output_padding=strides[3]/2),
            nn.BatchNorm2d(units[2]),
            nn.ReLU()
        )#[64,12,12]
        self.ct2 = nn.Sequential(
            nn.ConvTranspose2d(units[2],units[1],k_size[2],stride=strides[2],padding=padding[2],output_padding=strides[2]/2),
            nn.BatchNorm2d(units[1]),
            nn.ReLU()
        )#[32,27,27]
        self.ct3 = nn.Sequential(
            nn.ConvTranspose2d(units[1],units[0],k_size[1],stride=strides[1],padding=padding[1],output_padding=strides[1]/2),
            nn.BatchNorm2d(units[0]),
            nn.ReLU()
        )#[3,57,57]
        self.ct4 = nn.Sequential(
            nn.ConvTranspose2d(units[0],small_image_size[0],k_size[0],stride=strides[0],padding=padding[0],output_padding=strides[0]/2),
            nn.Tanh()
        )#[3,289,289]
    def forward(self,X):
        X = self.fc1(X)
        X = self.ct1(X.view(-1,units[3],fs[0],fs[1]))
        X = self.ct2(X)
        X = self.ct3(X)
        return self.ct4(X)
```

## Discriminator


```python
class D_net(nn.Module):
    def __init__(self):
        super(D_net,self).__init__()
        self.conv1=nn.Sequential(
            nn.Conv2d(image_size[0],units[0],k_size[0],strides[0],padding=padding[0]),
            nn.BatchNorm2d(units[0]),
            nn.LeakyReLU(0.2)
        )
        self.conv2=nn.Sequential(
            nn.Conv2d(units[0],units[1],k_size[1],strides[1],padding=padding[1]),
            nn.BatchNorm2d(units[1]),
            nn.LeakyReLU(0.2)
        )
        self.conv3=nn.Sequential(
            nn.Conv2d(units[1],units[2],k_size[2],strides[2],padding=padding[2]),
            nn.BatchNorm2d(units[2]),
            nn.LeakyReLU(0.2)
        )
        self.conv4=nn.Sequential(
            nn.Conv2d(units[2],units[3],k_size[3],strides[3],padding=padding[3]),
            nn.BatchNorm2d(units[3]),
            nn.LeakyReLU(0.2)
        )
        self.fc1 = nn.Linear(units[3]*fs[0]*fs[1],units[4])
        self.dp = nn.Dropout(0.5)
        self.d_out = nn.Linear(units[4],1)
    def forward(self,X):
        X = self.conv1(X)
        X = self.conv2(X)
        X = self.conv3(X)
        X = self.conv4(X)
        X = X.view((-1,units[3]*fs[0]*fs[1]))
        X = self.dp(F.leaky_relu(self.fc1(X)))
        out = self.d_out(X)
        return out
```

## Train

```python
d_iter = 1
g_iter = 1
epoch = 3000
for e in range(epoch):
    for data in Pk_dataloader:

        for i in range(d_iter):
            #real data
            for p in d_model.parameters():
                p.requires_grad = True
            d_model.zero_grad()
            true_data = Variable(data)
            if use_GPU:
                true_data=true_data.cuda(device)
            d_true_score = d_model(true_data)
            true_loss = -d_true_score.mean()
            #fake data
            noise = get_noise(true_data.size()[0])
            if use_GPU:
                noise = noise.cuda(device)
            fake_data = g_model(noise)
            d_fake_score = d_model(fake_data)
            fake_loss = d_fake_score.mean()
            w_loss = loss_with_penalty(d_model,true_data.data,fake_data.data)
            loss =true_loss+fake_loss+w_loss
            loss.backward()
            d_optimizer.step()
        for i in range(g_iter):
            #train G
            g_model.zero_grad()
            for p in d_model.parameters():
                p.requires_grad = False
            noise = get_noise()
            if use_GPU:
                noise = noise.cuda(device)
            fake_data = g_model(noise)
            g_score = d_model(fake_data)
            g_loss = -g_score.mean()
            g_loss.backward()
            g_optimizer.step()
    if e%10==0:
        fake_imgs=fake_data.cpu().data*0.5 +0.5
        img = ToPILImage()(fake_imgs[3])
        img.save('result/%s/G_result/iter_%d.png'%(code,e))
        torch.save(d_model,'result/%s/D_checkpoint/iter_d_%d.pt'%(code,e))
        torch.save(g_model,'result/%s/G_checkpoint/iter_g_%d.pt'%(code,e))
```

Paper:[3]
Code: [4]




[1]: Gulrajani,  I.,  Ahmed,  F.,  Arjovsky,  M.,  Dumoulin,  V.,  and  Courville,A. C. (2017). Improved training of Wasserstein GANs. InAdvances inNeural Information Processing Systems(pp. 5767-5777).
[2]: https://lanpartis.github.io/deep%20learning/2018/06/24/Use-GANs-to-Generate-Pokemons.html
[3]: https://arxiv.org/abs/1704.00028
[4]: https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py
[5]: https://github.com/thisisiron/TF2-GAN/blob/master/wgan-gp/utils.py

TODO:
[6]: https://github.com/lilianweng/unified-gan-tensorflow
[7]: https://github.com/igul222/improved_wgan_training
[8]: https://www.jiqizhixin.com/articles/2019-06-13-11
[9]: https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(GAN)/ch7.md
https://github.com/caogang/wgan-gp
https://github.com/igul222/improved_wgan_training
