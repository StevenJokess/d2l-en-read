

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-19 11:17:17
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-14 20:44:07
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/gan.html
 * https://github.com/d2l-ai/d2l-en/edit/master/chapter_generative-adversarial-networks/gan.md
-->

# 生成对抗网络
:label:`sec_basic_gan`

在本书的大部分内容中，我们都讨论了如何进行预测。 在某种形式上，我们使用了深度神经网络学习的从数据示例到标签的映射。 这种学习称为判别学习，因为我们希望能够区分照片中的猫和狗中的照片。 分类器和回归器都是歧视性学习的例子。 而且，通过反向传播训练的神经网络颠覆了我们认为有关大型复杂数据集的判别式学习的所有知识。 在短短5至6年间，高分辨率图像的分类精度已从无用变成了人类级别（有些警告）。 我们将为您提供其他所有关于深度神经网络功能惊人的出色判别任务的帮助。

但是，机器学习还有很多功能，而不仅仅是解决区分性任务。 例如，给定一个大型数据集，而没有任何标签，我们可能想学习一个可以精确捕获此数据特征的模型。 给定这样一个模型，我们可以对类似于训练数据分布的综合数据示例进行采样。 例如，给定大量的面部照片，我们可能希望能够生成新的真实感图像，看起来好像它可能来自同一数据集。 这种学习称为生成建模。

直到最近，我们还没有方法可以合成新颖的逼真的图像。 但是，深度神经网络用于判别学习的成功开辟了新的可能性。 在过去三年中，一大趋势是应用区分性深网来克服我们通常不认为是监督学习的问题中的挑战。 递归神经网络语言模型是使用判别网络（经过训练可预测下一个字符）的一个示例，该网络一旦受过训练就可以充当生成模型。

2014年，一篇突破性的论文引入了生成式对抗网络(GANs) :cite:`Goodfellow.Pouget-Abadie.Mirza.ea.2014`，这是一种巧妙的新方法，可以利用歧视模型的力量来获得好的生成模型。在GANs的核心思想中，他们认为如果我们不能区分真实数据和假数据，那么数据生成器就是好的。在统计学中，这称为双样本检验，该检验用于回答数据集$X=\{x_1,\ldots, x_n\}$ 和 $X'=\{x'_1,\ldots, x'_n\}$是否来自同一分布。大多数统计论文和甘斯论文的主要区别在于，甘斯论文以一种建设性的方式使用了这一观点。换句话说，他们不是训练一个模型说“嘿，这两个数据集看起来不像是来自同一个分布”，而是使用双样本测试为生成模型提供训练信号。这允许我们改进数据生成器，直到它生成类似于真实数据的东西。至少，它需要愚弄分类器。即使我们的分类器是一种先进的深度神经网络。

图17.1.1生成对抗网络
:label:`fig_gan`

GAN架构如:numref:`fig_gan`所示。 如您所见，GAN架构中有两个部分-首先，我们需要一个设备（例如，深层网络，但实际上可能是任何东西，例如游戏渲染引擎），它可能能够生成看起来很漂亮的数据。 就像真实的东西一样。 如果要处理图像，则需要生成图像。 如果要处理语音，则需要生成音频序列，依此类推。 我们称其为发电机网络。 第二部分是鉴别器网络。 它试图将伪造数据与真实数据区分开。 这两个网络相互竞争。 生成器网络尝试欺骗鉴别器网络。 此时，鉴别器网络将适应新的伪数据。 该信息继而用于改善发电机网络，等等。

鉴别器是一个二进制分类器，用于区分输入x是真实的（来自真实数据）还是伪造的（来自生成器）。 通常，鉴别器为输入x输出标量预测o∈R，例如使用具有隐藏大小1的密集层，然后应用S形函数获得预测概率$D(\mathbf x) = 1/(1+e^{-o})$ 。 假设真实数据的标签y为1，假数据的标签y为0。 我们训练鉴别器以最小化交叉熵损失，即

$$ \min_D \{ - y \log D(\mathbf x) - (1-y)\log(1-D(\mathbf x)) \},$$

对于生成器，它首先从随机性源中提取一些参数$\mathbf z\in\mathbb R^d$，例如正态分布$\mathbf z \sim \mathcal{N} (0, 1)$。 我们通常将zz称为潜在变量。 然后，它应用一个函数来生成x'= G（z））。 生成器的目的是欺骗鉴别器，将x'= G（z分类为真实数据，即，我们希望D（G（z））≈1。 换句话说，对于给定的鉴别器$D$，我们更新生成器$G$的参数，以在$y=0$（即，

$$ \max_G \{ - (1-y) \log(1-D(G(\mathbf z))) \} = \max_G \{ - \log(1-D(G(\mathbf z))) \}.$$

如果生成器完成了完美的工作，则$D(\mathbf x')\approx 1$，因此上述损失接近于0，这导致梯度太小，无法为鉴别器取得良好进展。所以通常我们会尽量减少以下损失:

$$ \min_G \{ - y \log(D(G(\mathbf z))) \} = \min_G \{ - \log(D(G(\mathbf z))) \}, $$

也就是将$\mathbf x'=G(\mathbf z)$输入到鉴别器中，但给标签$y=1$。

综上所述，D和G正以综合目标函数进行“极大极小”博弈:

$$min_D max_G \{ -E_{x \sim \text{Data}} log D(\mathbf x) - E_{z \sim \text{Noise}} log(1 - D(G(\mathbf z))) \}.$$

许多GANs应用程序都在图像上下文中。作为演示目的，我们首先满足于拟合一个更简单的分布。我们将说明如果我们使用GANs来建立世界上最低效的高斯参数估计会发生什么。让我们开始吧。

TODO:CODE

## 生成一些“真实的”数据

因为这将是世界上最蹩脚的例子，我们简单地从高斯分布中生成数据。

TODO:CODE

让我们看看我们得到了什么。这应该是用平均bb和协方差矩阵ATAATA任意地进行高斯偏移。

TODO:CODE

## 生成器

我们的生成器网络将是最简单的网络-单层线性模型。 这是因为我们将使用高斯数据生成器来驱动线性网络。 因此，它实际上只需要学习参数就可以完美地伪造事物。

TODO:CODE

## 鉴别器

对于鉴别器，我们将更有鉴别能力:我们将使用一个3层的MLP使事情更有趣。

TODO:CODE

## 训练

首先，我们定义一个函数来更新鉴别器。

TODO:CODE

生成器的更新也是类似的。这里我们重用了交叉熵损失，但将伪数据的标签从0改为1。

TODO:CODE

鉴别器和产生器都对交叉熵损失进行二元逻辑回归。我们使用Adam使训练过程平稳。在每次迭代中，我们首先更新鉴别器，然后更新生成器。我们将损失和生成的示例可视化。

TODO:CODE

现在我们指定超参数来适应高斯分布。

TODO:CODE

## 总结

* 生成对抗网络由生成网络和鉴别网络两种深度网络组成。
* 生成器生成的图像尽可能接近真实图像愚弄鉴别器,通过最大化熵损失,也就是说,$\max \log(D(\mathbf{x'}))$。
* 鉴别器试图区分生成的图像和真实图像,通过最小化叉损失,即$\min - y \log D(\mathbf{x}) - (1-y)\log(1-D(\mathbf{x}))$ 。

## 练习

* 是否存在一个均衡，即在有限样本上，鉴别器最终无法区分这两个分布?

