

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-24 22:02:12
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-26 20:22:26
 * @Description:
 * @TODO::
 * @Reference:
-->

# Variational Autoencoders (VAEs)

## Expectation-Maximization (EM)

A straightforward way to approach VAE is through the construction of the well-known Expectation-Maximization (EM) algorithm. Please refer to this tutorial or this blog as a refresher on EM. Just to quicly recap a few key elements in EM: insteand of optimizing the log-liklihood ($\ell(\theta)$) directly with observable data $x$, latent variable $z$, EM constructs and optimize on a lower bound $\mathcal{L}(q,\theta)$ often referred to as Evidence Lower Bond (EBLO). The following equation derives from Jensen's inequality and holds for any $q(z)$ as long as it is a valid probability distribution.


$$\ell(\theta^{t-1}) \underset{E-step}{=} \mathcal L(q^t,\theta^{t-1}) \underset{M-step}{\le} \mathcal L(q^t,\theta^t) \underset{Jensen}{\le} \ell(\theta^{t})$$

## From EM to VAE

With more complex distributions of $p_\theta(x\vert z)$, the integration in E-step for exact inference of the posterier $p_\theta(z\vert x)$ is intractable. This posterier inference problem can be addressed with variational inference methods such as mean-field approximation (where we assume factorizable $q(z)$) or sampling based methods (e.g. collapsed Gibbs sampling for solving Latent Dirichlet allocation). Mean-field approximation put undue constraints on the variational family $q(z)$, and sampling based methods could have slow convergence problems. Moreover, both methods involves arduous derivation of update functions, that would require rederivation even for small changes in model and thus could limit the exploration of more complex models.

Auto-Encoding Variational Bayes brought about a flexible neural-network based approach. In this framework, the variational inference / variational optimization task of finding the optimal $q$ become a matter of finding the best parameters of a neural network via backpropagation and stochastic gradient descent. Thus making blackbox inference possible as well as allowing scalable trainng for deeper and larger neural network models. We refer to this framework as Neural Variational Inference.


VAEï¼Œä¹Ÿå¯ä»¥å«åšå˜åˆ†è‡ªç¼–ç å™¨ï¼Œå±äºè‡ªåŠ¨ç¼–ç å™¨çš„å˜ä½“ã€‚

VAEæ˜¯å¯¹è‡ªåŠ¨ç¼–ç å™¨çš„æ¦‚ç‡å¤„ç†ï¼Œå®ƒæ˜¯ä¸€ç§å°†é«˜ç»´è¾“å…¥æ•°æ®å‹ç¼©æˆæ›´å°è¡¨ç¤ºçš„æ¨¡å‹ã€‚ä¼ ç»Ÿçš„è‡ªåŠ¨ç¼–ç å™¨å°†è¾“å…¥æ˜ å°„åˆ°æ½œåœ¨çš„å‘é‡ä¸Šï¼ŒVAEä¸åŒäºæ­¤ï¼Œå®ƒå°†è¾“å…¥æ•°æ®æ˜ å°„åˆ°æ¦‚ç‡åˆ†å¸ƒçš„å‚æ•°ä¸Šï¼Œä¾‹å¦‚é«˜æ–¯åˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ã€‚è¿™ç§æ–¹æ³•äº§ç”Ÿäº†ä¸€ä¸ªè¿ç»­çš„ã€ç»“æ„åŒ–çš„æ½œåœ¨ç©ºé—´ï¼Œå¯¹å›¾åƒçš„ç”Ÿæˆéå¸¸æœ‰ç”¨ã€‚[6]

å¯¹äºå˜åˆ†è‡ªç¼–ç å™¨æˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ªä¸æ˜“å¤„ç†çš„å¯†åº¦å‡½æ•°ï¼Œé€šè¿‡é™„åŠ çš„éšå˜é‡$z$å¯¹å¯†åº¦å‡½æ•°è¿›è¡Œå»ºæ¨¡ã€‚[15] VAEåŸç†å›¾å¦‚ä¸‹[6]ï¼š

VAEé€šè¿‡çº¦æŸéšå˜é‡$z$æœä»æ ‡å‡†æ­£å¤ªåˆ†å¸ƒä»¥åŠé‡æ„æ•°æ®å®ç°äº†åˆ†å¸ƒè½¬æ¢æ˜ å°„$X=G(z)$[15]
VAEé€šè¿‡éšå˜é‡$z$ä¸æ ‡å‡†æ­£å¤ªåˆ†å¸ƒçš„KLæ•£åº¦å’Œé‡æ„è¯¯å·®å»åº¦é‡ã€‚[15]

å‡è®¾ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚å›¾13.3æ‰€ç¤ºï¼‰ä¸­åŒ…å«éšå˜é‡ï¼Œå³æœ‰éƒ¨åˆ†å˜é‡æ˜¯ä¸å¯è§‚æµ‹çš„ï¼Œå…¶ä¸­è§‚æµ‹å˜é‡ğ‘¿æ˜¯ä¸€ä¸ªé«˜ç»´ç©ºé—´ğ’³ä¸­çš„éšæœºå‘é‡ï¼Œéšå˜é‡ğ’æ˜¯ä¸€ä¸ªç›¸å¯¹ä½ç»´çš„ç©ºé—´ğ’µä¸­çš„éšæœºå‘é‡ï¼

è‡ªåŠ¨ç¼–ç å™¨æ˜¯ä¸€ç§äººå·¥ç¥ç»ç½‘ç»œï¼Œç”¨äºå­¦ä¹ é«˜æ•ˆçš„æ•°æ®å€¼ç¼–ç ä»¥æ— ç›‘ç£æ–¹å¼ã€‚è‡ªåŠ¨ç¼–ç å™¨çš„ç›®çš„æ˜¯é€šè¿‡è®­ç»ƒç½‘ç»œå¿½ç•¥ä¿¡å·â€œå™ªå£°â€ æ¥å­¦ä¹ ä¸€ç»„æ•°æ®çš„è¡¨ç¤ºï¼ˆç¼–ç ï¼‰ï¼Œé€šå¸¸ç”¨äºé™ç»´ã€‚åŸºæœ¬æ¨¡å‹å­˜åœ¨å‡ ç§å˜ä½“ï¼Œå…¶ç›®çš„æ˜¯å¼ºåˆ¶å­¦ä¹ çš„è¾“å…¥è¡¨ç¤ºå½¢å¼å…·æœ‰æœ‰ç”¨çš„å±æ€§ã€‚

ä¸ç»å…¸ï¼ˆç¨€ç–ï¼Œå»å™ªç­‰ï¼‰è‡ªåŠ¨ç¼–ç å™¨ä¸åŒï¼Œå˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰æ˜¯ç”Ÿæˆæ¨¡å‹ï¼Œä¾‹å¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œã€‚æ–‡ç« é‡ç‚¹è§£å†³ï¼Œåœ¨å­˜åœ¨å…·æœ‰éš¾è§£çš„åéªŒåˆ†å¸ƒçš„è¿ç»­æ½œåœ¨å˜é‡å’Œå¤§å‹æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•åœ¨å®šå‘æ¦‚ç‡æ¨¡å‹ä¸­è¿›è¡Œæœ‰æ•ˆçš„æ¨ç†å’Œå­¦ä¹ ã€‚ä»–ä»¬å¼•å…¥äº†ä¸€ç§éšæœºå˜åˆ†æ¨ç†å’Œå­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥æ‰©å±•åˆ°å¤§å‹æ•°æ®é›†ï¼Œå¹¶ä¸”åœ¨æŸäº›å¾®åˆ†å¯å¾®æ€§æ¡ä»¶ä¸‹ç”šè‡³å¯ä»¥åœ¨éš¾å¤„ç†çš„æƒ…å†µä¸‹å·¥ä½œã€‚

ä½œè€…è¯æ˜äº†å˜åŒ–ä¸‹ç•Œçš„é‡æ–°å‚æ•°åŒ–äº§ç”Ÿäº†ä¸€ä¸ªä¸‹ç•Œä¼°è®¡é‡ï¼Œè¯¥ä¼°è®¡é‡å¯ä»¥ä½¿ç”¨æ ‡å‡†éšæœºæ¢¯åº¦æ–¹æ³•ç›´æ¥è¿›è¡Œä¼˜åŒ–ã€‚ å…¶æ¬¡è¡¨æ˜ï¼Œå¯¹äºæ¯ä¸ªæ•°æ®ç‚¹å…·æœ‰è¿ç»­æ½œåœ¨å˜é‡çš„iidæ•°æ®é›†ï¼Œé€šè¿‡ä½¿ç”¨æ‹Ÿè®®çš„ä¸‹ç•Œä¼°è®¡å™¨å°†è¿‘ä¼¼æ¨ç†æ¨¡å‹ï¼ˆä¹Ÿç§°ä¸ºè¯†åˆ«æ¨¡å‹ï¼‰æ‹Ÿåˆåˆ°éš¾å¤„ç†çš„åéªŒï¼Œå¯ä»¥ä½¿åéªŒæ¨ç†ç‰¹åˆ«æœ‰æ•ˆã€‚

ä¸»è¦æå‡ºè€…Durk Kingmaï¼ˆDiederik P. Kingmaï¼‰ï¼Œç›®å‰å°±èŒäºGoogleã€‚ åœ¨åŠ å…¥Googleä¹‹å‰ï¼Œäº2017å¹´è·å¾—é˜¿å§†æ–¯ç‰¹ä¸¹å¤§å­¦åšå£«å­¦ä½ï¼Œå¹¶äº2015å¹´æˆä¸ºOpenAIåˆ›å§‹å›¢é˜Ÿçš„ä¸€å‘˜ã€‚ ä»–ä¸»è¦ç ”ç©¶çš„æ–¹å‘ä¸ºï¼šæ¨ç†ï¼Œéšæœºä¼˜åŒ–ï¼Œå¯è¯†åˆ«æ€§ã€‚å…¶ä¸­çš„ç ”ç©¶æˆå°±åŒ…æ‹¬å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVAEï¼‰ï¼ˆä¸€ç§ç”¨äºç”Ÿæˆå»ºæ¨¡çš„æœ‰åŸåˆ™çš„æ¡†æ¶ï¼‰ä»¥åŠå¹¿æ³›ä½¿ç”¨çš„éšæœºä¼˜åŒ–æ–¹æ³•Adamã€‚[9]


In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term â€œvariationalâ€ comes from the close relation there is between the regularisation and the variational inference method in statistics.

autoencoders are neural networks architectures composed of both an encoder and a decoder that create a bottleneck to go through for data and that are trained to lose a minimal quantity of information during the encoding-decoding process (training by gradient descent iterations with the goal to reduce the reconstruction error)[4]

Auto-Encoding Variational Bayes by Diederik P Kingma and Max Welling, presented at ICLR 2014 (https://arxiv.org/abs/1312.6114).

```python
#[5]
class VAE(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def train_step(self, data):
        if isinstance(data, tuple):
            data = data[0]
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = encoder(data)
            reconstruction = decoder(z)
            reconstruction_loss = tf.reduce_mean(
                keras.losses.binary_crossentropy(data, reconstruction)
            )
            reconstruction_loss *= 28 * 28
            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            kl_loss = tf.reduce_mean(kl_loss)
            kl_loss *= -0.5
            total_loss = reconstruction_loss + kl_loss
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "loss": total_loss,
            "reconstruction_loss": reconstruction_loss,
            "kl_loss": kl_loss,
        }
```

```py
# VAE model
# [14]
class VAE(nn.Module):
    def __init__(self, image_size=784, h_dim=400, z_dim=20):
        super(VAE, self).__init__()
        self.fc1 = nn.Linear(image_size, h_dim)
        self.fc2 = nn.Linear(h_dim, z_dim)
        self.fc3 = nn.Linear(h_dim, z_dim)
        self.fc4 = nn.Linear(z_dim, h_dim)
        self.fc5 = nn.Linear(h_dim, image_size)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc2(h), self.fc3(h)

    def reparameterize(self, mu, log_var):
        std = torch.exp(log_var/2)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc4(z))
        return F.sigmoid(self.fc5(h))

    def forward(self, x):
        mu, log_var = self.encode(x)
        z = self.reparameterize(mu, log_var)
        x_reconst = self.decode(z)
        return x_reconst, mu, log_var
```

A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility. [3]

variational autoencoders (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a distribution over the latent space instead of a single point and by adding in the loss function a regularisation term over that returned distribution in order to ensure a better organisation of the latent space[4]


![VAE_Encoder](img\VAE_Encoder.png)[5]

![VAE_Decoder](img\VAE_Decoder.png)[5]

![VAE](img\autoencoder_loss.png)[5]

å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational AutoEncoderï¼ŒVAEï¼‰[Kingma et al.,2014]æ˜¯ä¸€ç§æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œå…¶æ€æƒ³æ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œæ¥åˆ†åˆ«å»ºæ¨¡ä¸¤ä¸ªå¤æ‚çš„æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ï¼ˆ1ï¼‰ç”¨ç¥ç»ç½‘ç»œæ¥ä¼°è®¡å˜åˆ†åˆ†å¸ƒğ‘(ğ’›;ğœ™)ï¼Œç§°ä¸ºæ¨æ–­ç½‘ç»œï¼ç†è®ºä¸Šğ‘(ğ’›;ğœ™)å¯ä»¥ä¸ä¾èµ–ğ’™ï¼ä½†ç”±äºğ‘(ğ’›;ğœ™)çš„ç›®æ ‡æ˜¯è¿‘ä¼¼åéªŒåˆ†å¸ƒğ‘(ğ’›|ğ’™;ğœƒ)ï¼Œå…¶å’Œğ’™ç›¸å…³ï¼Œå› æ­¤å˜åˆ†å¯†åº¦å‡½æ•°ä¸€èˆ¬å†™ä¸ºğ‘(ğ’›|ğ’™;ğœ™)ï¼æ¨æ–­ç½‘ç»œçš„è¾“å…¥ä¸ºğ’™ï¼Œè¾“å‡ºä¸ºå˜åˆ†åˆ†å¸ƒğ‘(ğ’›|ğ’™;ğœ™)ï¼ï¼ˆ2ï¼‰ç”¨ç¥ç»ç½‘ç»œæ¥ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒğ‘(ğ’™|ğ’›;ğœƒ)ï¼Œç§°ä¸ºç”Ÿæˆç½‘ç»œï¼ç”Ÿæˆç½‘ç»œçš„è¾“å…¥ä¸ºğ’›ï¼Œè¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒğ‘(ğ’™|ğ’›;ğœƒ)ï¼å°†æ¨æ–­ç½‘ç»œå’Œç”Ÿæˆç½‘ç»œåˆå¹¶å°±å¾—åˆ°äº†å˜åˆ†è‡ªç¼–ç å™¨çš„æ•´ä¸ªç½‘ç»œç»“æ„ï¼Œå¦‚å›¾13.4æ‰€ç¤ºï¼Œå…¶ä¸­å®çº¿è¡¨ç¤ºç½‘ç»œè®¡ç®—æ“ä½œï¼Œè™šçº¿è¡¨ç¤ºé‡‡æ ·æ“ä½œï¼



æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œæ¯”å¦‚å˜åˆ†è‡ªç¼–ç å™¨ã€æ·±åº¦ä¿¡å¿µç½‘ç»œç­‰ï¼Œéƒ½æ˜¯æ˜¾ç¤ºåœ°æ„å»ºå‡ºæ ·æœ¬çš„å¯†åº¦å‡½æ•°ğ‘(ğ’™;ğœƒ)ï¼Œå¹¶é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥æ±‚è§£å‚æ•°ï¼Œç§°ä¸ºæ˜¾å¼å¯†åº¦æ¨¡å‹ï¼ˆExplicit Density Modelï¼‰ï¼æ¯”å¦‚ï¼Œå˜åˆ†è‡ªç¼–ç å™¨çš„å¯†åº¦å‡½æ•°ä¸ºğ‘(ğ’™,ğ’›;ğœƒ) = ğ‘(ğ’™|ğ’›;ğœƒ)ğ‘(ğ’›;ğœƒ)ï¼è™½ç„¶ä½¿ç”¨äº†ç¥ç»ç½‘ç»œæ¥ä¼°è®¡ğ‘(ğ’™|ğ’›;ğœƒ)ï¼Œä½†æ˜¯æˆ‘ä»¬ä¾ç„¶å‡è®¾ğ‘(ğ’™|ğ’›;ğœƒ)ä¸ºä¸€ä¸ªå‚æ•°åˆ†å¸ƒæ—ï¼Œè€Œç¥ç»ç½‘ç»œåªæ˜¯ç”¨æ¥é¢„æµ‹è¿™ä¸ªå‚æ•°åˆ†å¸ƒæ—çš„å‚æ•°ï¼è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šé™åˆ¶äº†ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ï¼[6]

å˜åˆ†è‡ªç¼–ç å™¨æ˜¯ä¸€ä¸ªéå¸¸å…¸å‹çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›æ¥æœ‰æ•ˆåœ°è§£å†³å«éšå˜é‡çš„æ¦‚ç‡æ¨¡å‹ä¸­åéªŒåˆ†å¸ƒéš¾ä»¥ä¼°è®¡çš„é—®é¢˜[Kingma et al.,2014;Rezende et al.,2014]ï¼å˜åˆ†è‡ªç¼–ç å™¨çš„è¯¦å°½ä»‹ç»å¯ä»¥å‚è€ƒæ–‡çŒ®[Doersch,2016]ï¼[Bowman et al.,2016]è¿›ä¸€æ­¥å°†å˜åˆ†è‡ªç¼–ç å™¨åº”ç”¨äºåºåˆ—ç”Ÿæˆé—®é¢˜ï¼å†å‚æ•°åŒ–æ˜¯å˜åˆ†è‡ªç¼–ç å™¨çš„é‡è¦æŠ€å·§ï¼å¯¹äºç¦»æ•£å˜é‡çš„å†å‚æ•°åŒ–ï¼Œå¯ä»¥ä½¿ç”¨Gumbel-Softmaxæ–¹æ³•[Jang et al.,2017][6]

Auto-Encoding Variational Bayes by Kingma and Welling. It uses ReLUs and the adam optimizer, instead of sigmoids and adagrad.[13]

[1]: https://learning.oreilly.com/library/view/hands-on-artificial-intelligence/9781788836067/de965259-e07e-461a-8d0f-717745273397.xhtml
[2]: https://learning.oreilly.com/library/view/advanced-deep-learning/9781788629416/ch08.html
[3]: https://github.com/AntixK/PyTorch-VAE
[4]: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
[5]: https://keras.io/examples/generative/vae/
[6]: https://nndl.github.io/
[7]: Doersch C, 2016. Tutorial on variational autoencoders[J/OL]. CoRR, abs/1606.05908.http://arxiv.org/abs/1606.05908.
TODO:
[8]: https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book/blob/master/ch12-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/vae.py
[9]: https://www.aminer.cn/ai-history
[10]: https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example
[11]: https://www.tensorflow.org/tutorials/generative/cvae
[12]: https://github.com/pytorch/examples/tree/master/vae
[13]: http://arxiv.org/abs/1312.6114
https://github.com/zackchase/mxnet-the-straight-dope/blob/master/chapter13_unsupervised-learning/vae-gluon.ipynb
[14]: https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py#L38-L65
[15]: https://github.com/scutan90/DeepLearning-500-questions/blob/master/ch07_%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C(GAN)/ch7.md
