

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-24 22:02:12
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-17 14:47:54
 * @Description:
 * @TODO::
 * @Reference:
-->

# Variational Autoencoders (VAEs)

å‡è®¾ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚å›¾13.3æ‰€ç¤ºï¼‰ä¸­åŒ…å«éšå˜é‡ï¼Œå³æœ‰éƒ¨åˆ†å˜é‡æ˜¯ä¸å¯è§‚æµ‹çš„ï¼Œå…¶ä¸­è§‚æµ‹å˜é‡ğ‘¿æ˜¯ä¸€ä¸ªé«˜ç»´ç©ºé—´ğ’³ä¸­çš„éšæœºå‘é‡ï¼Œéšå˜é‡ğ’æ˜¯ä¸€ä¸ªç›¸å¯¹ä½ç»´çš„ç©ºé—´ğ’µä¸­çš„éšæœºå‘é‡ï¼



In a nutshell, a VAE is an autoencoder whose encodings distribution is regularised during the training in order to ensure that its latent space has good properties allowing us to generate some new data. Moreover, the term â€œvariationalâ€ comes from the close relation there is between the regularisation and the variational inference method in statistics.

autoencoders are neural networks architectures composed of both an encoder and a decoder that create a bottleneck to go through for data and that are trained to lose a minimal quantity of information during the encoding-decoding process (training by gradient descent iterations with the goal to reduce the reconstruction error)[4]

Auto-Encoding Variational Bayes by Diederik P Kingma and Max Welling, presented at ICLR 2014 (https://arxiv.org/abs/1312.6114).

```python
#[5]
class VAE(keras.Model):
    def __init__(self, encoder, decoder, **kwargs):
        super(VAE, self).__init__(**kwargs)
        self.encoder = encoder
        self.decoder = decoder

    def train_step(self, data):
        if isinstance(data, tuple):
            data = data[0]
        with tf.GradientTape() as tape:
            z_mean, z_log_var, z = encoder(data)
            reconstruction = decoder(z)
            reconstruction_loss = tf.reduce_mean(
                keras.losses.binary_crossentropy(data, reconstruction)
            )
            reconstruction_loss *= 28 * 28
            kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)
            kl_loss = tf.reduce_mean(kl_loss)
            kl_loss *= -0.5
            total_loss = reconstruction_loss + kl_loss
        grads = tape.gradient(total_loss, self.trainable_weights)
        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))
        return {
            "loss": total_loss,
            "reconstruction_loss": reconstruction_loss,
            "kl_loss": kl_loss,
        }
```

A collection of Variational AutoEncoders (VAEs) implemented in pytorch with focus on reproducibility. [3]

variational autoencoders (VAEs) are autoencoders that tackle the problem of the latent space irregularity by making the encoder return a distribution over the latent space instead of a single point and by adding in the loss function a regularisation term over that returned distribution in order to ensure a better organisation of the latent space[4]


![VAE_Encoder](img\VAE_Encoder.png)[5]

![VAE_Decoder](img\VAE_Decoder.png)[5]

![VAE](img\autoencoder_loss.png)[5]

å˜åˆ†è‡ªç¼–ç å™¨ï¼ˆVariational AutoEncoderï¼ŒVAEï¼‰[Kingma et al.,2014]æ˜¯ä¸€ç§æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œå…¶æ€æƒ³æ˜¯åˆ©ç”¨ç¥ç»ç½‘ç»œæ¥åˆ†åˆ«å»ºæ¨¡ä¸¤ä¸ªå¤æ‚çš„æ¡ä»¶æ¦‚ç‡å¯†åº¦å‡½æ•°ï¼ï¼ˆ1ï¼‰ç”¨ç¥ç»ç½‘ç»œæ¥ä¼°è®¡å˜åˆ†åˆ†å¸ƒğ‘(ğ’›;ğœ™)ï¼Œç§°ä¸ºæ¨æ–­ç½‘ç»œï¼ç†è®ºä¸Šğ‘(ğ’›;ğœ™)å¯ä»¥ä¸ä¾èµ–ğ’™ï¼ä½†ç”±äºğ‘(ğ’›;ğœ™)çš„ç›®æ ‡æ˜¯è¿‘ä¼¼åéªŒåˆ†å¸ƒğ‘(ğ’›|ğ’™;ğœƒ)ï¼Œå…¶å’Œğ’™ç›¸å…³ï¼Œå› æ­¤å˜åˆ†å¯†åº¦å‡½æ•°ä¸€èˆ¬å†™ä¸ºğ‘(ğ’›|ğ’™;ğœ™)ï¼æ¨æ–­ç½‘ç»œçš„è¾“å…¥ä¸ºğ’™ï¼Œè¾“å‡ºä¸ºå˜åˆ†åˆ†å¸ƒğ‘(ğ’›|ğ’™;ğœ™)ï¼ï¼ˆ2ï¼‰ç”¨ç¥ç»ç½‘ç»œæ¥ä¼°è®¡æ¦‚ç‡åˆ†å¸ƒğ‘(ğ’™|ğ’›;ğœƒ)ï¼Œç§°ä¸ºç”Ÿæˆç½‘ç»œï¼ç”Ÿæˆç½‘ç»œçš„è¾“å…¥ä¸ºğ’›ï¼Œè¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒğ‘(ğ’™|ğ’›;ğœƒ)ï¼å°†æ¨æ–­ç½‘ç»œå’Œç”Ÿæˆç½‘ç»œåˆå¹¶å°±å¾—åˆ°äº†å˜åˆ†è‡ªç¼–ç å™¨çš„æ•´ä¸ªç½‘ç»œç»“æ„ï¼Œå¦‚å›¾13.4æ‰€ç¤ºï¼Œå…¶ä¸­å®çº¿è¡¨ç¤ºç½‘ç»œè®¡ç®—æ“ä½œï¼Œè™šçº¿è¡¨ç¤ºé‡‡æ ·æ“ä½œï¼



æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œæ¯”å¦‚å˜åˆ†è‡ªç¼–ç å™¨ã€æ·±åº¦ä¿¡å¿µç½‘ç»œç­‰ï¼Œéƒ½æ˜¯æ˜¾ç¤ºåœ°æ„å»ºå‡ºæ ·æœ¬çš„å¯†åº¦å‡½æ•°ğ‘(ğ’™;ğœƒ)ï¼Œå¹¶é€šè¿‡æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ¥æ±‚è§£å‚æ•°ï¼Œç§°ä¸ºæ˜¾å¼å¯†åº¦æ¨¡å‹ï¼ˆExplicit Density Modelï¼‰ï¼æ¯”å¦‚ï¼Œå˜åˆ†è‡ªç¼–ç å™¨çš„å¯†åº¦å‡½æ•°ä¸ºğ‘(ğ’™,ğ’›;ğœƒ) = ğ‘(ğ’™|ğ’›;ğœƒ)ğ‘(ğ’›;ğœƒ)ï¼è™½ç„¶ä½¿ç”¨äº†ç¥ç»ç½‘ç»œæ¥ä¼°è®¡ğ‘(ğ’™|ğ’›;ğœƒ)ï¼Œä½†æ˜¯æˆ‘ä»¬ä¾ç„¶å‡è®¾ğ‘(ğ’™|ğ’›;ğœƒ)ä¸ºä¸€ä¸ªå‚æ•°åˆ†å¸ƒæ—ï¼Œè€Œç¥ç»ç½‘ç»œåªæ˜¯ç”¨æ¥é¢„æµ‹è¿™ä¸ªå‚æ•°åˆ†å¸ƒæ—çš„å‚æ•°ï¼è¿™åœ¨æŸç§ç¨‹åº¦ä¸Šé™åˆ¶äº†ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ï¼[6]

å˜åˆ†è‡ªç¼–ç å™¨æ˜¯ä¸€ä¸ªéå¸¸å…¸å‹çš„æ·±åº¦ç”Ÿæˆæ¨¡å‹ï¼Œåˆ©ç”¨ç¥ç»ç½‘ç»œçš„æ‹Ÿåˆèƒ½åŠ›æ¥æœ‰æ•ˆåœ°è§£å†³å«éšå˜é‡çš„æ¦‚ç‡æ¨¡å‹ä¸­åéªŒåˆ†å¸ƒéš¾ä»¥ä¼°è®¡çš„é—®é¢˜[Kingma et al.,2014;Rezende et al.,2014]ï¼å˜åˆ†è‡ªç¼–ç å™¨çš„è¯¦å°½ä»‹ç»å¯ä»¥å‚è€ƒæ–‡çŒ®[Doersch,2016]ï¼[Bowman et al.,2016]è¿›ä¸€æ­¥å°†å˜åˆ†è‡ªç¼–ç å™¨åº”ç”¨äºåºåˆ—ç”Ÿæˆé—®é¢˜ï¼å†å‚æ•°åŒ–æ˜¯å˜åˆ†è‡ªç¼–ç å™¨çš„é‡è¦æŠ€å·§ï¼å¯¹äºç¦»æ•£å˜é‡çš„å†å‚æ•°åŒ–ï¼Œå¯ä»¥ä½¿ç”¨Gumbel-Softmaxæ–¹æ³•[Jang et al.,2017][6]

[1]: https://learning.oreilly.com/library/view/hands-on-artificial-intelligence/9781788836067/de965259-e07e-461a-8d0f-717745273397.xhtml
[2]: https://learning.oreilly.com/library/view/advanced-deep-learning/9781788629416/ch08.html
[3]: https://github.com/AntixK/PyTorch-VAE
[4]: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73
[5]: https://keras.io/examples/generative/vae/
[6]: https://nndl.github.io/
[7]: Doersch C, 2016. Tutorial on variational autoencoders[J/OL]. CoRR, abs/1606.05908.http://arxiv.org/abs/1606.05908.
TODO:
[8]: https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book/blob/master/ch12-%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8/vae.py
