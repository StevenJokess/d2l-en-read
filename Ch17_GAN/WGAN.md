

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-24 21:54:28
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 20:00:53
 * @Description:
 * @TODO::
 * @Reference:
-->

# Wasserstein GAN (WGAN)

ä¸€ç¯‡æ–°é²œå‡ºç‚‰çš„arXivè®ºæ–‡Martin Arjovskyç­‰äººã€ŠWasserstein GANã€‹[8]å´åœ¨Redditçš„Machine Learningé¢‘é“ç«äº†[7]

\begin{array}{l}
\mathbb{E}_{x \sim P_{g}}[\log (1-D(x))] \quad(\text { å…¬å¼ } 2) \\
\mathbb{E}_{x \sim P_{g}}[-\log D(x)] \quad \text { (å…¬å¼3 })
\end{array}

åè€…åœ¨WGANä¸¤ç¯‡è®ºæ–‡ä¸­ç§°ä¸ºâ€œthe - log D alternativeâ€æˆ–â€œthe - log D trickâ€ã€‚WGANå‰ä½œåˆ†åˆ«åˆ†æäº†è¿™ä¸¤ç§å½¢å¼çš„åŸå§‹GANå„è‡ªçš„é—®é¢˜æ‰€åœ¨ï¼Œä¸‹é¢åˆ†åˆ«è¯´æ˜ã€‚


æ”¹è¿›DCGANä¾é çš„æ˜¯å¯¹åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æ¶æ„è¿›è¡Œå®éªŒæšä¸¾ï¼Œæœ€ç»ˆæ‰¾åˆ°ä¸€ç»„æ¯”è¾ƒå¥½çš„ç½‘ç»œæ¶æ„è®¾ç½®ï¼Œä½†æ˜¯å®é™…ä¸Šæ˜¯æ²»æ ‡ä¸æ²»æœ¬ï¼Œæ²¡æœ‰å½»åº•è§£å†³é—®é¢˜ã€‚

ä»Šå¤©çš„ä¸»è§’Wasserstein GANï¼ˆä¸‹é¢ç®€ç§°WGANï¼‰æˆåŠŸåœ°åšåˆ°äº†ä»¥ä¸‹çˆ†ç‚¸æ€§çš„å‡ ç‚¹ï¼š

- å½»åº•è§£å†³GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä¸å†éœ€è¦å°å¿ƒå¹³è¡¡ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„è®­ç»ƒç¨‹åº¦
- åŸºæœ¬è§£å†³äº†collapse modeçš„é—®é¢˜ï¼Œç¡®ä¿äº†ç”Ÿæˆæ ·æœ¬çš„å¤šæ ·æ€§
- è®­ç»ƒè¿‡ç¨‹ä¸­ç»ˆäºæœ‰ä¸€ä¸ªåƒäº¤å‰ç†µã€å‡†ç¡®ç‡è¿™æ ·çš„æ•°å€¼æ¥æŒ‡ç¤ºè®­ç»ƒçš„è¿›ç¨‹ï¼Œè¿™ä¸ªæ•°å€¼è¶Šå°ä»£è¡¨GANè®­ç»ƒå¾—è¶Šå¥½ï¼Œä»£è¡¨
- ç”Ÿæˆå™¨äº§ç”Ÿçš„å›¾åƒè´¨é‡è¶Šé«˜ï¼ˆå¦‚é¢˜å›¾æ‰€ç¤ºï¼‰


ä»¥ä¸Šä¸€åˆ‡å¥½å¤„ä¸éœ€è¦ç²¾å¿ƒè®¾è®¡çš„ç½‘ç»œæ¶æ„ï¼Œæœ€ç®€å•çš„å¤šå±‚å…¨è¿æ¥ç½‘ç»œå°±å¯ä»¥åšåˆ°

WGAN modified of DCGAN in:
1. remove sigmoid in the last layer of discriminator(classification -> regression)                                       # å›å½’é—®é¢˜,è€Œä¸æ˜¯äºŒåˆ†ç±»æ¦‚ç‡
2. no log Loss (Wasserstein distance)
3. clip param norm to c (Wasserstein distance and Lipschitz continuity) æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c
4. No momentum-based optimizer, use RMSPropï¼ŒSGD instead


æ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´äº† GAN è®­ç»ƒå¦‚æ­¤ä¸ç¨³å®šå‘¢ï¼ŸWGAN æå‡ºæ˜¯å› ä¸º JS æ•£åº¦åœ¨ä¸é‡å çš„åˆ† å¸ƒğ‘å’Œğ‘ä¸Šçš„æ¢¯åº¦æ›²é¢æ˜¯æ’å®šä¸º 0 çš„ã€‚å¦‚å›¾ 13.19 æ‰€ç¤ºï¼Œå½“åˆ†å¸ƒğ‘å’Œğ‘ä¸é‡å æ—¶ï¼ŒJS æ•£åº¦çš„æ¢¯ åº¦å€¼å§‹ç»ˆä¸º 0ï¼Œä»è€Œå¯¼è‡´æ­¤æ—¶ GAN çš„è®­ç»ƒå‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œå‚æ•°é•¿æ—¶é—´å¾—ä¸åˆ°æ›´æ–°ï¼Œç½‘ç»œæ— æ³•æ”¶æ•›ã€‚



## Wassersteinè·ç¦»[10]

Wassersteinè·ç¦»æ˜¯ä»æœ€ä¼˜è¿è¾“ç†è®ºä¸­çš„Kantoroviché—®é¢˜è¡ç”Ÿè€Œæ¥çš„ï¼Œå¯ä»¥å¦‚ä¸‹å®šä¹‰çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒçš„Wasserstein-1è·ç¦»ï¼š



æå‡ºWasserstein distanceè·ç¦»ä½œä¸ºè¡¡é‡ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºæ±‚è§£æœ€ä¼˜çš„åˆ©æ™®å¸ŒèŒ¨è¿ç»­å‡½æ•°çš„é—®é¢˜ï¼Œä¸ºæ­¤è¿›è¡Œå‚æ•°çº¦æŸï¼šå°†è¿‡å¤§çš„å‚æ•°ç›´æ¥è£å‰ªåˆ°ä¸€ä¸ªé˜ˆå€¼ä»¥ä¸‹ã€‚

å‡è®¾æˆ‘ä»¬æœ‰äº†ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒp(x),q(x)ï¼Œé‚£ä¹ˆWassersteinè·ç¦»çš„å®šä¹‰ä¸º

$\mathcal{W}[p, q]=\inf _{\gamma \in \Pi[p, q]} \iint \gamma(\boldsymbol{x}, \boldsymbol{y}) d(\boldsymbol{x}, \boldsymbol{y}) d \boldsymbol{x} d \boldsymbol{y}$

äº‹å®ä¸Šï¼Œè¿™ä¹Ÿç®—æ˜¯æœ€ä¼˜ä¼ è¾“ç†è®ºä¸­æœ€æ ¸å¿ƒçš„å®šä¹‰äº†ã€‚

d(x,y)ä¸ä¸€å®šæ˜¯è·ç¦»ï¼Œå…¶å‡†ç¡®å«ä¹‰åº”è¯¥æ˜¯ä¸€ä¸ªæˆæœ¬å‡½æ•°ï¼Œä»£è¡¨ç€ä»xè¿è¾“åˆ°yçš„æˆæœ¬ã€‚å¸¸ç”¨çš„dæ˜¯åŸºäºlèŒƒæ•°è¡ç”Ÿå‡ºæ¥çš„



## JS æ•£åº¦çš„ç¼ºé™·

Martin Arjovskyç­‰äººå…ˆé˜è¿°äº†æœ´ç´ GANå› ç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è€Œè®­ç»ƒå¤±è´¥çš„åŸå› [8]ï¼šä»–ä»¬è®¤ä¸ºï¼Œæœ´ç´ GANçš„ç›®æ ‡å‡½æ•°åœ¨æœ¬è´¨ä¸Šå¯ä»¥ç­‰ä»·äºä¼˜åŒ–çœŸå®åˆ†å¸ƒä¸ç”Ÿæˆåˆ†å¸ƒçš„Jensen-Shannonæ•£åº¦ã€‚è€Œæ ¹æ®Jensen-Shannonæ•£åº¦çš„ç‰¹æ€§ï¼Œå½“ä¸¤ä¸ªåˆ†å¸ƒé—´äº’ä¸é‡å æ—¶ï¼Œå…¶å€¼ä¼šè¶‹å‘äºä¸€ä¸ªå¸¸æ•°ï¼Œè¿™ä¹Ÿå°±æ˜¯æ¢¯åº¦æ¶ˆå¤±çš„åŸå› ã€‚


## EM è·ç¦»

Wassersteinè·ç¦»ä¹Ÿç§°ä¸ºEarth-Mover Distanc æ¨åœŸæœºè·ç¦»ï¼Œç®€ç§°EM è·ç¦»



å®ƒ è¡¨ç¤ºäº†ä»ä¸€ä¸ªåˆ†å¸ƒå˜æ¢åˆ°å¦ä¸€ä¸ªåˆ†å¸ƒçš„æœ€å°ä»£ä»·ï¼Œå®šä¹‰ä¸ºï¼š

$W(p, q)=\inf _{v \sim \prod(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]$



ç»˜åˆ¶å‡º JS æ•£åº¦å’Œ EM è·ç¦»çš„æ›²çº¿ï¼Œå¦‚å›¾ 13.20 æ‰€ç¤ºï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒJS æ•£åº¦åœ¨ğœƒ = 0å¤„ä¸è¿ ç»­ï¼Œå…¶ä»–ä½ç½®å¯¼æ•°å‡ä¸º 0ï¼Œè€Œ EM è·ç¦»æ€»èƒ½å¤Ÿäº§ç”Ÿæœ‰æ•ˆçš„å¯¼æ•°ä¿¡æ¯ï¼Œå› æ­¤ EM è·ç¦»ç›¸å¯¹äº JS æ•£åº¦æ›´é€‚åˆæŒ‡å¯¼ GAN ç½‘ç»œçš„è®­ç»ƒã€‚

ğ‘Šğ‘

![JS æ•£åº¦å’Œ EM è·ç¦»éšğœƒå˜æ¢æ›²çº¿](img\JS_EM.jpg)

WGAN, which uses a modified loss function based on the so-called Wasserstein-1 (or earth mover's) distance between the distributions of real and fake images for improving the training performance.[1]

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\inf _{\gamma \sim \Gamma\left(p_{r}, p_{\theta}\right)} \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \gamma}[\|\boldsymbol{x}-\boldsymbol{y}\|]$

å…¶ä¸­Î“(ğ‘ğ‘Ÿ,ğ‘ğœƒ)æ˜¯è¾¹é™…åˆ†å¸ƒä¸ºğ‘ğ‘Ÿå’Œğ‘ğœƒçš„æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒé›†åˆ

æ¢¯åº¦æˆªæ–­?

å’ŒåŸå§‹GANç›¸æ¯”ï¼ŒW-GANçš„è¯„ä»·ç½‘ç»œæœ€åä¸€å±‚ä¸ä½¿ç”¨Sigmoidå‡½æ•°ï¼ŒæŸå¤±å‡½æ•°ä¸å–å¯¹æ•°

å½“ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„KLæ•£åº¦ä¸º+âˆï¼ŒJSæ•£åº¦ä¸ºlog2ï¼Œå¹¶ä¸éšç€ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»è€Œå˜åŒ–ï¼è€Œ1st-Wassersteinè·ç¦»ä¾ç„¶å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ²¡æœ‰é‡å åˆ†å¸ƒä¹‹é—´çš„è·ç¦»

ä¸¤ä¸ªåˆ†å¸ƒğ‘ğ‘Ÿå’Œğ‘ğœƒçš„1st-Wassersteinè·ç¦»é€šå¸¸éš¾ä»¥ç›´æ¥è®¡ç®—ï¼Œä½†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„1st-Wassersteinè·ç¦»æœ‰ä¸€ä¸ªå¯¹å¶å½¢å¼

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\sup _{\|f\|_{L} \leq 1}\left(\mathbb{E}_{\boldsymbol{x} \sim p_{r}}[f(\boldsymbol{x})]-\mathbb{E}_{\boldsymbol{x} \sim p_{\theta}}[f(\boldsymbol{x})]\right),$

we talked about the WGAN with GP to maintain the 1-Lipschitz property instead of clipping the weights.[1]


As we've mentioned before, GANs are notoriously hard to train. The opposing objectives of the two networks, the discriminator and the generator, can easily cause training instability. The discriminator attempts to correctly classify the fake data from the real data. Meanwhile, the generator tries its best to trick the discriminator. If the discriminator learns faster than the generator, the generator parameters will fail to optimize. On the other hand, if the discriminator learns more slowly, then the gradients may vanish before reaching the generator. In the worst case, if the discriminator is unable to converge, the generator is not going to be able to get any useful feedback.[2]
...

## åˆ¤åˆ«å™¨

ç„¶è€Œï¼Œè¦è®¡ç®—å…¬å¼(13.54)ä¸­çš„ä¸Šç•Œä¹Ÿå¹¶ä¸å®¹æ˜“ï¼æ ¹æ®ç¥ç»ç½‘ç»œçš„é€šç”¨è¿‘ä¼¼å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä½¿å¾—å¯ä»¥è¾¾åˆ°è¿™ä¸ªä¸Šç•Œï¼ä»¤ğ‘“(ğ’™;ğœ™)ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå‡è®¾å­˜åœ¨å‚æ•°é›†åˆÎ¦ï¼Œå¯¹äºæ‰€æœ‰çš„ğœ™ âˆˆ Î¦ï¼Œğ‘“(ğ’™;ğœ™)ä¸ºK-Lipschitzè¿ç»­å‡½æ•°ï¼Œé‚£ä¹ˆå…¬å¼ï¼ˆ13.54ï¼‰ä¸­çš„ä¸Šç•Œå¯ä»¥è¿‘ä¼¼è½¬æ¢ä¸º

$\max _{\phi \in \Phi}\left(\mathbb{E}_{x \sim p_{r}}[f(\boldsymbol{x} ; \phi)]-\mathbb{E}_{x \sim p_{\theta}}[f(\boldsymbol{x} ; \phi)]\right)$




çš„åå¯¼æ•°çš„æ¨¡$\left\|\frac{\partial f(x ; \phi)}{\partial x}\right\|$å°äºæŸä¸ªä¸Šç•Œï¼ç”±äºè¿™ä¸ªåå¯¼æ•°çš„å¤§å°ä¸€èˆ¬å’Œå‚æ•°çš„å–å€¼èŒƒå›´ç›¸å…³ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶å‚æ•°ğœ™çš„å–å€¼èŒƒå›´æ¥è¿‘ä¼¼ï¼Œä»¤ğœ™ âˆˆ [âˆ’ğ‘,ğ‘]ï¼Œğ‘ä¸ºä¸€ä¸ªæ¯”è¾ƒå°çš„æ­£æ•°ï¼Œæ¯”å¦‚0.01

WGAN åˆ¤åˆ«å™¨çš„æŸå¤±å‡½æ•°è®¡ç®—ä¸ GAN ä¸ä¸€æ ·ï¼ŒWGAN æ˜¯ç›´æ¥æœ€å¤§åŒ–çœŸå®æ ·æœ¬çš„è¾“å‡º å€¼ï¼Œæœ€å°åŒ–ç”Ÿæˆæ ·æœ¬çš„è¾“å‡ºå€¼ï¼Œå¹¶æ²¡æœ‰äº¤å‰ç†µè®¡ç®—çš„è¿‡ç¨‹ã€‚

WGANä¸åŸå§‹GANç¬¬ä¸€ç§å½¢å¼ç›¸æ¯”ï¼Œåªæ”¹äº†å››ç‚¹ï¼š
1. åˆ¤åˆ«å™¨æœ€åä¸€å±‚å»æ‰sigmoid             #no sigmoid!            #nn.Sigmoid(),[9]
1. ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨çš„lossä¸å–log
1. æ¯æ¬¡æ›´æ–°åˆ¤åˆ«å™¨çš„å‚æ•°ä¹‹åæŠŠå®ƒä»¬çš„ç»å¯¹å€¼æˆªæ–­åˆ°ä¸è¶…è¿‡ä¸€ä¸ªå›ºå®šå¸¸æ•°c
1. ä¸è¦ç”¨åŸºäºåŠ¨é‡çš„ä¼˜åŒ–ç®—æ³•ï¼ˆåŒ…æ‹¬momentumå’ŒAdamï¼‰ï¼Œæ¨èRMSPropï¼ŒSGDä¹Ÿè¡Œ

```
Discriminator(
  (main): Sequential(
    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (1): LeakyReLU(negative_slope=0.2, inplace)
    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): LeakyReLU(negative_slope=0.2, inplace)
    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (7): LeakyReLU(negative_slope=0.2, inplace)
    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (10): LeakyReLU(negative_slope=0.2, inplace)
    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)
  )
)
```

## ç”Ÿæˆå™¨

ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡æ˜¯ä½¿å¾—è¯„ä»·ç½‘ç»œğ‘“(ğ’™;ğœ™)å¯¹å…¶ç”Ÿæˆæ ·æœ¬çš„æ‰“åˆ†å°½å¯èƒ½é«˜ï¼Œå³$\max _{\theta} \mathbb{E}_{z \sim p(z)}[f(G(z ; \theta) ; \phi)]$

```
Generator(
  (main): Sequential(
    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)
    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): ReLU(inplace)
    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (5): ReLU(inplace)
    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): ReLU(inplace)
    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (11): ReLU(inplace)
    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
    (13): Tanh()
  )
)
```

å› ä¸ºğ‘“(ğ’™;ğœ™)ä¸ºä¸é¥±å’Œå‡½æ•°ï¼Œæ‰€ä»¥ç”Ÿæˆç½‘ç»œå‚æ•°ğœƒçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œç†è®ºä¸Šè§£å†³äº†åŸå§‹GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼å¹¶ä¸”W-GANä¸­ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡å‡½æ•°ä¸å†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„æ¯”ç‡ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†æ¨¡å‹åå¡Œé—®é¢˜ï¼Œä½¿å¾—ç”Ÿæˆçš„æ ·æœ¬å…·æœ‰å¤šæ ·æ€§

## æŸå¤±å‡½æ•°[9]

ç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°ä¸º

$\min _{G}-E_{z \sim P_{z}}\left[f_{w}(G(z))\right]$

#don't use BCE loss!
#criterion = nn.BCELoss()

#now use RMSprop instead of Adam, with lr of 0.00005
G_optimizer = optim.RMSprop(G.parameters(), lr=0.00005)
D_optimizer = optim.RMSprop(D.parameters(), lr=0.00005)
## ä¼˜åŒ–å™¨

åœ¨è¯¯å·®å‡½æ•°è®¡ç®—æ—¶ï¼ŒWGAN ä¹Ÿæ²¡ æœ‰ log å‡½æ•°å­˜åœ¨ã€‚åœ¨è®­ç»ƒ WGAN æ—¶ï¼ŒWGAN ä½œè€…æ¨èä½¿ç”¨ RMSProp æˆ– SGD ç­‰ä¸å¸¦åŠ¨é‡ çš„ä¼˜åŒ–å™¨ã€‚




WGAN è¿˜åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†æ¨¡ å¼å´©å¡Œçš„é—®é¢˜ï¼Œä½¿ç”¨ WGAN çš„æ¨¡å‹ä¸å®¹æ˜“å‡ºç°æ¨¡å¼å´©å¡Œçš„ç°è±¡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒWGAN ä¸€èˆ¬å¹¶ä¸èƒ½æå‡æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœï¼Œä»…ä»…æ˜¯ä¿è¯äº†æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å½“ç„¶ï¼Œä¿è¯æ¨¡å‹èƒ½å¤Ÿ ç¨³å®šåœ°è®­ç»ƒä¹Ÿæ˜¯å–å¾—è‰¯å¥½æ•ˆæœçš„å‰æã€‚å¦‚å›¾ 13.21 æ‰€ç¤ºï¼ŒåŸå§‹ç‰ˆæœ¬çš„ DCGAN åœ¨ä¸ä½¿ç”¨ BN å±‚ç­‰è®¾å®šæ—¶å‡ºç°äº†è®­ç»ƒä¸ç¨³å®šçš„ç°è±¡ï¼Œåœ¨åŒæ ·è®¾å®šä¸‹ï¼Œä½¿ç”¨ WGAN æ¥è®­ç»ƒåˆ¤åˆ«å™¨å¯ä»¥ é¿å…æ­¤ç°è±¡

WGANæœ¬ä½œå¼•å…¥äº†Wassersteinè·ç¦»ï¼Œç”±äºå®ƒç›¸å¯¹KLæ•£åº¦ä¸JSæ•£åº¦å…·æœ‰ä¼˜è¶Šçš„å¹³æ»‘ç‰¹æ€§ï¼Œç†è®ºä¸Šå¯ä»¥è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ¥ç€é€šè¿‡æ•°å­¦å˜æ¢å°†Wassersteinè·ç¦»å†™æˆå¯æ±‚è§£çš„å½¢å¼ï¼Œåˆ©ç”¨ä¸€ä¸ªå‚æ•°æ•°å€¼èŒƒå›´å—é™çš„åˆ¤åˆ«å™¨ç¥ç»ç½‘ç»œæ¥æœ€å¤§åŒ–è¿™ä¸ªå½¢å¼ï¼Œå°±å¯ä»¥è¿‘ä¼¼Wassersteinè·ç¦»ã€‚åœ¨æ­¤è¿‘ä¼¼æœ€ä¼˜åˆ¤åˆ«å™¨ä¸‹ä¼˜åŒ–ç”Ÿæˆå™¨ä½¿å¾—Wassersteinè·ç¦»ç¼©å°ï¼Œå°±èƒ½æœ‰æ•ˆæ‹‰è¿‘ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒã€‚WGANæ—¢è§£å†³äº†è®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼Œä¹Ÿæä¾›äº†ä¸€ä¸ªå¯é çš„è®­ç»ƒè¿›ç¨‹æŒ‡æ ‡ï¼Œè€Œä¸”è¯¥æŒ‡æ ‡ç¡®å®ä¸ç”Ÿæˆæ ·æœ¬çš„è´¨é‡é«˜åº¦ç›¸å…³ã€‚ä½œè€…å¯¹WGANè¿›è¡Œäº†å®éªŒéªŒè¯ã€‚[6]

WGANçš„è´¡çŒ®åœ¨äºï¼Œä»ç†è®ºä¸Šé˜è¿°äº†å› ç”Ÿæˆå™¨æ¢¯åº¦æ¶ˆå¤±è€Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šçš„åŸå› ï¼Œå¹¶ç”¨Wassersteinè·ç¦»æ›¿ä»£äº†Jensen-Shannonæ•£åº¦ï¼Œåœ¨ç†è®ºä¸Šè§£å†³äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ­¤å¤–ï¼ŒWGANè¿˜ä»ç†è®ºä¸Šç»™å‡ºäº†æœ´ç´ GANå‘ç”Ÿæ¨¡å¼åå¡Œ(mode collapse)çš„åŸå› ï¼Œå¹¶ä»å®éªŒè§’åº¦è¯´æ˜äº†WGANåœ¨è¿™ä¸€ç‚¹ä¸Šçš„ä¼˜è¶Šæ€§ã€‚æœ€åï¼Œé’ˆå¯¹ç”Ÿæˆåˆ†å¸ƒä¸çœŸå®åˆ†å¸ƒçš„è·ç¦»å’Œç›¸å…³ç†è®ºä»¥åŠä»Wassersteinè·ç¦»æ¨å¯¼è€Œå‡ºçš„Lipschitzçº¦æŸï¼Œä¹Ÿç»™äº†åæ¥è€…æ›´æ·±å±‚æ¬¡çš„å¯å‘ï¼Œå¦‚åŸºäºLipschitzå¯†åº¦çš„ æŸå¤±æ•æ„ŸGAN(loss sensitive GAN, LS-GAN)ã€‚[11]


[1]: https://learning.oreilly.com/library/view/python-machine-learning/9781789955750/Text/Chapter_17.xhtml#_idParaDest-342
[2]: https://learning.oreilly.com/library/view/advanced-deep-learning/9781788629416/ch05.html
[3]: https://nndl.github.io/ 13.3
[4]: Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein GAN[J/OL]. CoRR, abs/1701.07875.http://arxiv.org/abs/1701.07875.
[5]: https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book/blob/master/%E3%80%90%E3%80%8ATensorFlow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E3%80%91.pdf 13.7
[6]: https://zhuanlan.zhihu.com/p/25071913
[7]: https://github.com/chenyuntc/pytorch-GAN/blob/master/WGAN.ipynb
[8]: https://arxiv.org/abs/1701.07875
[9]: https://github.com/bentrevett/pytorch-generative-models/blob/master/4%20-%20WGAN.ipynb
[10]: https://kexue.fm/archives/6280
[11]: http://www.tensorinfinity.com/paper_26.html
