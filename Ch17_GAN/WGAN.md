

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-24 21:54:28
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-17 16:53:20
 * @Description:
 * @TODO::
 * @Reference:
-->

# Wasserstein GAN (WGAN)



æ˜¯ä»€ä¹ˆåŸå› å¯¼è‡´äº† GAN è®­ç»ƒå¦‚æ­¤ä¸ç¨³å®šå‘¢ï¼ŸWGAN æå‡ºæ˜¯å› ä¸º JS æ•£åº¦åœ¨ä¸é‡å çš„åˆ† å¸ƒğ‘å’Œğ‘ä¸Šçš„æ¢¯åº¦æ›²é¢æ˜¯æ’å®šä¸º 0 çš„ã€‚å¦‚å›¾ 13.19 æ‰€ç¤ºï¼Œå½“åˆ†å¸ƒğ‘å’Œğ‘ä¸é‡å æ—¶ï¼ŒJS æ•£åº¦çš„æ¢¯ åº¦å€¼å§‹ç»ˆä¸º 0ï¼Œä»è€Œå¯¼è‡´æ­¤æ—¶ GAN çš„è®­ç»ƒå‡ºç°æ¢¯åº¦å¼¥æ•£ç°è±¡ï¼Œå‚æ•°é•¿æ—¶é—´å¾—ä¸åˆ°æ›´æ–°ï¼Œç½‘ç»œæ— æ³•æ”¶æ•›ã€‚

## JS æ•£åº¦çš„ç¼ºé™·



## EM è·ç¦»

Wassersteinè·ç¦»ä¹Ÿç§°ä¸ºEarth-Mover Distanc æ¨åœŸæœºè·ç¦»ï¼Œç®€ç§°EM è·ç¦»



å®ƒ è¡¨ç¤ºäº†ä»ä¸€ä¸ªåˆ†å¸ƒå˜æ¢åˆ°å¦ä¸€ä¸ªåˆ†å¸ƒçš„æœ€å°ä»£ä»·ï¼Œå®šä¹‰ä¸ºï¼š

$W(p, q)=\inf _{v \sim \prod(p, q)} \mathbb{E}_{(x, y) \sim \gamma}[\|x-y\|]$



ç»˜åˆ¶å‡º JS æ•£åº¦å’Œ EM è·ç¦»çš„æ›²çº¿ï¼Œå¦‚å›¾ 13.20 æ‰€ç¤ºï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒJS æ•£åº¦åœ¨ğœƒ = 0å¤„ä¸è¿ ç»­ï¼Œå…¶ä»–ä½ç½®å¯¼æ•°å‡ä¸º 0ï¼Œè€Œ EM è·ç¦»æ€»èƒ½å¤Ÿäº§ç”Ÿæœ‰æ•ˆçš„å¯¼æ•°ä¿¡æ¯ï¼Œå› æ­¤ EM è·ç¦»ç›¸å¯¹äº JS æ•£åº¦æ›´é€‚åˆæŒ‡å¯¼ GAN ç½‘ç»œçš„è®­ç»ƒã€‚

ğ‘Šğ‘

![JS æ•£åº¦å’Œ EM è·ç¦»éšğœƒå˜æ¢æ›²çº¿](img\JS_EM.jpg)

WGAN, which uses a modified loss function based on the so-called Wasserstein-1 (or earth mover's) distance between the distributions of real and fake images for improving the training performance.[1]

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\inf _{\gamma \sim \Gamma\left(p_{r}, p_{\theta}\right)} \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \gamma}[\|\boldsymbol{x}-\boldsymbol{y}\|]$

å…¶ä¸­Î“(ğ‘ğ‘Ÿ,ğ‘ğœƒ)æ˜¯è¾¹é™…åˆ†å¸ƒä¸ºğ‘ğ‘Ÿå’Œğ‘ğœƒçš„æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒé›†åˆ

æ¢¯åº¦æˆªæ–­?

å’ŒåŸå§‹GANç›¸æ¯”ï¼ŒW-GANçš„è¯„ä»·ç½‘ç»œæœ€åä¸€å±‚ä¸ä½¿ç”¨Sigmoidå‡½æ•°ï¼ŒæŸå¤±å‡½æ•°ä¸å–å¯¹æ•°

å½“ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„KLæ•£åº¦ä¸º+âˆï¼ŒJSæ•£åº¦ä¸ºlog2ï¼Œå¹¶ä¸éšç€ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»è€Œå˜åŒ–ï¼è€Œ1st-Wassersteinè·ç¦»ä¾ç„¶å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ²¡æœ‰é‡å åˆ†å¸ƒä¹‹é—´çš„è·ç¦»

ä¸¤ä¸ªåˆ†å¸ƒğ‘ğ‘Ÿå’Œğ‘ğœƒçš„1st-Wassersteinè·ç¦»é€šå¸¸éš¾ä»¥ç›´æ¥è®¡ç®—ï¼Œä½†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„1st-Wassersteinè·ç¦»æœ‰ä¸€ä¸ªå¯¹å¶å½¢å¼

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\sup _{\|f\|_{L} \leq 1}\left(\mathbb{E}_{\boldsymbol{x} \sim p_{r}}[f(\boldsymbol{x})]-\mathbb{E}_{\boldsymbol{x} \sim p_{\theta}}[f(\boldsymbol{x})]\right),$

we talked about the WGAN with GP to maintain the 1-Lipschitz property instead of clipping the weights.[1]


As we've mentioned before, GANs are notoriously hard to train. The opposing objectives of the two networks, the discriminator and the generator, can easily cause training instability. The discriminator attempts to correctly classify the fake data from the real data. Meanwhile, the generator tries its best to trick the discriminator. If the discriminator learns faster than the generator, the generator parameters will fail to optimize. On the other hand, if the discriminator learns more slowly, then the gradients may vanish before reaching the generator. In the worst case, if the discriminator is unable to converge, the generator is not going to be able to get any useful feedback.[2]
...

## åˆ¤åˆ«å™¨

ç„¶è€Œï¼Œè¦è®¡ç®—å…¬å¼(13.54)ä¸­çš„ä¸Šç•Œä¹Ÿå¹¶ä¸å®¹æ˜“ï¼æ ¹æ®ç¥ç»ç½‘ç»œçš„é€šç”¨è¿‘ä¼¼å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä½¿å¾—å¯ä»¥è¾¾åˆ°è¿™ä¸ªä¸Šç•Œï¼ä»¤ğ‘“(ğ’™;ğœ™)ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå‡è®¾å­˜åœ¨å‚æ•°é›†åˆÎ¦ï¼Œå¯¹äºæ‰€æœ‰çš„ğœ™ âˆˆ Î¦ï¼Œğ‘“(ğ’™;ğœ™)ä¸ºK-Lipschitzè¿ç»­å‡½æ•°ï¼Œé‚£ä¹ˆå…¬å¼ï¼ˆ13.54ï¼‰ä¸­çš„ä¸Šç•Œå¯ä»¥è¿‘ä¼¼è½¬æ¢ä¸º

$\max _{\phi \in \Phi}\left(\mathbb{E}_{x \sim p_{r}}[f(\boldsymbol{x} ; \phi)]-\mathbb{E}_{x \sim p_{\theta}}[f(\boldsymbol{x} ; \phi)]\right)$




çš„åå¯¼æ•°çš„æ¨¡$\left\|\frac{\partial f(x ; \phi)}{\partial x}\right\|$å°äºæŸä¸ªä¸Šç•Œï¼ç”±äºè¿™ä¸ªåå¯¼æ•°çš„å¤§å°ä¸€èˆ¬å’Œå‚æ•°çš„å–å€¼èŒƒå›´ç›¸å…³ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶å‚æ•°ğœ™çš„å–å€¼èŒƒå›´æ¥è¿‘ä¼¼ï¼Œä»¤ğœ™ âˆˆ [âˆ’ğ‘,ğ‘]ï¼Œğ‘ä¸ºä¸€ä¸ªæ¯”è¾ƒå°çš„æ­£æ•°ï¼Œæ¯”å¦‚0.01

WGAN åˆ¤åˆ«å™¨çš„æŸå¤±å‡½æ•°è®¡ç®—ä¸ GAN ä¸ä¸€æ ·ï¼ŒWGAN æ˜¯ç›´æ¥æœ€å¤§åŒ–çœŸå®æ ·æœ¬çš„è¾“å‡º å€¼ï¼Œæœ€å°åŒ–ç”Ÿæˆæ ·æœ¬çš„è¾“å‡ºå€¼ï¼Œå¹¶æ²¡æœ‰äº¤å‰ç†µè®¡ç®—çš„è¿‡ç¨‹ã€‚




## ç”Ÿæˆå™¨

ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡æ˜¯ä½¿å¾—è¯„ä»·ç½‘ç»œğ‘“(ğ’™;ğœ™)å¯¹å…¶ç”Ÿæˆæ ·æœ¬çš„æ‰“åˆ†å°½å¯èƒ½é«˜ï¼Œå³$\max _{\theta} \mathbb{E}_{z \sim p(z)}[f(G(z ; \theta) ; \phi)]$


å› ä¸ºğ‘“(ğ’™;ğœ™)ä¸ºä¸é¥±å’Œå‡½æ•°ï¼Œæ‰€ä»¥ç”Ÿæˆç½‘ç»œå‚æ•°ğœƒçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œç†è®ºä¸Šè§£å†³äº†åŸå§‹GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼å¹¶ä¸”W-GANä¸­ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡å‡½æ•°ä¸å†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„æ¯”ç‡ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†æ¨¡å‹åå¡Œé—®é¢˜ï¼Œä½¿å¾—ç”Ÿæˆçš„æ ·æœ¬å…·æœ‰å¤šæ ·æ€§

## æŸå¤±å‡½æ•°

## ä¼˜åŒ–å™¨

åœ¨è¯¯å·®å‡½æ•°è®¡ç®—æ—¶ï¼ŒWGAN ä¹Ÿæ²¡ æœ‰ log å‡½æ•°å­˜åœ¨ã€‚åœ¨è®­ç»ƒ WGAN æ—¶ï¼ŒWGAN ä½œè€…æ¨èä½¿ç”¨ RMSProp æˆ– SGD ç­‰ä¸å¸¦åŠ¨é‡ çš„ä¼˜åŒ–å™¨ã€‚



WGAN è¿˜åœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†æ¨¡ å¼å´©å¡Œçš„é—®é¢˜ï¼Œä½¿ç”¨ WGAN çš„æ¨¡å‹ä¸å®¹æ˜“å‡ºç°æ¨¡å¼å´©å¡Œçš„ç°è±¡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒWGAN ä¸€èˆ¬å¹¶ä¸èƒ½æå‡æ¨¡å‹çš„ç”Ÿæˆæ•ˆæœï¼Œä»…ä»…æ˜¯ä¿è¯äº†æ¨¡å‹è®­ç»ƒçš„ç¨³å®šæ€§ã€‚å½“ç„¶ï¼Œä¿è¯æ¨¡å‹èƒ½å¤Ÿ ç¨³å®šåœ°è®­ç»ƒä¹Ÿæ˜¯å–å¾—è‰¯å¥½æ•ˆæœçš„å‰æã€‚å¦‚å›¾ 13.21 æ‰€ç¤ºï¼ŒåŸå§‹ç‰ˆæœ¬çš„ DCGAN åœ¨ä¸ä½¿ç”¨ BN å±‚ç­‰è®¾å®šæ—¶å‡ºç°äº†è®­ç»ƒä¸ç¨³å®šçš„ç°è±¡ï¼Œåœ¨åŒæ ·è®¾å®šä¸‹ï¼Œä½¿ç”¨ WGAN æ¥è®­ç»ƒåˆ¤åˆ«å™¨å¯ä»¥ é¿å…æ­¤ç°è±¡





[1]: https://learning.oreilly.com/library/view/python-machine-learning/9781789955750/Text/Chapter_17.xhtml#_idParaDest-342
[2]: https://learning.oreilly.com/library/view/advanced-deep-learning/9781788629416/ch05.html
[3]: https://nndl.github.io/ 13.3
[4]: Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein GAN[J/OL]. CoRR, abs/1701.07875.http://arxiv.org/abs/1701.07875.
[5]: https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book/blob/master/%E3%80%90%E3%80%8ATensorFlow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E3%80%91.pdf 13.7
