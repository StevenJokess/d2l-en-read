

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-24 21:54:28
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-14 22:22:32
 * @Description:
 * @TODO::
 * @Reference:
-->

# Wasserstein GAN (WGAN)

Wassersteinè·ç¦»ä¹Ÿç§°ä¸ºæ¨åœŸæœºè·ç¦»



WGAN, which uses a modified loss function based on the so-called Wasserstein-1 (or earth mover's) distance between the distributions of real and fake images for improving the training performance.[1]

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\inf _{\gamma \sim \Gamma\left(p_{r}, p_{\theta}\right)} \mathbb{E}_{(\boldsymbol{x}, \boldsymbol{y}) \sim \gamma}[\|\boldsymbol{x}-\boldsymbol{y}\|]$

å…¶ä¸­Î“(ğ‘ğ‘Ÿ,ğ‘ğœƒ)æ˜¯è¾¹é™…åˆ†å¸ƒä¸ºğ‘ğ‘Ÿå’Œğ‘ğœƒçš„æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒé›†åˆ

æ¢¯åº¦æˆªæ–­?

å’ŒåŸå§‹GANç›¸æ¯”ï¼ŒW-GANçš„è¯„ä»·ç½‘ç»œæœ€åä¸€å±‚ä¸ä½¿ç”¨Sigmoidå‡½æ•°ï¼ŒæŸå¤±å‡½æ•°ä¸å–å¯¹æ•°

å½“ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„KLæ•£åº¦ä¸º+âˆï¼ŒJSæ•£åº¦ä¸ºlog2ï¼Œå¹¶ä¸éšç€ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»è€Œå˜åŒ–ï¼è€Œ1st-Wassersteinè·ç¦»ä¾ç„¶å¯ä»¥è¡¡é‡ä¸¤ä¸ªæ²¡æœ‰é‡å åˆ†å¸ƒä¹‹é—´çš„è·ç¦»

ä¸¤ä¸ªåˆ†å¸ƒğ‘ğ‘Ÿå’Œğ‘ğœƒçš„1st-Wassersteinè·ç¦»é€šå¸¸éš¾ä»¥ç›´æ¥è®¡ç®—ï¼Œä½†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„1st-Wassersteinè·ç¦»æœ‰ä¸€ä¸ªå¯¹å¶å½¢å¼

$\boldsymbol{W}^{1}\left(p_{\boldsymbol{r}}, p_{\theta}\right)=\sup _{\|f\|_{L} \leq 1}\left(\mathbb{E}_{\boldsymbol{x} \sim p_{r}}[f(\boldsymbol{x})]-\mathbb{E}_{\boldsymbol{x} \sim p_{\theta}}[f(\boldsymbol{x})]\right),$

we talked about the WGAN with GP to maintain the 1-Lipschitz property instead of clipping the weights.[1]


As we've mentioned before, GANs are notoriously hard to train. The opposing objectives of the two networks, the discriminator and the generator, can easily cause training instability. The discriminator attempts to correctly classify the fake data from the real data. Meanwhile, the generator tries its best to trick the discriminator. If the discriminator learns faster than the generator, the generator parameters will fail to optimize. On the other hand, if the discriminator learns more slowly, then the gradients may vanish before reaching the generator. In the worst case, if the discriminator is unable to converge, the generator is not going to be able to get any useful feedback.[2]
...

## è¯„ä»·ç½‘ç»œ

ç„¶è€Œï¼Œè¦è®¡ç®—å…¬å¼(13.54)ä¸­çš„ä¸Šç•Œä¹Ÿå¹¶ä¸å®¹æ˜“ï¼æ ¹æ®ç¥ç»ç½‘ç»œçš„é€šç”¨è¿‘ä¼¼å®šç†ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾å­˜åœ¨ä¸€ä¸ªç¥ç»ç½‘ç»œä½¿å¾—å¯ä»¥è¾¾åˆ°è¿™ä¸ªä¸Šç•Œï¼ä»¤ğ‘“(ğ’™;ğœ™)ä¸ºä¸€ä¸ªç¥ç»ç½‘ç»œï¼Œå‡è®¾å­˜åœ¨å‚æ•°é›†åˆÎ¦ï¼Œå¯¹äºæ‰€æœ‰çš„ğœ™ âˆˆ Î¦ï¼Œğ‘“(ğ’™;ğœ™)ä¸ºK-Lipschitzè¿ç»­å‡½æ•°ï¼Œé‚£ä¹ˆå…¬å¼ï¼ˆ13.54ï¼‰ä¸­çš„ä¸Šç•Œå¯ä»¥è¿‘ä¼¼è½¬æ¢ä¸º

$\max _{\phi \in \Phi}\left(\mathbb{E}_{x \sim p_{r}}[f(\boldsymbol{x} ; \phi)]-\mathbb{E}_{x \sim p_{\theta}}[f(\boldsymbol{x} ; \phi)]\right)$




çš„åå¯¼æ•°çš„æ¨¡$\left\|\frac{\partial f(x ; \phi)}{\partial x}\right\|$å°äºæŸä¸ªä¸Šç•Œï¼ç”±äºè¿™ä¸ªåå¯¼æ•°çš„å¤§å°ä¸€èˆ¬å’Œå‚æ•°çš„å–å€¼èŒƒå›´ç›¸å…³ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é™åˆ¶å‚æ•°ğœ™çš„å–å€¼èŒƒå›´æ¥è¿‘ä¼¼ï¼Œä»¤ğœ™ âˆˆ [âˆ’ğ‘,ğ‘]ï¼Œğ‘ä¸ºä¸€ä¸ªæ¯”è¾ƒå°çš„æ­£æ•°ï¼Œæ¯”å¦‚0.01

## ç”Ÿæˆç½‘ç»œ

ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡æ˜¯ä½¿å¾—è¯„ä»·ç½‘ç»œğ‘“(ğ’™;ğœ™)å¯¹å…¶ç”Ÿæˆæ ·æœ¬çš„æ‰“åˆ†å°½å¯èƒ½é«˜ï¼Œå³$\max _{\theta} \mathbb{E}_{z \sim p(z)}[f(G(z ; \theta) ; \phi)]$
å› ä¸ºğ‘“(ğ’™;ğœ™)ä¸ºä¸é¥±å’Œå‡½æ•°ï¼Œæ‰€ä»¥ç”Ÿæˆç½‘ç»œå‚æ•°ğœƒçš„æ¢¯åº¦ä¸ä¼šæ¶ˆå¤±ï¼Œç†è®ºä¸Šè§£å†³äº†åŸå§‹GANè®­ç»ƒä¸ç¨³å®šçš„é—®é¢˜ï¼å¹¶ä¸”W-GANä¸­ç”Ÿæˆç½‘ç»œçš„ç›®æ ‡å‡½æ•°ä¸å†æ˜¯ä¸¤ä¸ªåˆ†å¸ƒçš„æ¯”ç‡ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šç¼“è§£äº†æ¨¡å‹åå¡Œé—®é¢˜ï¼Œä½¿å¾—ç”Ÿæˆçš„æ ·æœ¬å…·æœ‰å¤šæ ·æ€§


[1]: https://learning.oreilly.com/library/view/python-machine-learning/9781789955750/Text/Chapter_17.xhtml#_idParaDest-342
[2]: https://learning.oreilly.com/library/view/advanced-deep-learning/9781788629416/ch05.html
[3]:https://nndl.github.io/ 13.3
[4]: Arjovsky M, Chintala S, Bottou L, 2017. Wasserstein GAN[J/OL]. CoRR, abs/1701.07875.http://arxiv.org/abs/1701.07875.
