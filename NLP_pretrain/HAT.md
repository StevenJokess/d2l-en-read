

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-12-03 19:30:34
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-03 19:30:46
 * @Description:
 * @TODO::
 * @Reference:
-->
为了获得更高效和快速的 Transformer 模型，MIT 等机构的研究者提出了 HAT: Hardware-Aware Transformers，借助神经网络搜索（NAS）技术，在搜索过程中加入硬件反馈，来对每一个硬件平台设计一个专用的高效 Transformer 网络结构。

https://mbd.baidu.com/newspage/data/landingshare?pageType=1&isBdboxFrom=1&context=%7B%22nid%22%3A%22news_10309232866955380751%22%2C%22sourceFrom%22%3A%22bjh%22%7D
