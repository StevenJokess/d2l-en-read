

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-11-17 19:54:54
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-11-17 20:21:34
 * @Description:
 * @TODO::
 * @Reference:https://0809zheng.github.io/2020/04/27/elmo-bert-gpt.html
-->
Generative Pre-Training (GPT)的结构是

Transformer的Decoder：


paper：Improving Language Understanding by Generative Pre-Training
arXiv：NLPIR
