

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-29 20:17:24
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-29 20:21:01
 * @Description:translate by machine half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/language-models-and-dataset.html
-->

# 语言模型和数据集

在8.2节中，我们将看到如何将文本数据映射到令牌中，这些令牌可以被视为离散观察的时间序列。假设长度为TT的文本中的令牌依次为x1,x2，…，xTx1,x2，…，xT，则在离散时间序列中，xtxt(1≤t≤T1≤t≤t)可视为时间步长TT的输出或标签。给定这样一个序列，语言模型的目标就是估计概率

(8.3.1)

p (x1, x2,…, xT)。

语言模型非常有用。例如，理想的语言模型应该能够自己生成自然文本，只需一次画一个单词，wt∼p(wt∣wt−1，…，w1)。与使用打字机的猴子完全不同的是，从这种模型中产生的所有文本都将作为自然语言通过，例如，英语文本。此外，这对于生成有意义的对话框已经足够了，只需根据前面的对话框片段调整文本即可。显然，我们离设计这样一个系统还有很长的路要走，因为它需要理解文本，而不仅仅是产生合乎语法的内容。

尽管如此，语言模型在其有限的形式中还是有很大的作用。例如，短语“to recognize speech”和“to wreck a nice beach”听起来非常相似。这可能会导致语音识别中的歧义，这种歧义很容易通过一个语言模型来解决，该语言模型将第二次翻译视为怪异的。同样地，在文档总结算法中，值得知道的是，“狗咬人”比“人咬狗”要频繁得多，或者“我要吃奶奶”是一个相当令人不安的声明，而“我要吃，奶奶”则温和得多。



## 小结

- 语言模型是自然语言处理的一项重要技术。
- 神经网络图通过截断依赖关系，为处理长序列提供了一种方便的模型。
- 长序列的问题是它们很少或从不发生。
- Zipf定律不仅适用于单字，也适用于其他nn -字。
- 有很多结构，但没有足够的频率处理不频繁的词组合有效地通过拉普拉斯平滑。
- 序列划分的主要选择是在连续序列和随机序列之间进行选择。
- 考虑到整个文档的长度，对文档稍微浪费一点并丢弃半空的小批通常是可以接受的。

## 练习

1. 假设训练数据集中有100,000100,000个单词。 4克需要存储多少个单词频率和多单词相邻频率？
2. 查看平滑的概率估计。 为什么它们不准确？ 提示：我们正在处理连续序列而不是单例。
3. 您将如何建立对话模型？
4. 估计齐普夫定律的单字，双字和三​​字的指数。
5. 您还能想到其他哪些小批量数据采样方法？
6. 为什么有一个随机偏移是个好主意？
    是否真的导致文档序列上的分布完全均匀？
    您要做什么才能使事情更加统一？
7. 如果我们希望序列示例是完整的句子，那么在小批量采样中会带来什么问题？ 我们为什么仍要这样做？
