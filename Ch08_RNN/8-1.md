

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-29 19:54:15
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-29 20:11:08
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/sequence.html
-->

# 序列模型

想象一下，你正在Netflix上看电影。作为一个优秀的Netflix用户，你决定虔诚地评价每一部电影。毕竟，一部好电影就是一部好电影，你会想多看一些，对吧?事实证明，事情并非如此简单。随着时间的推移，人们对电影的看法会有很大的变化。事实上，心理学家甚至为一些影响取了名字:

- 根据其他人的意见进行锚定。例如，在获得奥斯卡奖之后，即使它仍是同一部电影，相应电影的收视率也会提高。这种影响持续了几个月，直到该奖项被遗忘为止。[Wu et al。，2017]表明，这种效果将评级提高了一半以上。
- 有享乐主义的适应，人类可以迅速适应新的常态来接受改善的（或恶劣的）情况。例如，在观看了许多好电影之后，人们对下一部电影同样好或更好的期望很高，因此即使是一部普通电影也可以被认为是许多好电影之后的劣质电影。
- 有季节性。很少有观众喜欢在八月看圣诞老人的电影。
- 在某些情况下，由于导演或演员在作品中的不当行为，电影变得不受欢迎。
- 一些电影成为邪教电影，因为它们在漫画上几乎是糟糕的。由于这个原因，《外太空》和《巨魔2》中的计划9获得了很高的声誉。

简而言之，评级并非固定不变。使用时间动态有助于[Koren，2009]更准确地推荐电影。但这不仅仅是电影。

- 当涉及到打开应用程序的时间时，许多用户会有非常特殊的行为。例如，社交媒体应用在放学后受到学生的欢迎。当市场开放时，更常使用股票交易应用程序。
- 预测明天的股价要比填补我们昨天遗漏的股价的空白要困难得多，尽管两者都只是估计一个数字而已。毕竟，远见比后见要难得多。在统计中，前者（预测超出已知观测值）称为外推法，而后者（估计现有观测值之间）称为内插法。
- 音乐，语音，文字，电影，步骤等本质上都是顺序的。如果我们要对它们进行置换，它们将毫无意义。头条狗咬人并不比人咬狗惊讶，即使这两个词是相同的。
- 地震之间的相关性很强，即在发生大地震后，很可能会发生几次较小的余震，比没有发生强烈地震的余震要严重得多。实际上，地震是时空相关的，也就是说，余震通常发生在很短的时间范围内并且非常接近。
- 人与人之间的互动具有顺序性，例如在Twitter的打架，舞蹈方式和辩论中可以看到。

## 统计工具

简而言之，我们需要统计工具和新的深度神经网络架构来处理序列数据。为了简单起见，我们使用图8.1.1所示的股票价格作为示例。

我们用xt≥0表示价格，即，时间t∈Nt∈N，观察价格xtxt。对于一个交易者来说，要想在当天在股票市场上做得好，他应该通过预测

### 自回归模型

为了实现这一目标，我们的交易者可以使用回归器，例如我们在第3.3节中训练的回归器。仅有一个主要问题：输入数量xt-1，…，x1xt-1，…，x1取决于tt。也就是说，数量随着我们遇到的数据量的增加而增加，我们将需要一个近似值以使其在计算上易于处理。本章后面的大部分内容将围绕如何有效估计p（xt∣xt-1，…，x1）p（xt∣xt-1，…，x1）。简而言之，它可以归结为两种策略：

1. 假设潜在的相当长的序列xt-1，...，x1xt-1，...，x1并不是真正必要的。在这种情况下，我们可能会满足于某个时间跨度ττ，而仅使用xt-1，…，xt-τxt-1，…，xt-τ观测值。直接的好处是，现在参数的数量总是相同的，至少对于t>τt>τ而言。如上所述，这使我们能够训练深度网络。这样的模型将被称为自回归模型，因为它们实际上是在对其自身执行回归。

2. 如图8.1.2所示，另一种策略是尝试保留过去观察的一些摘要，同时除预测x ^ tx ^ t之外还更新htht。这导致模型估计xtxt的x ^ t = p（xt∣xt-1，ht）x ^ t = p（xt∣xt-1，ht）并更新ht = g（ht-1， xt-1）ht = g（ht-1，xt-1）。由于从未观察到htht，因此这些模型也称为潜在自回归模型。LSTM和GRU就是这样的例子。

图8.1.2 潜在的自回归模型

这两种情况都提出了如何生成训练数据的明显问题。人们通常使用历史观测来预测下一个观测结果给定到目前为止的观测结果。显然，我们不希望时间停滞不前。然而，一个常见的假设是，虽然xtxt的特定值可能会改变，但至少时间序列本身的动态不会改变。这是合理的，因为新动态只是新颖的，因此使用我们目前拥有的数据是不可预测的。统计学家称不改变的动态是静止的。无论我们做什么，我们将因此得到整个时间序列的估计通过

(8.1.2)

p (x1,…, xT) =∏t = 1 tp (xT∣xT−1,…, x1)。

请注意，如果我们处理的是离散对象，比如单词，而不是数字，那么上面的考虑仍然成立。唯一的区别是，在这种情况下，我们需要使用分类器而不是回归器来估计p(xt∣xt−1，…，x1)p(xt∣xt−1，…，x1)。

### 马尔可夫模型

回想一下，在自回归模型中，我们仅使用(xt−1，…，xt−−，)(xt−1，…，xt−−，)而不是(xt−1，…，x1)(xt−1，…，x1)来估计xtxt。只要这个近似是准确的，我们就说这个序列满足马尔可夫条件。特别地，如果(1)(1)(1)，我们有一个一阶马尔可夫模型，p(x)p(x)为

当xtxt只假设一个离散值时，这样的模型特别好，因为在这种情况下，可以使用动态规划沿着链精确地计算值。例如，我们可以有效地计算p(xt+1∣xt−1)p(xt+1∣xt−1)这一事实，我们只需要考虑过去观测的一个非常短的历史:

TODO:MATH

讨论动态规划的细节超出了本节的范围，但我们将在第9.4节中介绍它。控制和强化学习算法广泛使用这些工具。


### 因果关系

原则上，以相反的顺序展开p(x1，…，xT)p(x1，…，xT)没有什么错。毕竟，通过条件设置，我们总是可以写出它

TODO:MATH

实际上，如果我们有一个马尔可夫模型，我们也可以获得反向条件概率分布。但是，在许多情况下，数据存在一个自然的方向，即及时向前发展。显然，未来的事件不会影响过去。因此，如果我们更改xtxt，我们也许能够影响xt + 1xt + 1的情况，但反之则不然。也就是说，如果我们更改xtxt，则过去事件的分布将不会更改。因此，应该比p（xt∣xt + 1）p（xt∣xt + 1）更容易解释p（xt + 1∣xt）p（xt + 1∣xt）。例如，[Hoyer et al。，2009]表明，在某些情况下，对于某些加性噪声，我们可以找到xt + 1 = f（xt）+ ϵxt + 1 = f（xt）+ ϵ，而反之则不成立。这是个好消息，因为这通常是我们感兴趣的向前方向。有关此主题的更多信息，请参见[Peters等，2017a]所著的书。我们勉强能抓到它的表面。

## 简单例子

讲了这么多理论之后，让我们在实践中试试吧。让我们从生成一些数据开始。为了简单起见，我们使用带有一些附加噪声的正弦函数来生成时间序列。

TODO:CODE

接下来，我们需要将此时间序列转换为网络可以训练的功能和标签。基于嵌入维ττ，我们将数据映射为yt = xtyt = xt和zt =（xt-1，...，xt-τ）zt =（xt-1，...，xt-τ）。精明的读者可能已经注意到，这给了我们ττ更少的数据点，因为我们没有足够的历史记录来记录它们的第一个ττ。一个简单的解决方法，尤其是在时间序列较长的情况下，是丢弃这几个术语。或者，我们可以在时间序列上填充零。下面的代码与上一节中的培训代码基本相同。我们使架构相当简单。

TODO:CODE

一个完全连接的网络的几层，ReLU激活和L2L2丢失。由于在构建回归估计器时，建模的大部分内容与前面的部分相同，因此我们将不深入研究太多细节。

TODO:CODE

现在我们准备好训练了。

TODO:CODE

## 预测

由于训练损失很小，我们期望我们的模型能很好地工作。让我们看看这在实践中意味着什么。首先要检查的是，该模型能否很好地预测下一个时间步会发生什么。

TODO:CODE

正如我们预期的那样，这看起来不错。即使超过600次观察，估计值仍然值得信赖。这只有一个小问题：如果我们仅观察到时间步长600之前的数据，我们就不能希望为将来的所有预测都接受地面真理。相反，我们需要一次向前迈出一步：



换句话说，我们将不得不使用自己的预测来进行未来的预测。让我们看看这进展如何。

TODO:CODE

## 小结

- 许多用户在打开应用程序的时候都有非常特殊的行为。例如，社交媒体应用程序在学生放学后更受欢迎。当市场开放时，股票市场交易应用程序的使用更为普遍。
- 预测明天的股票价格比填补我们昨天错过的股票价格的空白要难得多，尽管两者都只是估计一个数字而已。毕竟，预见远比事后诸葛亮难得多。在统计学中，前者(超出已知观测值的预测)称为外推，而后者(在现有观测值之间进行估计)称为内插。
- 音乐、演讲、文本、电影、步骤等都是顺序的。如果我们把它们置换，它们就毫无意义了。狗咬人的标题比人咬狗的标题要少得多，尽管这两个词是一样的。
- 地震具有很强的相关性。大地震发生后，很可能会有几次较小的余震，比没有强烈地震时要多得多。事实上，地震在时空上是相互关联的。余震通常发生在较短的时间跨度内，而且距离较近。
- 人类之间的互动是连续的，就像我们在推特上看到的战斗、舞蹈和辩论。


## 练习

1. 改进以上模型。
   - 加入比过去4个观察更多的信息？ 您真的需要几个？
   - 如果没有噪音，您需要几个？ 提示：您可以将sinsin和coscos写为微分方程。
   - 您可以合并较旧的功能，同时保持功能总数不变吗？ 这会提高准确性吗？ 为什么？
   - 更改神经网络架构，看看会发生什么。
1. 投资者想找到一个好的证券来购买。她查看过往的收益，以决定哪一项可能做得更好。此策略可能会出什么问题？
1. 因果关系也适用于文本吗？ 在什么程度上？
1. 举例说明何时可能需要潜在的自回归模型来捕获数据的动态性。
