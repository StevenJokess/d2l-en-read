

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-29 20:22:45
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 19:34:12
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/rnn.html
-->

# 递归神经网络

在8.3节中，我们介绍了nn -gram模型，其中在位置tt处单词xtxt的条件概率仅取决于前n-1n-1个单词。如果我们想检查早于t-（n-1）t-（n-1）的单词对xtxt的可能影响，我们需要增加nn。但是，模型参数的数量也会随之成倍增加，因为我们需要为词汇VV存储| V | n | V | n个数字。因此，与其建模p（xt∣xt-1，...，xt-n + 1）p（xt∣xt-1，...，xt-n + 1），不如使用潜变量模型，其中

TODO:MATH

这里htht是存储序列信息的潜在变量。潜变量又称为隐变量、隐状态或隐状态变量。tt时刻的隐藏状态可以通过输入xtxt和隐藏状态ht−1ht−1来计算，即

TODO:MATH

对于一个足够强大的函数f，潜变量模型不是一个近似。毕竟，ht可以简单地存储到目前为止观察到的所有数据。我们在8.1节中对此进行了讨论。但它有可能使计算和存储都变得昂贵。

注意，我们也使用h来表示隐藏层的隐藏单位的数量。隐藏层和隐藏状态指的是两个非常不同的概念。如前所述，隐藏层是在从输入到输出的路径上隐藏的层。从技术上讲，隐状态是我们在给定步骤中所做的任何操作的输入。相反，它们只能通过查看前一次迭代的数据来计算。从这个意义上说，它们与统计中的潜在变量模型有很多共同之处，比如集群或主题模型，其中集群影响输出，但不能直接观察到。

递归神经网络是具有隐状态的神经网络。在介绍这个模型之前，让我们先回顾一下4.1节中介绍的多层感知器。

## RNN简化模型[2]

1）x(t)代表在序列索引号t时训练样本的输入。同样的，x(t−1)和x(t+1)代表在序列索引号t−1和t+1时训练样本的输入。
2）h(t)代表在序列索引号t时模型的隐藏状态。h(t)h(t)由x(t)和h(t−1)共同决定。
3）o(t)代表在序列索引号t时模型的输出。o(t)只由模型当前的隐藏状态h(t)决定。
4）L(t)代表在序列索引号t时模型的损失函数。
5）y(t)代表在序列索引号t时训练样本序列的真实输出。
6）U,W,V这三个矩阵是RNN网络模型的参数，并且时间t上是共享的，这体现了RNN的模型的“循环反馈”的思想。


为了估计RNN网络参数，依然可以采用反向传播方法，即通过梯度下降法一轮轮的迭代来解决这个问题。但是RNN是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT(back-propagation through time)。但是这个反向传播的效果实在不好，传播过程中的梯度很容易消散（爆炸），导致训练终止。研究人员对于序列索引位置t的隐藏结构做了改进，可以说通过一些技巧让隐藏结构复杂了起来，来避免梯度消失(爆炸)的问题，因此能够处理更长的序列数据。这个特殊RNN就是长短记忆模型Long Short-Term Memory (LSTM)，如下图所示：

## 没有隐藏状态的递归网络

让我们看一下具有单个隐藏层的多层感知器。给定实例的小批量X∈Rn×dX∈Rn×d，样本大小为nn和dd输入。让隐藏层的激活功能为。因此，隐藏层的输出H∈Rn×hH∈Rn×h计算为

H ＝ ϕ（XWxh + bh）。

在这里，我们具有用于隐藏层的权重参数Wxh∈Rd×hWxh∈Rd×h，偏差参数bh∈R1×hbh∈R1×h和隐藏单元数hh。

隐藏变量HH用作输出层的输入。输出层由下式给出

O = HWhq + bq。

在此，O∈Rn×qO∈Rn×q是输出变量，Whq∈Rh×qWhq∈Rh×q是权重参数，bq∈R1×qbq∈R1×q是输出层的偏差参数。如果是分类问题，则可以使用softmax（O）softmax（O）计算输出类别的概率分布。

这完全类似于我们先前在8.1节中解决的回归问题，因此我们省略了细节。可以说我们可以随机选择（xt，xt-1）对，并通过autograd和随机梯度下降来估计网络的参数W和b。

## 有隐藏状态的递归网络

当我们具有隐藏状态时，情况完全不同。让我们更详细地看一下结构。请记住，在优化算法中，我们通常将迭代tt称为时间tt，循环神经网络中的时间是指迭代中的步骤。假设我们有Xt∈Rn×dXt∈Rn×d，则t = 1，…，Tt = 1，…，T迭代。并且Ht∈Rn×hHt∈Rn×h是序列中时间步tt的隐藏变量。与多层感知器不同，这里我们保存上一个时间步的隐藏变量Ht-1Ht-1，并引入一个新的权重参数Whh∈Rh×hWhh∈Rh×h，以描述如何在上一个时间步中使用前一个时间步的隐藏变量。当前时间步。具体来说，当前时间步长的隐藏变量的计算是通过当前时间步长的输入以及前一个时间步长的隐藏变量来确定的：

Ht ＝ ϕ（XtWxh + Ht-1Whh + bh）。

与（8.4.3）相比，我们在这里又增加了Ht-1WhhHt-1Whh。从相邻时间步的隐藏变量HtHt和Ht-1Ht-1之间的关系，我们知道这些变量捕获并保留了直到当前时间步的序列历史信息，就像神经网络当前时间步的状态或内存一样。因此，这种隐藏变量称为隐藏状态。由于隐藏状态使用当前时间步中先前时间步的相同定义，因此上面方程的计算是递归的，因此称为递归神经网络（RNN）。

有许多不同的RNN构造方法。由上式定义的具有隐藏状态的RNN非常常见。对于时间步tt，输出层的输出类似于多层感知器中的计算：

（8.4.6）

Ot = HtWhq + bq。

RNN参数包括具有偏置bh∈R1×hb的隐藏层的权重Wxh∈Rd×h，Whh∈Rh×hWxh∈Rd×h，Whh∈Rh×h和权重Whq∈Rh× 偏置为bq∈R1×qbq∈R1×q的输出层的qWhq∈Rh×q。值得一提的是，即使对于不同的时间步长，RNN始终使用这些模型参数。因此，RNN模型参数的数量不会随着时间步长的增加而增加。

图8.4.1显示了RNN在三个相邻时间步的计算逻辑。在时间步tt中，在将输入XtXt与前一时间步的隐藏状态Ht-1Ht-1串联之后，可以将隐藏状态的计算视为具有激活函数的完全连接层的条目。完全连接层的输出是当前时间步HtHt的隐藏状态。它的模型参数是WxhWxh和WhhWhh的串联，偏差为bhbh。当前时间步tt的隐藏状态Ht将参与计算下一个时间步t + 1t + 1的隐藏状态Ht + 1。而且，Ht将成为Ot的输入，OtOt是当前时间步的完全连接的输出层。

图8.4.1具有隐藏状态的RNN

## 语言模型中的步骤

现在我们将说明如何使用RNNs来构建语言模型。为了便于说明，我们使用单词而不是字符作为输入，因为前者更容易理解。让小批大小为1，文本序列为数据集的开始，即。，《时间机器》，作者是h·g·威尔斯。图8.4.2说明了如何根据现在和以前的单词估计下一个单词。在训练过程中，我们对每个时间步长输出层的输出进行softmax操作，然后利用交叉熵损失函数计算结果与标签之间的误差。由于对隐藏层的隐藏状态进行递归计算，timestep 3 O3O3的输出分别由文本序列“the”、“time”和“machine”决定。由于训练数据中序列的下一个单词是“by”，所以时间步长3的损失取决于根据该时间步长特征序列“the”、“time”、“machine”和标签“by”生成的下一个单词的概率分布。

字级RNN语言模型。输入序列和标签序列分别是h的时间机器和h的时间机器

在实践中，每个单词都用dd维向量表示，我们使用批量大小n>1n>1。因此，时间步长tt处的输入XtXt将是一个n×dn×d矩阵，这与我们之前讨论的是相同的。

## 困惑

最后，让我们讨论一下如何测量序列模型的质量。一种方法是检查文本的惊奇程度。一个好的语言模型能够以高准确度的令牌预测接下来将要看到的内容。考虑不同语言模型提出的短语“下雨了”的以下延续：

“外面下雨了”

“正在下雨的香蕉树”

“正在下雨； kcj pwepoiut”

我们可以通过计算p(w)来衡量模型的质量，即。，序列的可能性。不幸的是，这是一个难以理解和比较的数字。毕竟，较短的序列比较长的序列更容易出现，因此，以托尔斯泰的巨著《战争与和平》为模型进行评估，不可避免地会产生比Saint-Exupery的中篇小说《小王子》小得多的可能性。缺少的是相当于平均值的东西。

信息论在这里很有用，我们将在18。11节中介绍更多信息。如果我们想要压缩文本，我们可以询问在给定当前符号集的情况下估计下一个符号。一个下界的比特数是由−log2p (xt∣xt−1,…, x1)−log2⁡p (xt∣xt−1,…, x1)。一个好的语言模型应该允许我们相当准确地预测下一个单词。因此，它应该允许我们花费很少的比特来压缩序列。所以我们可以用我们需要花费的平均比特数来衡量它。

TODO:MATH

它可以被最好地理解为我们在决定接下来选择哪个词时所拥有的真实选择数量的调和平均数。注意，当我们介绍softmax回归(第3.4节)时，困惑自然地概括了交叉熵损失的概念。也就是说，对于一个符号，除了一个是另一个的指数之外，两种定义是相同的。让我们看几个例子:

- 在最好的情况下，模型总是估计下一个符号的概率为11。此时模型的perplexity为11。
- 在最坏的情况下，模型总是预测标签类别的概率为0。在这种情况下，困惑是无限的。
- 在基线上，模型预测了所有令牌的均匀分布。在这种情况下，perplexity等于字典len(vocab)的大小。事实上，如果我们存储序列而不进行任何压缩，这将是我们能做的最好的编码。因此，这提供了一个任何模型都必须满足的非平凡上界。

## 小结

- 使用递归计算的网络称为递归神经网络(RNN)。
- 该神经网络的隐藏状态可以捕捉到序列到当前时间步长的历史信息。
- RNN模型参数的数量不会随着时间步长的增加而增加。
- 我们可以使用字符级RNN创建语言模型。

## 练习

1. 如果我们使用RNN来预测文本序列中的下一个字符，我们需要多少个输出维度?
2. 你能设计一种映射，使具有隐藏状态的RNN是精确的吗?提示:有限数量的单词呢?
3. 如果你通过一个很长的序列反向传播，梯度会发生什么变化?
4. 与上述简单序列模型相关的一些问题是什么?

[2]: http://www.tensorinfinity.com/paper_208.html
