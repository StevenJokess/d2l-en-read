

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-29 20:36:19
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-29 21:00:15
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/rnn-scratch.html
-->

# 从零实现递归神经网络

在本节中，我们将从头开始实现第8节中介绍的语言模型。 它基于在H. G. Wells的“时间机器”上训练的字符级循环神经网络。 和以前一样，我们首先读取数据集，这在第8.3节中介绍。

TODO:CODE

## One-hot编码

请记住，每个标记在train_iter中均以数字索引形式显示。 将这些索引直接输入到神经网络可能会使学习变得困难。 我们通常将每个标记表示为更具表达力的特征向量。 最简单的表示方式称为One-hot编码。

简而言之，我们将每个索引映射到不同的单位向量：假设词汇表中不同标记的数量为N（len（vocab）），并且标记索引的范围为0到N-1。 如果令牌的索引是整数ii，那么我们将创建一个全为0且长度为NN的向量ei，并将位置i处的元素设置为1。此向量是原始令牌的单向向量。 索引为0和2的单热点矢量如下所示。

TODO:CODE

我们每次采样的小批量的形状为（批量大小，时间步长）。 one_hot函数将这样的小批量转换为最后一维等于词汇量的3-D张量。 我们经常对输入进行转置，以便获得（时间步长，批大小，词汇量）输出，该输出更容易适合序列模型。

TODO:CODE

## 初始化模型参数

接下来，我们初始化RNN模型的模型参数。隐藏单元数num_hiddens是一个可调参数。

TODO:CODE

## RNN模型

首先，我们需要一个init_rnn_state函数来在初始化时返回隐藏状态。它返回一个张量，填充为0，形状为(批大小，隐藏单元的数量)。使用元组可以更容易地处理隐藏状态包含多个变量的情况(例如，在RNN中组合多个层时，每个层都需要初始化)。

TODO:CODE

下面的rnn函数定义了如何计算一个时间步长的隐藏状态和输出。这里的激活函数使用tanhtanh函数。如:numref:sec_mlp中所述，tanhtanh函数的平均值为0，当元素均匀分布在实数上时。

TODO:CODE

现在我们已经定义了所有的函数，接下来我们创建一个类来包装这些函数并存储参数。

TODO:CODE

让我们检查输入和输出是否具有正确的维数，例如，确保隐藏状态的维数没有改变。

TODO:CODE

我们可以看到，输出的形状是(数步数×批大小，词汇表大小)，而隐藏状态的形状保持不变，即，(批大小，隐藏单元的数量)。

## 预测

我们首先解释预测函数，这样我们可以在训练中定期检查预测。这个函数根据前缀预测下一个num_predict字符(包含几个字符的字符串)。对于序列的开始，我们只更新隐藏状态。在此之后，我们开始生成新的字符并发送它们。

TODO:CODE

我们首先测试predict_ch8函数。 鉴于我们没有训练网络，它将产生无意义的预测。 我们使用序列行进器对其进行初始化，并使其生成10个其他字符。

TODO:CODE

## 梯度剪裁

对于长度为TT的序列，我们在一次迭代中计算这些TT时间步长的梯度，从而在反向传播过程中产生长度为O(T)O(T)的矩阵乘积链。如4.8节所述，它可能会导致数值不稳定性，例如，当TT较大时，梯度可能会爆炸或消失。因此，RNN模型经常需要额外的帮助来稳定训练。

回想一下,当求解一个优化问题,我们将更新步骤权重ww一般负梯度方向gtgt minibatch,说w−η⋅gtw−η⋅gt。让我们进一步假设目标是行为良好的，即。， l为常数的Lipschitz连续，即，

TODO:MATH

在这种情况下我们可以安全地假设如果我们更新权向量η⋅gtη⋅gt,我们不会看到一个变化超过Lη∥gt∥Lη为gt为。这既是一种诅咒也是一种祝福。它是诅咒，因为它限制了进步的速度;而它是祝福，因为它限制了如果我们朝着错误的方向前进，事情可能出错的程度。

有时梯度可能很大，优化算法可能无法收敛。我们可以通过降低学习速度来解决这个问题，或者通过一些其他的高阶技巧。但是如果我们很少得到大的梯度呢?在这种情况下，这种方法似乎完全没有道理。另一种方法是通过将渐变投影回一个给定半径的球，比如“”

TODO:MATH

通过这样做，我们知道梯度范数永远不会超过，并且更新后的梯度完全与原始方向gg对齐。它还具有理想的副作用，即限制任何给定的小批(以及其中的任何给定样本)对权重向量的影响。这赋予了模型一定程度的健壮性。梯度剪辑提供了一个快速修复梯度爆炸。虽然它不能完全解决问题，但它只是缓解问题的众多技术之一。

下面我们定义了一个函数来剪辑模型的渐变，该模型要么是从头构建的实例，要么是由高级api构建的模型。还要注意，我们计算所有参数的梯度范数。

TODO:CODE

## 训练

让我们首先定义在一个数据时代训练模型的函数。 它与3.6节的模型训练在三个地方不同：

1. 顺序数据的不同采样方法（随机采样和顺序分区）将导致隐藏状态的初始化有所不同。
2. 我们在更新模型参数之前裁剪渐变。 这样可以确保即使在训练过程中的某个时候出现梯度梯度变化时模型也不会发散，并且可以有效地自动减小步长。
3. 我们使用困惑来评估模型。 这确保了不同长度的序列是可比的。

当使用顺序分区时，我们在每个时期的开始初始化隐藏状态。 由于下一个微型批处理中的第ith个示例与当前ithith示例相邻，因此下一个微型批处理可以直接使用当前隐藏状态，因此我们仅分离梯度，以便计算微型批处理中的梯度。 当使用随机采样时，由于每个示例都使用随机位置采样，因此我们需要为每次迭代重新初始化隐藏状态。 与3.6节中的train_epoch_ch3函数相同，我们使用广义更新器，它可以是高级API训练器，也可以是临时实现。

training函数同样支持从头开始或使用高级api实现模型。

TODO:CODE

现在我们可以训练一个模型。由于我们在数据集中只使用了10,000个标记，因此模型需要更多的epoch来收敛。

TODO:CODE

最后让我们用一个随机抽样迭代器来检查结果。

TODO:CODE

虽然从头实现上述RNN模型具有指导意义，但并不方便。在下一节中，我们将看到如何对当前模型进行显著改进，以及如何使其更快更容易实现。

## 小结

- 序列模型的训练需要状态初始化。
- 在序列模型之间，您需要确保分离梯度，以确保自动区分不会传播超出当前样本的效果。
- 一个简单的RNN语言模型由一个编码器、一个RNN模型和一个解码器组成。
- 渐变剪辑防止渐变爆炸(但它不能修复渐变消失)。
- Perplexity通过不同的序列长度校准模型性能。它是交叉熵损失的指数平均值。
- 顺序划分通常会产生更好的模型。

## 练习

1. 表明单一热点编码等价于为每个对象选择不同的嵌入。
1. 调整超参数以提高复杂度。
   你能跌多低?调整嵌入、隐藏单元、学习率等。
   它在其他h·g·威尔斯的书，如《世界大战》中效果如何?
1. 修改预测函数，比如使用抽样，而不是选择最有可能的下一个字符。
   会发生什么呢?
   向更有可能输出偏差模型,例如,通过抽样从q (wt∣wt−1,…, w1)∝pα(wt∣wt−1,…, w1)问(wt∣wt−1,…, w1)∝pα(wt∣wt−1,…, w1)α> 1α> 1。
1. 在不剪切渐变的情况下运行本节中的代码。会发生什么呢?
1. 更改顺序分区，使其不会从计算图中分离隐藏状态。运行时间有变化吗?精确度呢?
1. 将本节中使用的激活函数替换为ReLU，重复本节中的实验。
1. 证明复杂度是条件词概率调和均值的倒数。
