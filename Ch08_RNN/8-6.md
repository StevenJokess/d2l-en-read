

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-29 21:01:09
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-29 21:09:15
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/rnn-concise.html
-->

# RNN的简洁实现

虽然第8.5节对了解如何实现递归神经网络（RNN）具有指导意义，但这并不方便也不快捷。本节将展示如何使用Gluon提供的功能更有效地实现相同的语言模型。通过阅读“ Time Machine”语料库，我们像以前一样开始。

TODO:CODE

## 定义模型

胶子的rnn模块提供了递归神经网络实现(超越了许多其他序列模型)。构造了具有单隐层和256个隐单元的递归神经网络层rnn_layer，并对权值进行了初始化。

TODO:CODE

初始化状态很简单。我们调用成员函数rnn_layer.begin_state(batch_size)。这将返回迷你批处理中每个元素的初始状态。也就是说，它返回一个大小对象(隐藏层、批大小、隐藏单元的数量)。隐藏层的数量默认为1。事实上，我们甚至还没有讨论多层意味着什么—这将在9.3节中发生。现在，我们可以简单地说，多个层相当于一个RNN的输出，它被用作下一个RNN的输入。

TODO:CODE

有了状态变量和输入，我们就可以用更新后的状态来计算输出。

TODO:CODE

与8.5节类似，我们通过将一个完整的递归神经网络的block类子类化来定义RNNModel block。注意，rnn_layer只包含隐藏的递归层，我们需要创建一个单独的输出层。在上一节中，我们在rnn块中有一个输出层。

TODO:CODE

## 训练与预测

在训练模型之前，让我们对具有随机权重的模型进行预测。

TODO:CODE

很明显，这种模式根本行不通。接下来，我们使用8.5节中定义的相同超参数调用train_ch8，并使用胶子训练我们的模型。

TODO:CODE

与上一节相比，这个模型达到了类似的复杂程度，尽管在更短的时间内，因为代码得到了更优化。

## 小结

胶子的rnn模块提供了递归神经网络层的实现。

胶子的神经网络。RNN实例在前向计算后返回输出和隐藏状态。此前向计算不涉及输出层计算。

与前面一样，为了提高效率，计算图需要与前面的步骤分离。

## 练习

1. 将实现与上一节进行比较。
    * 为什么胶子的实现运行得更快?
    * 如果你观察到速度以外的显著差异，试着找出原因。
1. 你能把这个模型做得太合身吗?
    * 增加隐藏单位的数量。
    * 增加迭代的次数。
    * 如果你调整剪切参数会发生什么?
1. 利用神经网络实现本章引言的自回归模型。
1. 如果你在RNN模型中增加隐藏层的数量会发生什么?你能使这个模型工吗?
1. 使用此模型压缩文本的效果如何?
   * 你需要多少位?
   * 为什么不是每个人都使用这个模型进行文本压缩呢?提示:压缩机本身呢?
