

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-19 20:00:08
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-16 19:45:53
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/bptt.html
-->

# 通过时间的反向传播

到目前为止，我们已经多次暗示了像爆炸梯度，消失梯度，以及需要为RNNs分离梯度。例如，在8.5节中，我们对序列调用了detach函数。为了能够快速构建模型并了解它是如何工作的，这些都没有得到真正充分的解释。在这一节中，我们将更深入地探究序列模型的反向传播的细节，以及数学原理是如何工作的。

在第一次实现RNNs时，我们遇到了一些梯度爆炸的影响(章节8.5)。特别是，如果你解决了这些练习，你就会看到梯度裁剪对于确保适当的收敛是至关重要的。为了更好地理解这个问题，本节将回顾如何为序列模型计算梯度。注意，它的工作方式在概念上没有什么新东西。毕竟，我们仍然只是应用链式法则来计算梯度。尽管如此，还是有必要再次回顾一下反向传播(第4.7节)。

在第4.7节中，我们已经描述了MLPs中的正向和反向传播以及计算图。RNN中的正向传播相对简单。穿越时间的反向传播实际上是反向传播在RNNs中的一个具体应用[Werbos, 1990]。它要求我们一次一步地扩展RNN的计算图，得到模型变量和参数之间的依赖关系。然后，基于链式法则，应用反向传播来计算和存储梯度。由于序列可能相当长，因此依赖关系也可能相当长。例如，对于一个包含1000个字符的序列，第一个标记可能会对最后位置的标记产生重大影响。这在计算上是不可行的(它需要太长的时间和太多的内存)，它需要超过1000个矩阵乘积，我们才会到达这个非常难以捉摸的梯度。这是一个充满计算和统计不确定性的过程。在下面我们将说明发生了什么以及如何在实践中解决这个问题。

## 简化的循环网络

我们从RNN如何工作的简化模型开始。这个模型忽略了隐藏状态的细节以及它是如何更新的。这里的数学符号不像过去那样明确区分标量、向量和矩阵。这些细节对分析无关紧要，只会使本小节的符号混乱。

在这个简化模型中，我们表示htht为隐藏状态，xtxt为输入，otot为时间步长tt时的输出。回想一下我们在8.4.2节中讨论的输入和隐藏状态可以连接起来，在隐藏层中乘以一个权值变量。因此，我们使用whwh和wowo分别表示隐含层和输出层的权值。因此，每个时间步长的隐藏状态和输出可以解释为

TODO:CODE

因此，我们有一个值链{ ... ，(h t-1，x t-1，o t-1) ，(h t，x t，o t) ，... }{ ... ，(ht-1，xt-1，ot-1) ，(ht，xt，ot) ，... } ，它们通过递归计算相互依赖。正向传播是相当简单的。我们所需要做的就是一次循环完成(xt，ht，ot)(xt，ht，ot)三元组。然后用目标函数作为评价指标，计算出期望目标的输出值与期望目标的偏差

对于反向传播来说，问题就有点棘手了，尤其是当我们根据目标函数 l 的参数来计算梯度时。具体来说，根据链式法则,

导数的第一部分和第二部分很容易计算。在第三部分，我们需要计算出参数对系统的影响，这也是影响系统性能的关键因素。

为了得到上述梯度，假设我们有三个序列{ a t } ，{ b t } ，{ c t }{ at } ，{ bt } ，{ ct }满足 a 0 = 0，a 1 = b 1 a0 = 0，a 1 = b 1 = b 1，a t = b t + c t a t-1 at = bt + ctat-1 for t = 1,2，... t = 1,2，..。然后对于 t ≥1 t ≥1，很容易证明

现在让我们代入(8.7.4)

因此，一个 t = b t + c t a t-1 at = bt + ctat-1成为下面的递归

到(8.7.4)时，第三部分将是

虽然我们可以用链式规则递归地计算环境影响系数，但是只要 t 很大，这个链式就会变得很长。让我们讨论一些处理这个问题的策略。

计算总和。这是非常缓慢和梯度可以爆炸，因为微妙的变化，在初始条件可以潜在地影响结果很多。也就是说，我们可以看到类似于蝴蝶效应的情况，即初始条件的最小变化导致结果的不成比例的变化。就我们想要估计的模型而言，这实际上是相当不合适的。毕竟，我们正在寻找能够很好地推广的稳健估计。因此，这种策略几乎从未在实践中使用过。

在步骤之后截断求和。这就是我们到目前为止一直在讨论的。这导致了一个真正梯度的近似值，只需要将上面的求和终止在环境影响系数为-的基础上。因此，这个逼近误差是由基于分布式环境的两岸平衡系数 f (x t，h t-1，w) w h t-1 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x。实际上，这种方法很有效。这就是通常所说的截短 BPTT (通过时间的倒退)。这样做的结果之一是，这种模式主要侧重于短期影响，而不是长期后果。这实际上是可取的，因为它使估计偏向于更简单和更稳定的模型。

随机截断。最后我们可以把∂whht∂whht替换成一个随机变量它在期望中是正确的但是它截断了序列。这是通过使用一个序列的ξtξt E(ξt) = 1 E(ξt) = 1, P(ξt = 0) = 1−πP(ξt = 0) = 1−π而且P(ξt =π−1)=πP(ξt =π−1)=π。我们用这个来替换梯度:

它遵循从ξ的定义tξt E [zt型]=∂whtE (zt型)=∂什么。每当ξt = 0ξt = 0扩大终止。这导致了变长序列的加权和，其中长序列很少，但适当地增加了权重。[Tallec & Ollivier, 2017]在他们的论文中提出了这一点。不幸的是，尽管理论上很有吸引力，但这种模型并不比简单的截断效果好多少，这很可能是由于许多因素造成的。首先，在向过去执行了许多反向传播步骤之后，观察到的结果足以在实践中捕获依赖关系。其次，增加的方差抵消了梯度更准确的事实。第三，我们实际上想要的模型只有一个短范围的互动。因此，BPTT具有令人满意的轻微的正则化效果。

### 全计算

显然，我们只需计算（8.7.7）中的总和即可。 但是，这非常慢，并且梯度可能会增大，因为初始条件的细微变化可能会对结果产生很大影响。 也就是说，我们可以看到类似于蝴蝶效应的情况，其中初始条件的最小变化导致结果变化不成比例。 就我们要估计的模型而言，这实际上是非常不希望的。 毕竟，我们正在寻找能很好地概括的可靠估计量。 因此，这种策略几乎从未在实践中使用。

### 截断时间步

或者，我们可以在ττ个步骤后截断（8.7.7）中的总和。 到目前为止，这就是我们一直在讨论的内容，例如，当我们在8.5节中分离渐变时。 只需将总和终止于∂ht-τ/∂wh∂ht-τ/∂wh，就可以得出真实梯度的近似值。 实际上，这很好。 这就是通常所说的随时间的截短反向传播[Jaeger，2002]。 其后果之一是该模型主要关注短期影响，而不是长期影响。 这实际上是理想的，因为它会将估算值偏向更简单，更稳定的模型。

### 随机截断

最后，我们可以用∂ht/∂wh，来替换∂ht/∂wh，用一个随机变量它在期望中是正确的，但是将序列截断。这是通过使用一系列ξt t和预定义的ξ0≤πt≤10≤πt≤1,P(ξt = 0) = 1−πtP(ξt = 0) = 1−πt和P(ξt =π−1 t) =πtP(ξt =π−1)=πt,因此E(ξt) = 1 E(ξt) = 1。我们用这个来替换梯度∂ht/∂wh∂ht/∂wh in(8。4。4

(8.7.8)

zt型=∂f (xt ht−1,wh)∂wh +ξt∂f (xt, ht−1,wh)∂ht−1∂ht−1∂wh。

zt型=∂f (xt ht−1,wh)∂wh +ξt∂f (xt, ht−1,wh)∂ht−1∂ht−1∂wh。

它遵循从ξ的定义tξt E [zt型]=∂ht /∂假名(zt型)=∂ht /∂wh。每当ξt = 0ξt = 0反复计算终止时间步tt。这导致了变长序列的加权和，其中长序列很少，但适当地增加了权重。这个想法是由Tallec和Ollivier提出的[Tallec & Ollivier, 2017]。



### 比较策略

图8.7.1从上到下:随机化的BPTT、规则截断的BPTT和完整的BPTT实现

图8.7.1展示了使用rnn的时间反向传播分析*Time Machine book*的前几个字符时的三种策略:

* 第一行是随机截断，将文本分割成不同长度的段。
* 第二行是常规截断，它将文本分成相同长度的子序列。这就是我们在RNN实验中所做的。
* 第三行是通过时间的完全反向传播，它导致一个计算上不可行的表达式。

不幸的是，尽管理论上很有吸引力，但随机截断法的效果并不比常规截断法好多少，这很可能是由于许多因素造成的。首先，在向过去执行了许多反向传播步骤之后，观察到的结果足以在实践中捕获依赖关系。其次，增加的方差抵消了梯度步长越大越精确的事实。第三，我们实际上想要的模型只有短范围的交互。因此，随着时间的推移，有规律地截断的反向传播具有可取的轻微正则化效果。

## BPTT详细

在讨论了一般原则之后，让我们来详细讨论BPTT。将WW分解为不同的权矩阵集(Whx,WhhWhx,Whh, WohWoh)，得到简单的线性潜变量模型:

根据第4.7节的讨论，我们计算出

在l(⋅)表示被选中的损失函数。对WohWoh求导很简单



其中prod（⋅）prod（⋅）表示两个或多个矩阵的乘积。

对WhxWhx和WhhWhh的依赖关系比较棘手，因为它涉及到一连串的导数。 我们从

毕竟，隐藏状态相互依赖，也依赖于过去的输入。关键数量是过去的隐藏状态如何影响未来的隐藏状态。


TODO:MATH

这种潜在的非常令人恐惧的表达有许多原因。 首先，它需要存储中间结果，即在我们通过损失函数LL的方式工作时的Whh的幂。 其次，这个简单的线性示例已经显示出长序列模型的一些关键问题：它涉及潜在的非常大的幂Wjhh。 在其中，小于1的特征值对于大j消失，而大于1的特征值发散。 这在数值上是不稳定的，并且对可能不相关的过去细节给予了不适当的重视。 解决此问题的一种方法是以计算方便的大小截断和。 在第9节的后面，我们将看到更复杂的序列模型（例如LSTM）如何进一步减轻这种情况。 实际上，这种截断是通过在给定数量的步骤后分离梯度来实现的。

## 通过时间进行详细的反向传播

在讨论了一般原理之后，让我们详细讨论一下穿越时间的反向传播。与8.7.1节的分析不同，下面我们将展示如何计算目标函数相对于所有分解的模型参数的梯度。为简单起见,我们考虑一个RNN没有偏差参数,隐藏层的激活函数使用标识映射(ϕ(x) = xϕ(x) = x)。对于时间步长tt，让单个示例输入和标签分别为xt∈Rdxt∈Rd和ytyt。计算隐藏状态ht∈Rhht∈Rh，输出ot∈Rqot∈Rq

TODO:MATH

其中，Whx∈Rh×dWhx∈Rh×d, Whh∈Rh×hWhh∈Rh×h, Wqh∈Rq×hWqh∈Rq×h为权重参数。用l(ot,yt)l(ot,yt)表示时间步长tt时的损失。我们的目标函数，从序列开始的TT时间步长损失是这样的

TODO:MATH

为了使RNN计算过程中模型变量和参数之间的依赖关系可视化，我们可以绘制模型的计算图，如图8.7.2所示。例如，计算时间步长3 h3h3的隐藏状态取决于模型参数WhxWhx和WhhWhh，最后一个时间步长h2h2的隐藏状态，以及当前时间步长x3x3的输入。

显示三个时间步的RNN模型依赖关系的计算图。框表示变量(未着色)或参数(着色)，圆表示操作符

TODO:MATH

如前所述，图8.7.2中的模型参数是WhxWhx，WhhWhh和WqhWqh。 通常，训练该模型需要针对这些参数∂L/∂Whx∂L/∂Whx，∂L/∂Whh∂L/∂Whh和∂L/∂Wqh∂L/∂Wqh进行梯度计算。 根据图8.7.2中的依赖关系，我们可以沿箭头的相反方向遍历以依次计算和存储梯度。 为了灵活地表达链规则中不同形状的矩阵，向量和标量的乘法，我们将继续使用第4.7节中所述的prodprod运算符。

首先，在任何时间步tt相对于模型输出区分目标函数都非常简单：

TODO:MATH

现在，我们可以在输出层中计算目标函数相对于参数WqhWqh的梯度：∂L/∂Wqh∈Rq×h∂L/∂Wqh∈Rq×h。 基于图8.7.2，目标函数LL通过o1，...，oTo1，...，oT取决于WqhWqh。 使用链式规则产生

TODO:MATH

对于任何时间步长t<Tt< t，它都变得更加棘手，目标函数通过ht+1ht+1和otot依赖于htht。根据链式法则，∂L/∂ht∈Rh∂L/∂ht∈Rh在任何时间步长t<Tt< t的梯度可以递归计算为:

TODO:MATH


为了进行分析，将递归计算扩展到任何时间步长1≤t≤T1≤t≤T，得出


TODO:MATH

从（8.7.15）中我们可以看到，这个简单的线性示例已经显示出长序列模型的一些关键问题：它涉及W⊤hhWhh⊤的潜在非常大的幂。 在其中，小于1的特征值消失，而大于1的特征值发散。 这在数值上是不稳定的，以消失和爆炸的梯度形式表现出来。 解决此问题的一种方法是按照第8.7.1节中的讨论，以计算方便的大小截断时间步长。 实际上，这种截断是通过在给定数量的时间步长后分离梯度来实现的。 稍后我们将看到更复杂的序列模型（例如长短期记忆）如何进一步缓解这种情况。

最后，图8.7.2显示了目标函数LL通过隐藏状态h1，...，hTh1，...，hT依赖于隐藏层中的模型参数WhxWhx和WhhWhh。 为了计算关于这些参数computeL /∂Whx∈Rh×d∂L/∂Whx∈Rh×d和∂L/∂Whh∈Rh×h∂L/∂Whh∈Rh×h的梯度，我们应用链 给出的规则

TODO:MATH

其中，∂L/∂ht由(8。7.13)和(8。7.14)递归计算的∂L/∂ht是影响数值稳定性的关键量。

由于反向传播是反向传播在神经网络中的应用，正如我们在第4.7节中解释的，训练神经网络时交替进行正向传播和反向传播。此外，通过时间反向传播依次计算和存储上述梯度。具体来说，储存的中间值被重复使用以避免重复计算，比如储存∂L/∂ht∂L/∂ht用于计算∂L/∂Whx∂L/∂Whx和∂L/∂Whh∂L/∂Whh。



## 总结

* 时间反向传播只是反向传播应用于具有隐藏状态的序列模型。
* 为了计算方便和数值稳定性，需要截断。
* 矩阵的高次幂可能导致特征值发散和消失。这以爆炸或消失的梯度形式表现出来。
* 为了高效计算，中间值被缓存。

## 练习

1. 假设我们有一个对称矩阵M∈Rn×n，特征值是λi。在不失一般性的前提下，假定它们以升序λi≤λi+1排序。证明MkMk具有特征值λki。
2. 证明对于一个随机向量x∈Rn，Mkx很有可能与MM的最大特征向量vn高度对齐。将此语句形式化。
3. 以上结果对于递归神经网络中的梯度意味着什么？
4. 除了梯度削波，您还能想到其他方法来应对递归神经网络中的梯度爆炸吗？
