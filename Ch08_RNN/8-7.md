

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-19 20:00:08
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-26 17:48:50
 * @Description:MT, improve
 * @TODO::
 * @Reference:https://d2l.ai/chapter_recurrent-neural-networks/bptt.html
-->

# 通过时间的反向传播

到目前为止，我们反复提到了诸如爆炸梯度、消失梯度、截断反螺旋以及需要分离计算图表之类的东西。例如，在上一节中，我们对序列调用了 s.detach ()。为了能够快速构建模型并了解其工作原理，所有这些都没有得到充分的解释。在本节中，我们将更深入地研究序列模型的反向传播的细节，以及这种数学运算的原理和方式。关于随机化和反向传播的更详细的讨论，请参阅[ Tallec & Ollivier，2017]的论文。

当我们第一次实现回归神经网络时，我们遇到了梯度爆炸的一些影响(第8.5节)。特别是，如果你解决了问题集中的问题，你会发现梯度裁剪对于确保正确的收敛是至关重要的。为了更好地理解这个问题，本节将回顾如何为序列模型计算梯度。请注意，它的工作方式在概念上没有什么新的东西。毕竟，我们仍然只是应用链式规则来计算梯度。尽管如此，还是值得再次回顾一下反向传播(4.7节)。

在递归神经网络中的正向传播是相对简单的。时间反向传播算法是反向传播算法在递归神经网络中的一个具体应用。它要求我们一次一个步骤地展开递归神经网络，以获得模型变量和参数之间的依赖关系。然后，基于链式规则，应用反向传播算法计算和存储梯度。由于序列可能相当长，依赖关系可能相当长。例如，对于1000个字符的序列，第一个符号可能对位置1000处的符号有重大影响。这在计算上是不可行的(它需要太长的时间和太多的内存) ，而且在我们达到这个难以捉摸的梯度之前，它需要超过1000个矩阵向量乘积。这是一个充满计算和统计不确定性的过程。下面我们将阐明在实践中会发生什么以及如何解决这个问题。

## 简化的循环网络

我们从一个 RNN 如何工作的简化模型开始。这个模型忽略了关于隐藏状态的细节以及如何更新的细节。这些细节对于分析来说是无关紧要的，只会使符号变得杂乱无章，但是会使它看起来更令人生畏。在这个简化的模型中，我们用 h t t t 表示隐藏状态，x t t t 表示输入，o t 表示时间步 t t 的输出。另外，w h wh 和 w o 分别表示隐状态和输出层的权重。因此，每个时间步骤的隐藏状态和输出可以解释为

因此，我们有一个值链{ ... ，(h t-1，x t-1，o t-1) ，(h t，x t，o t) ，... }{ ... ，(ht-1，xt-1，ot-1) ，(ht，xt，ot) ，... } ，它们通过递归计算相互依赖。正向传播是相当简单的。我们所需要做的就是一次循环完成(xt，ht，ot)(xt，ht，ot)三元组。然后用目标函数作为评价指标，计算出期望目标的输出值与期望目标的偏差

对于反向传播来说，问题就有点棘手了，尤其是当我们根据目标函数 l 的参数来计算梯度时。具体来说，根据链式法则,

导数的第一部分和第二部分很容易计算。在第三部分，我们需要计算出参数对系统的影响，这也是影响系统性能的关键因素。

为了得到上述梯度，假设我们有三个序列{ a t } ，{ b t } ，{ c t }{ at } ，{ bt } ，{ ct }满足 a 0 = 0，a 1 = b 1 a0 = 0，a 1 = b 1 = b 1，a t = b t + c t a t-1 at = bt + ctat-1 for t = 1,2，... t = 1,2，..。然后对于 t ≥1 t ≥1，很容易证明

现在让我们代入(8.7.4)

因此，一个 t = b t + c t a t-1 at = bt + ctat-1成为下面的递归

到(8.7.4)时，第三部分将是

虽然我们可以用链式规则递归地计算环境影响系数，但是只要 t 很大，这个链式就会变得很长。让我们讨论一些处理这个问题的策略。

计算总和。这是非常缓慢和梯度可以爆炸，因为微妙的变化，在初始条件可以潜在地影响结果很多。也就是说，我们可以看到类似于蝴蝶效应的情况，即初始条件的最小变化导致结果的不成比例的变化。就我们想要估计的模型而言，这实际上是相当不合适的。毕竟，我们正在寻找能够很好地推广的稳健估计。因此，这种策略几乎从未在实践中使用过。

在步骤之后截断求和。这就是我们到目前为止一直在讨论的。这导致了一个真正梯度的近似值，只需要将上面的求和终止在环境影响系数为-的基础上。因此，这个逼近误差是由基于分布式环境的两岸平衡系数 f (x t，h t-1，w) w h t-1 x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x。实际上，这种方法很有效。这就是通常所说的截短 BPTT (通过时间的倒退)。这样做的结果之一是，这种模式主要侧重于短期影响，而不是长期后果。这实际上是可取的，因为它使估计偏向于更简单和更稳定的模型。

随机截断。最后我们可以把∂whht∂whht替换成一个随机变量它在期望中是正确的但是它截断了序列。这是通过使用一个序列的ξtξt E(ξt) = 1 E(ξt) = 1, P(ξt = 0) = 1−πP(ξt = 0) = 1−π而且P(ξt =π−1)=πP(ξt =π−1)=π。我们用这个来替换梯度:

它遵循从ξ的定义tξt E [zt型]=∂whtE (zt型)=∂什么。每当ξt = 0ξt = 0扩大终止。这导致了变长序列的加权和，其中长序列很少，但适当地增加了权重。[Tallec & Ollivier, 2017]在他们的论文中提出了这一点。不幸的是，尽管理论上很有吸引力，但这种模型并不比简单的截断效果好多少，这很可能是由于许多因素造成的。首先，在向过去执行了许多反向传播步骤之后，观察到的结果足以在实践中捕获依赖关系。其次，增加的方差抵消了梯度更准确的事实。第三，我们实际上想要的模型只有一个短范围的互动。因此，BPTT具有令人满意的轻微的正则化效果。

图8.7.1从上到下:随机化的BPTT、规则截断的BPTT和完整的BPTT实现

图8.7.1给出了分析时间机器的前几个单词的三种情况:*第一行是随机截断，将文本分割成不同长度的段。*第二行是常规截断的BPTT，它将其分解为相同长度的序列。*第三行是导致计算上不可行的表达式的完整BPTT。

## 计算图

为了使递归神经网络计算过程中模型变量与参数之间的依赖关系可视化，我们可以绘制模型计算图，如图8.7.2所示。例如，时间步长3 h3h3的隐藏状态的计算取决于模型参数WhxWhx和WhhWhh，最后一个时间步长h2h2的隐藏状态，以及当前时间步长x3x3的输入。

## BPTT详细

在讨论了一般原则之后，让我们来详细讨论BPTT。将WW分解为不同的权矩阵集(Whx,WhhWhx,Whh, WohWoh)，得到简单的线性潜变量模型:

根据第4.7节的讨论，我们计算出

在l(⋅)表示被选中的损失函数。对WohWoh求导很简单



其中prod（⋅）prod（⋅）表示两个或多个矩阵的乘积。

对WhxWhx和WhhWhh的依赖关系比较棘手，因为它涉及到一连串的导数。 我们从

毕竟，隐藏状态相互依赖，也依赖于过去的输入。关键数量是过去的隐藏状态如何影响未来的隐藏状态。


TODO:MATH

这种潜在的非常令人恐惧的表达有许多原因。 首先，它需要存储中间结果，即在我们通过损失函数LL的方式工作时的Whh的幂。 其次，这个简单的线性示例已经显示出长序列模型的一些关键问题：它涉及潜在的非常大的幂Wjhh。 在其中，小于1的特征值对于大j消失，而大于1的特征值发散。 这在数值上是不稳定的，并且对可能不相关的过去细节给予了不适当的重视。 解决此问题的一种方法是以计算方便的大小截断和。 在第9节的后面，我们将看到更复杂的序列模型（例如LSTM）如何进一步减轻这种情况。 实际上，这种截断是通过在给定数量的步骤后分离梯度来实现的。

## 总结

* 时间反向传播只是反向传播应用于具有隐藏状态的序列模型。
* 为了计算方便和数值稳定性，需要截断。
* 矩阵的高次幂可能导致特征值发散和消失。这以爆炸或消失的梯度形式表现出来。
* 为了高效计算，中间值被缓存。

## 练习

1. 假设我们有一个对称矩阵M∈Rn×n，特征值是λi。在不失一般性的前提下，假定它们以升序λi≤λi+1排序。证明MkMk具有特征值λki。
2. 证明对于一个随机向量x∈Rn，Mkx很有可能与MM的最大特征向量vn高度对齐。将此语句形式化。
3. 以上结果对于递归神经网络中的梯度意味着什么？
4. 除了梯度削波，您还能想到其他方法来应对递归神经网络中的梯度爆炸吗？
