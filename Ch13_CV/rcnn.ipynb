{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "name": "rcnn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy770CeYUQFH"
      },
      "source": [
        "The following additional libraries are needed to run this\n",
        "notebook. Note that running on Colab is experimental, please report a Github\n",
        "issue if you have any problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QswZlx1EUXQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e631e621-72f6-445c-f1a4-d40927e080fb"
      },
      "source": [
        "!pip install -U git+https://github.com/d2l-ai/d2l-en.git@master"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/d2l-ai/d2l-en.git@master\n",
            "  Cloning https://github.com/d2l-ai/d2l-en.git (to revision master) to /tmp/pip-req-build-8nrpehyh\n",
            "  Running command git clone -q https://github.com/d2l-ai/d2l-en.git /tmp/pip-req-build-8nrpehyh\n",
            "Requirement already satisfied, skipping upgrade: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.15.1) (1.1.4)\n",
            "Requirement already satisfied, skipping upgrade: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.7.7)\n",
            "Requirement already satisfied, skipping upgrade: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.6.1)\n",
            "Requirement already satisfied, skipping upgrade: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (7.5.1)\n",
            "Requirement already satisfied, skipping upgrade: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.3.1)\n",
            "Requirement already satisfied, skipping upgrade: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (4.10.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.15.1) (5.2.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.15.1) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.15.1) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (2.6.1)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-client>=4.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (5.3.5)\n",
            "Requirement already satisfied, skipping upgrade: traitlets in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.3.3)\n",
            "Requirement already satisfied, skipping upgrade: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (20.0.0)\n",
            "Requirement already satisfied, skipping upgrade: jupyter-core in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.15.1) (4.7.0)\n",
            "Requirement already satisfied, skipping upgrade: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (2.11.2)\n",
            "Requirement already satisfied, skipping upgrade: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (5.0.8)\n",
            "Requirement already satisfied, skipping upgrade: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.3)\n",
            "Requirement already satisfied, skipping upgrade: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.4.4)\n",
            "Requirement already satisfied, skipping upgrade: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.15.1) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (5.5.0)\n",
            "Requirement already satisfied, skipping upgrade: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.15.1) (3.5.1)\n",
            "Requirement already satisfied, skipping upgrade: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (5.1.1)\n",
            "Requirement already satisfied, skipping upgrade: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (0.9.1)\n",
            "Requirement already satisfied, skipping upgrade: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.15.1) (1.5.0)\n",
            "Requirement already satisfied, skipping upgrade: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.15.1) (1.0.18)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->d2l==0.15.1) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets->qtconsole->jupyter->d2l==0.15.1) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->d2l==0.15.1) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (0.5.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.15.1) (20.4)\n",
            "Requirement already satisfied, skipping upgrade: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->d2l==0.15.1) (2.6.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (4.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.15.1) (0.7.5)\n",
            "Requirement already satisfied, skipping upgrade: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.15.1) (0.6.0)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.15.1) (0.2.5)\n",
            "Building wheels for collected packages: d2l\n",
            "  Building wheel for d2l (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d2l: filename=d2l-0.15.1-cp36-none-any.whl size=64775 sha256=0bdf260f9df53ae422b4a24d91398a66281ba59496fba2b8624a8974981c5912\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-va3townr/wheels/0f/41/8f/72ece70ede8a0e37eec72c03087eb4604925ba212b804f8cad\n",
            "Successfully built d2l\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 0,
        "id": "isEyAYfPUQFI"
      },
      "source": [
        "# Region-based CNNs (R-CNNs)\n",
        "\n",
        "\n",
        "Region-based convolutional neural networks or regions with CNN features (R-CNNs)\n",
        "are a pioneering approach that applies deep models to object detection\n",
        ":cite:`Girshick.Donahue.Darrell.ea.2014`. In this section, we will discuss\n",
        "R-CNNs and a series of improvements made to them: Fast R-CNN\n",
        ":cite:`Girshick.2015`, Faster R-CNN :cite:`Ren.He.Girshick.ea.2015`, and Mask R-CNN\n",
        ":cite:`He.Gkioxari.Dollar.ea.2017`. Due to space limitations, we will confine\n",
        "our discussion to the designs of these models.\n",
        "\n",
        "\n",
        "## R-CNNs\n",
        "\n",
        "R-CNN models first select several proposed regions from an image (for example,\n",
        "anchor boxes are one type of selection method) and then label their categories\n",
        "and bounding boxes (e.g., offsets). Then, they use a CNN to perform forward\n",
        "computation to extract features from each proposed area. Afterwards, we use the\n",
        "features of each proposed region to predict their categories and bounding\n",
        "boxes. :numref:`fig_r-cnn` shows an R-CNN model.\n",
        "\n",
        "![R-CNN model. ](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/r-cnn.svg?raw=1)\n",
        ":label:`fig_r-cnn`\n",
        "\n",
        "Specifically, R-CNNs are composed of four main parts:\n",
        "\n",
        "1. Selective search is performed on the input image to select multiple\n",
        "   high-quality proposed regions\n",
        "   :cite:`Uijlings.Van-De-Sande.Gevers.ea.2013`. These proposed regions are\n",
        "   generally selected on multiple scales and have different shapes and\n",
        "   sizes. The category and ground-truth bounding box of each proposed region is\n",
        "   labeled.\n",
        "1. A pre-trained CNN is selected and placed, in truncated form, before the\n",
        "   output layer. It transforms each proposed region into the input dimensions\n",
        "   required by the network and uses forward computation to output the features\n",
        "   extracted from the proposed regions.\n",
        "1. The features and labeled category of each proposed region are combined as an\n",
        "   example to train multiple support vector machines for object\n",
        "   classification. Here, each support vector machine is used to determine\n",
        "   whether an example belongs to a certain category.\n",
        "1. The features and labeled bounding box of each proposed region are combined as\n",
        "   an example to train a linear regression model for ground-truth bounding box\n",
        "   prediction.\n",
        "\n",
        "Although R-CNN models use pre-trained CNNs to effectively extract image\n",
        "features, the main downside is the slow speed. As you can imagine, we can select\n",
        "thousands of proposed regions from a single image, requiring thousands of\n",
        "forward computations from the CNN to perform object detection. This massive\n",
        "computing load means that R-CNNs are not widely used in actual applications.\n",
        "\n",
        "\n",
        "## Fast R-CNN\n",
        "\n",
        "The main performance bottleneck of an R-CNN model is the need to independently\n",
        "extract features for each proposed region. As these regions have a high degree\n",
        "of overlap, independent feature extraction results in a high volume of\n",
        "repetitive computations. Fast R-CNN improves on the R-CNN by only performing CNN\n",
        "forward computation on the image as a whole.\n",
        "\n",
        "![Fast R-CNN model. ](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/fast-rcnn.svg?raw=1)\n",
        ":label:`fig_fast_r-cnn`\n",
        "\n",
        ":numref:`fig_fast_r-cnn` shows a Fast R-CNN model. It is primary computation\n",
        "steps are described below:\n",
        "\n",
        "1. Compared to an R-CNN model, a Fast R-CNN model uses the entire image as the\n",
        "   CNN input for feature extraction, rather than each proposed region. Moreover,\n",
        "   this network is generally trained to update the model parameters. As the\n",
        "   input is an entire image, the CNN output shape is $1 \\times c \\times h_1\n",
        "   \\times w_1$.\n",
        "1. Assuming selective search generates $n$ proposed regions, their different\n",
        "   shapes indicate regions of interests (RoIs) of different shapes on the CNN\n",
        "   output. Features of the same shapes must be extracted from these RoIs (here\n",
        "   we assume that the height is $h_2$ and the width is $w_2$). Fast R-CNN\n",
        "   introduces RoI pooling, which uses the CNN output and RoIs as input to output\n",
        "   a concatenation of the features extracted from each proposed region with the\n",
        "   shape $n \\times c \\times h_2 \\times w_2$.\n",
        "1. A fully connected layer is used to transform the output shape to $n \\times\n",
        "   d$, where $d$ is determined by the model design.\n",
        "1. During category prediction, the shape of the fully connected layer output is\n",
        "   again transformed to $n \\times q$ and we use softmax regression ($q$ is the\n",
        "   number of categories). During bounding box prediction, the shape of the fully\n",
        "   connected layer output is again transformed to $n \\times 4$. This means that\n",
        "   we predict the category and bounding box for each proposed region.\n",
        "\n",
        "The RoI pooling layer in Fast R-CNN is somewhat different from the pooling\n",
        "layers we have discussed before. In a normal pooling layer, we set the pooling\n",
        "window, padding, and stride to control the output shape. In an RoI pooling\n",
        "layer, we can directly specify the output shape of each region, such as\n",
        "specifying the height and width of each region as $h_2, w_2$. Assuming that the\n",
        "height and width of the RoI window are $h$ and $w$, this window is divided into\n",
        "a grid of sub-windows with the shape $h_2 \\times w_2$. The size of each\n",
        "sub-window is about $(h/h_2) \\times (w/w_2)$. The sub-window height and width\n",
        "must always be integers and the largest element is used as the output for a\n",
        "given sub-window. This allows the RoI pooling layer to extract features of the\n",
        "same shape from RoIs of different shapes.\n",
        "\n",
        "In :numref:`fig_roi`, we select an $3\\times 3$ region as an RoI of the $4 \\times\n",
        "4$ input. For this RoI, we use a $2\\times 2$ RoI pooling layer to obtain a\n",
        "single $2\\times 2$ output. When we divide the region into four sub-windows, they\n",
        "respectively contain the elements 0, 1, 4, and 5 (5 is the largest); 2 and 6 (6\n",
        "is the largest); 8 and 9 (9 is the largest); and 10.\n",
        "\n",
        "![$2\\times 2$ RoI pooling layer. ](http://d2l.ai/_images/roi.svg)\n",
        ":label:`fig_roi`\n",
        "\n",
        "We use the `ROIPooling` function to demonstrate the RoI pooling layer computation. Assume that the CNN extracts the feature `X` with both a height and width of 4 and only a single channel.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "4"
        },
        "origin_pos": 1,
        "tab": [
          "mxnet"
        ],
        "id": "4Yzv3JCPUQFJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dd44f53-9b45-4e6b-e91f-e1e79b5c728a"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "X = torch.arange(16.).reshape(1, 1, 4, 4)\n",
        "X"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 0.,  1.,  2.,  3.],\n",
              "          [ 4.,  5.,  6.,  7.],\n",
              "          [ 8.,  9., 10., 11.],\n",
              "          [12., 13., 14., 15.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 2,
        "id": "C-zhVRCIUQFL"
      },
      "source": [
        "Assume that the height and width of the image are both 40 pixels and that selective search generates two proposed regions on the image. Each region is expressed as five elements: the region's object category and the $x, y$ coordinates of its upper-left and bottom-right corners.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "5"
        },
        "origin_pos": 3,
        "tab": [
          "mxnet"
        ],
        "id": "WdwK9TKjUQFL"
      },
      "source": [
        "rois = torch.Tensor([[0, 0, 0, 20, 20], [0, 0, 10, 30, 30]])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 4,
        "id": "RshbEzmcUQFL"
      },
      "source": [
        "Because the height and width of `X` are $1/10$ of the height and width of the image, the coordinates of the two proposed regions are multiplied by 0.1 according to the `spatial_scale`, and then the RoIs are labeled on `X` as `X[:, :, 0:3, 0:3]` and `X[:, :, 1:4, 0:4]`, respectively. Finally, we divide the two RoIs into a sub-window grid and extract features with a height and width of 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "attributes": {
          "classes": [],
          "id": "",
          "n": "6"
        },
        "origin_pos": 5,
        "tab": [
          "mxnet"
        ],
        "id": "qbxAUXWeUQFL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c05b27-6bcc-4f61-c95c-acee029d56c9"
      },
      "source": [
        "torchvision.ops.roi_pool(X, rois, output_size=(2, 2), spatial_scale=0.1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[ 5.,  6.],\n",
              "          [ 9., 10.]]],\n",
              "\n",
              "\n",
              "        [[[ 9., 11.],\n",
              "          [13., 15.]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 6,
        "id": "L8hz3I08UQFL"
      },
      "source": [
        "## Faster R-CNN\n",
        "\n",
        "In order to obtain precise object detection results, Fast R-CNN generally requires that many proposed regions be generated in selective search. Faster R-CNN replaces selective search with a region proposal network. This reduces the number of proposed regions generated, while ensuring precise object detection.\n",
        "\n",
        "\n",
        "![Faster R-CNN model. ](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/faster-rcnn.svg?raw=1)\n",
        ":label:`fig_faster_r-cnn`\n",
        "\n",
        "\n",
        ":numref:`fig_faster_r-cnn` shows a Faster R-CNN model. Compared to Fast R-CNN,\n",
        "Faster R-CNN only changes the method for generating proposed regions from\n",
        "selective search to region proposal network. The other parts of the model remain\n",
        "unchanged. The detailed region proposal network computation process is described\n",
        "below:\n",
        "\n",
        "1. We use a $3\\times 3$ convolutional layer with a padding of 1 to transform the\n",
        "   CNN output and set the number of output channels to $c$. This way, each\n",
        "   element in the feature map the CNN extracts from the image is a new feature\n",
        "   with a length of $c$.\n",
        "1. We use each element in the feature map as a center to generate multiple\n",
        "   anchor boxes of different sizes and aspect ratios and then label them.\n",
        "1. We use the features of the elements of length $c$ at the center on the anchor\n",
        "   boxes to predict the binary category (object or background) and bounding box\n",
        "   for their respective anchor boxes.\n",
        "1. Then, we use non-maximum suppression to remove similar bounding box results\n",
        "   that correspond to category predictions of \"object\". Finally, we output the\n",
        "   predicted bounding boxes as the proposed regions required by the RoI pooling\n",
        "   layer.\n",
        "\n",
        "\n",
        "It is worth noting that, as a part of the Faster R-CNN model, the region\n",
        "proposal network is trained together with the rest of the model. In addition,\n",
        "the Faster R-CNN object functions include the category and bounding box\n",
        "predictions in object detection, as well as the binary category and bounding box\n",
        "predictions for the anchor boxes in the region proposal network. Finally, the\n",
        "region proposal network can learn how to generate high-quality proposed regions,\n",
        "which reduces the number of proposed regions while maintaining the precision of\n",
        "object detection.\n",
        "\n",
        "\n",
        "## Mask R-CNN\n",
        "\n",
        "If training data is labeled with the pixel-level positions of each object in an image, a Mask R-CNN model can effectively use these detailed labels to further improve the precision of object detection.\n",
        "\n",
        "![Mask R-CNN model. ](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/mask-rcnn.svg?raw=1)\n",
        ":label:`fig_mask_r-cnn`\n",
        "\n",
        "As shown in :numref:`fig_mask_r-cnn`, Mask R-CNN is a modification to the Faster\n",
        "R-CNN model. Mask R-CNN models replace the RoI pooling layer with an RoI\n",
        "alignment layer. This allows the use of bilinear interpolation to retain spatial\n",
        "information on feature maps, making Mask R-CNN better suited for pixel-level\n",
        "predictions. The RoI alignment layer outputs feature maps of the same shape for\n",
        "all RoIs. This not only predicts the categories and bounding boxes of RoIs, but\n",
        "allows us to use an additional fully convolutional network to predict the\n",
        "pixel-level positions of objects. We will describe how to use fully\n",
        "convolutional networks to predict pixel-level semantics in images later in this\n",
        "chapter.\n",
        "\n",
        "\n",
        "\n",
        "## Summary\n",
        "\n",
        "* An R-CNN model selects several proposed regions and uses a CNN to perform\n",
        "  forward computation and extract the features from each proposed region. It\n",
        "  then uses these features to predict the categories and bounding boxes of\n",
        "  proposed regions.\n",
        "* Fast R-CNN improves on the R-CNN by only performing CNN forward computation on\n",
        "  the image as a whole. It introduces an RoI pooling layer to extract features\n",
        "  of the same shape from RoIs of different shapes.\n",
        "* Faster R-CNN replaces the selective search used in Fast R-CNN with a region\n",
        "  proposal network. This reduces the number of proposed regions generated, while\n",
        "  ensuring precise object detection.\n",
        "* Mask R-CNN uses the same basic structure as Faster R-CNN, but adds a fully\n",
        "  convolution layer to help locate objects at the pixel level and further\n",
        "  improve the precision of object detection.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Study the implementation of each model in the [GluonCV toolkit](https://github.com/dmlc/gluon-cv/) related to this section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "origin_pos": 7,
        "tab": [
          "mxnet"
        ],
        "id": "migkaAFGUQFL"
      },
      "source": [
        "[Discussions](https://discuss.d2l.ai/t/374)\n"
      ]
    }
  ]
}