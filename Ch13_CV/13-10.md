

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-30 19:25:26
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-30 19:34:38
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_computer-vision/transposed-conv.html
-->

# 转置卷积

到目前为止，我们介绍的卷积神经网络层，包括卷积层(第6.2节)和汇聚层(第6.5节) ，通常减少输入宽度和高度，或保持不变。但是，诸如语义分割(13.9节)和生成对抗性网络(17.2节)等应用程序需要预测每个像素的值，因此需要增加输入宽度和高度。转置卷积，也称为分数条纹卷积[ Dumoulin & Visin，2016]或去卷积[ Long 等人，2015] ，就是为了达到这个目的。

## 基本二维转置卷积

让我们考虑一个基本情况，输入和输出通道都是1,0 padding 和1 stride。图13.10.1演示了如何在2222输入矩阵上计算带有2222核的转置卷积。

图13.10.1带有2222核的对置卷积层。

我们可以通过给出矩阵核 k 和矩阵输入 x 来实现这一操作。

记住 y [ i，j ] = (x [ i: i + h，j: j + w ] * k)的卷积计算结果。Sum ()(参见6.2节中的 corr2d) ，它总结了通过内核输入的值。而转置的卷积则通过核广播输入值，从而产生更大的输出形状。

验证图13.10.1中的结果。

TODO:CODE

或者我们可以使用 nn。Conv2DTranspose 以获得相同的结果。当然。对于 Conv2D，输入和内核都应该是4d 张量。

然后通过适当的整形，用矩阵乘法实现卷积运算。

TODO:CODE

## 填充、阔步和通道

我们将填充元素应用于卷积中的输入，而将它们应用于转置卷积中的输出。1111填充意味着我们首先像平常一样计算输出，然后删除第一行/最后一行和列。

TODO:CODE

同样，跨步也适用于输出。

TODO:CODE

转置卷积的多通道扩展和卷积是一样的。当输入有多个通道(记cici)时，转置卷积为每个输入通道分配一个kh×kwkh×kw核矩阵。如果输出有一个通道大小coco，那么每个输出通道都有一个ci×kh×kwci×kh×kw内核。

结果,如果我们饲料XX卷积层ff计算Y = f (X) Y = f (X)和创建一个转置卷积层gg hyperparameters一样ff除了输出通道设置为XX的通道大小,那么g (Y) g (Y)应该具有相同的形状是XX。让我们来验证一下这个说法

TODO:CODE

## 类比矩阵转置

转置卷积因矩阵转置而得名。实际上，卷积运算也可以通过矩阵乘法来实现。在下面的示例中，我们定义3×3×输入XX和2×22×2内核KK，然后使用corr2d计算卷积输出。

TODO:CODE

接下来，我们将卷积核KK重写为矩阵WW。它的形状将是(4,9)(4,9)，其中ithith行对输入应用内核生成ithith输出元素。

TODO:CODE

然后，可以通过具有适当整形的矩阵乘法来实现卷积算符。

TODO:CODE

通过重用kernel2matrix，我们也可以将转置卷积实现为矩阵乘法。要重用生成的WW，我们构造一个2×22×2输入，因此相应的权重矩阵将具有形状（9,4）（9,4），即W⊤W⊤。让我们验证结果。

TODO:CODE

## 小结

* 与通过核减少输入的卷积相比，转置卷积广播输入。
* 如果卷积层将输入宽度和高度分别降低nwnw和hhhh时间。然后在核大小、填充和步长相同的情况下，对卷积层进行置位，输入宽度和高度将分别增加nw和nh
* 我们可以通过矩阵乘法来实现卷积运算，相应的转置卷积也可以通过矩阵乘法来实现。

## 练习

1. 用矩阵乘法来实现卷积运算有效吗?为什么?
