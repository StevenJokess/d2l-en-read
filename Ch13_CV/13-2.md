

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-14 22:40:22
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-14 22:56:01
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_computer-vision/fine-tuning.html
-->

# 微调

在前面的章节中，我们讨论了如何在只有60,000张图像的 Fashion-MNIST 训练数据集上训练模特。我们还描述了 ImageNet，学术界使用最广泛的大规模图像数据集，拥有超过1000种类型的超过1000万张图像和对象。然而，我们经常处理的数据集的大小通常比第一个大，但比第二个小。

假设我们想要在图像中识别不同种类的椅子，然后将购买链接推送给用户。一种可能的方法是先找到一百张常见的椅子，对每张椅子从不同的角度拍摄一千张不同的图像，然后在收集到的图像数据集上训练一个分类模型。虽然这个数据集可能比 Fashion-MNIST 大，但是例子的数量仍然不到 imageet 的十分之一。这可能会导致适用于 ImageNet 的复杂模型在这个数据集上的过度拟合。同时，由于数据量有限，最终训练的模型的精度可能不能满足实际要求。

为了解决上述问题，一个显而易见的解决办法是收集更多的数据。然而，收集和标记数据会消耗大量的时间和金钱。例如，为了收集 ImageNet 数据集，研究人员已经花费了数百万美元的研究资金。尽管最近数据采集成本大幅下降，但成本仍不容忽视。

另一个解决方案是应用迁移学习将从源数据集学到的知识迁移到目标数据集。例如，尽管 ImageNet 中的图像大部分与椅子无关，但在这个数据集上训练的模型可以提取更多的一般图像特征，这些特征可以帮助识别边缘、纹理、形状和对象组合。这些相似的特征对于识别椅子也同样有效。

在本节中，我们介绍迁移学习中的一种常用技术: 微调。如图13.2.1所示，微调包括以下四个步骤:

1. 在源数据集(例如 ImageNet 数据集)上预先训练一个神经网络模型，例如源模型。
2. 创建一个新的神经网络模型，即目标模型。这将在源模型上复制所有模型设计及其参数，但输出层除外。我们假设这些模型参数包含了从源数据集中学到的知识，并且这些知识同样适用于目标数据集。我们还假设源模型的输出层与源数据集的标签密切相关，因此不用于目标模型。
3. 添加一个输出层，其输出大小为目标数据集类别的数量到目标模型，并随机初始化该层的模型参数。
4. 在目标数据集(如椅子数据集)上训练目标模型。我们将从头开始训练输出层，同时根据源模型的参数对所有剩余层的参数进行微调。


然后，我们建立一个新的神经网络用作目标模型。它的定义方式与预训练的源模型相同，但是输出的最终数量等于目标数据集中的类别数量。在下面的代码中，目标模型实例finetune_net的成员变量特征中的模型参数被初始化为源模型的相应层的模型参数。由于特征中的模型参数是通过对ImageNet数据集进行预训练而获得的，因此足够好。因此，我们通常只需要使用小的学习率来“微调”这些参数。相反，成员变量输出中的模型参数是随机初始化的，通常需要更高的学习率才能从头开始学习。假设Trainer实例中的学习速率为ηη，并使用10η10η的学习速率来更新成员变量输出中的模型参数。

TODO:CODE

然后，我们建立一个新的神经网络用作目标模型。它的定义方式与预训练的源模型相同，但是输出的最终数量等于目标数据集中的类别数量。在下面的代码中，目标模型实例finetune_net的成员变量特征中的模型参数被初始化为源模型的相应层的模型参数。由于特征中的模型参数是通过对ImageNet数据集进行预训练而获得的，因此足够好。因此，我们通常只需要使用小的学习率来“微调”这些参数。相反，成员变量输出中的模型参数是随机初始化的，通常需要更高的学习率才能从头开始学习。假设Trainer实例中的学习速率为ηη，并使用10η10η的学习速率来更新成员变量输出中的模型参数。

TODO:CODE

## 微调模型

我们首先定义一个训练函数train_fine_tuning，它使用微调，因此可以多次调用。

我们将Trainer实例中的学习率设置为较小的值（例如0.01），以便微调在预训练中获得的模型参数。基于之前的设置，我们将使用十倍的学习率从头开始训练目标模型的输出层参数。

为了进行比较，我们定义了一个相同的模型，但是将其所有模型参数初始化为随机值。由于需要从头开始训练整个模型，因此我们可以使用更大的学习率。

TODO:CODE

如您所见，由于参数的初始值更好，因此经过微调的模型倾向于在同一时期获得更高的精度。

## 小结

* 迁移学习将学习的知识从源数据集迁移到目标数据集。微调是用于迁移学习的常用技术。
* 目标模型将在源模型上复制所有模型设计及其参数（输出层除外），并根据目标数据集对这些参数进行微调。相反，需要从头开始训练目标模型的输出层。
* 通常，微调参数使用较小的学习速率，而从头开始训练输出层可以使用较大的学习速率。

## 总结

1. 不断提高finetune_net的学习率。模型的精度是如何变化的?
1. 在比较实验中进一步调优finetune_net和scratch_net的超参数。它们仍然有不同的精确度吗?
1. 在finetune_net中设置参数。在训练过程中不要更新源模型的参数。会发生什么呢?您可以使用以下代码。
1. 实际上，ImageNet数据集中还有一个“热狗”类。可以使用以下代码获得其在输出层上的相应权重参数。我们如何使用此参数？
