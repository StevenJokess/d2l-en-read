

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-28 16:01:54
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-14 21:31:19
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_multilayer-perceptrons/numerical-stability-and-init.html
 *https://github.com/d2l-ai/d2l-en/edit/master/chapter_multilayer-perceptrons/numerical-stability-and-init.md
-->

# 数值稳定性和初始化
:label:`sec_numerical_stability`

到目前为止，我们实现的每个模型都要求我们根据一些预先指定的分布来初始化它的参数。到目前为止，我们一直认为初始化方案是理所当然的，忽略了这些选择是如何做出的细节。你甚至可能会觉得这些选择并不是特别重要。相反，初始化方案的选择在神经网络学习中起着重要的作用，并且对于维护神经网络数值稳定性是至关重要的。此外，这些选择可以用有趣的方式与非线性激活函数的选择联系在一起。我们选择的函数和参数的初始化方式决定了优化算法收敛的速度。在这里，糟糕的选择会导致我们在训练中遇到爆炸或消失的梯度。在本节中，我们将更详细地深入探讨这些主题，并讨论一些有用的启发法，你会发现这些启发法在你的职业生涯中对深度学习很有用。

## 梯度消失和梯度爆发

考虑一个具有 $L$ 层、输入 $\mathbf{x}$ 和输出 $\mathbf{o}$ 的深层网络。每个层 $l$ 由一个参数化为 $\mathbf{W}^{(l)}$的变换$f_l$来定义，其隐变量为 $\mathbf{h}^{(l)}$(设 $\mathbf{h}^{(0)} = \mathbf{x}$) ，我们的网络可以表示为:

$$\mathbf{h}^{(l)} = f_l (\mathbf{h}^{(l-1)}) \text{ and thus } \mathbf{o} = f_L \circ \ldots \circ f_1(\mathbf{x}).$$

如果所有的隐变量和输入都是向量，我们可以把 o 对任意一组参数 w (l) w (l)的梯度写成如下:

$$\partial_{\mathbf{W}^{(l)}} \mathbf{o} = \underbrace{\partial_{\mathbf{h}^{(L-1)}} \mathbf{h}^{(L)}}_{ \mathbf{M}^{(L)} \stackrel{\mathrm{def}}{=}} \cdot \ldots \cdot \underbrace{\partial_{\mathbf{h}^{(l)}} \mathbf{h}^{(l+1)}}_{ \mathbf{M}^{(l+1)} \stackrel{\mathrm{def}}{=}} \underbrace{\partial_{\mathbf{W}^{(l)}} \mathbf{h}^{(l)}}_{ \mathbf{v}^{(l)} \stackrel{\mathrm{def}}{=}}.$$

换句话说，这个梯度是 l-l l-l 矩阵 m (l) ... ... 泊可 m (l + 1) m (l) ... ... 泊可 m (l + 1)和梯度向量 v (l) v (l)的乘积。因此，我们很容易受到同样的数字底流问题的影响，这些问题经常出现在太多概率相乘的时候。在处理概率时，一个常用的技巧是切换到对数空间，即将压力从尾数转移到数字表示的指数。不幸的是，我们上面的问题更严重: 最初矩阵 m (l) m (l)可能有各种各样的特征值。他们可能是小的或大的，他们的产品可能是非常大或非常小。

不稳定的渐变所带来的风险超出了数值表示。不可预知的梯度也威胁到我们的优化算法的稳定性。我们可能面临的参数更新(i)过大，破坏了我们的模型(爆炸梯度问题) ; 或者(ii)过小(消失梯度问题) ，使学习变得不可能，因为参数几乎不会在每次更新中移动。

## 梯度消失

导致渐变消失问题的一个常见原因是在每个图层的线性操作之后附加的激活函数的选择。历史上，S形函数$1/(1 + \exp(-x))$(引入:numref:`sec_mlp`)很流行，因为它类似于阈值函数。由于早期的人工神经网络是受到生物神经网络的启发，神经元要么完全放电，要么完全不放电(如生物神经元)的想法似乎很有吸引力。让我们仔细看看乙状结肠，看看为什么它可以导致消失梯度。

TODO:CODE

正如你所看到的，乙状结肠的梯度消失时，它的输入是大的，当他们是小的。此外，当反向传播通过许多层时，除非我们在适居带，在那里许多信号的输入接近于零，整个积的梯度可能消失。当我们的网络拥有许多层，除非我们小心，梯度可能会被切断在一些层。事实上，这个问题曾经困扰着深度网络训练。因此，ReLUs，这是更稳定(但神经似乎不太可信) ，已成为默认的选择从业者。

## 梯度爆炸

另一个相反的问题，当梯度爆发时，也同样令人烦恼。为了更好地说明这一点，我们绘制了100个高斯随机矩阵，并将它们与一些初始矩阵相乘。对于我们选择的尺度(方差 $\sigma^2=1$) ，矩阵乘积是爆炸的。当由于深度网络的初始化而发生这种情况时，我们就没有机会让梯度下降法优化器收敛。

TODO:CODE

## 打破了对称

神经网络设计中的另一个问题是参数化的对称性。假设我们有一个简单的MLP，它有一个隐藏层和两个单位。在这种情况下，我们可以对第一层的权值W(1)W(1)进行排列，同样也可以对输出层的权值进行排列，从而得到相同的函数。区分第一个隐藏单位和第二个隐藏单位没有什么特别的。换句话说，我们在每一层的隐藏单位之间有排列对称。

这不仅仅是理论上的麻烦。考虑前面提到的带有两个隐藏单位的单层MLP。举例来说，假设输出层将两个隐藏单元转换为仅一个输出单元。想象一下，如果我们初始化隐藏层的所有参数，对于某个常数cc, W(1)=cW(1)=c会发生什么。在这种情况下，在正向传播期间，任何一个隐藏单元都接受相同的输入和参数，产生相同的激活，并将激活反馈给输出单元。在反向传播过程中，根据参数W(1)W(1)对输出单元进行微分，得到一个各元素都取相同值的梯度。因此，在基于梯度的迭代(如小批量随机梯度下降)后，W(1)W(1)的所有元素仍然取相同的值。这样的迭代本身不会打破对称性，我们也可能永远无法实现网络的表达能力。隐藏层会表现得好像它只有一个单元。注意，虽然小批量随机梯度下降不会打破这种对称性，dropout正则化会!

## 参数初始化

解决(或至少减轻)上述问题的一种方法是通过谨慎的初始化。在优化过程中附加的注意和适当的正则化可以进一步增强稳定性。

### 默认初始化

在前面的章节中，例如在3.3章节中，我们使用正态分布来初始化权重值。如果我们不指定初始化方法，框架将使用默认的随机初始化方法，在实际中，这种方法对于中等规模的问题通常很有效。

### Xavier初始化
:label:`subsec_xavier`

让我们来看一些没有非线性的全连接层的输出（例如，隐藏变量）$o_{i}$的比例分布。 对于此层，使用$n_\mathrm{in}$输入$x_j$及其关联的权重$w_{ij}$ ，输出为

$$o_{i} = \sum_{j=1}^{n_\mathrm{in}} w_{ij} x_j.$$

权重wijwij均独立于同一分布绘制。 此外，让我们假设该分布的均值为零且方差为σ2σ2。 注意，这并不意味着分布必须是高斯分布，仅意味着均值和方差必须存在。 现在，让我们假设xjxj层的输入也具有零均值和方差γ2γ2，并且它们独立于wijwij且彼此独立。 在这种情况下，我们可以如下计算oioi的均值和方差：

$$
\begin{aligned}
    E[o_i] & = \sum_{j=1}^{n_\mathrm{in}} E[w_{ij} x_j] \\&= \sum_{j=1}^{n_\mathrm{in}} E[w_{ij}] E[x_j] \\&= 0, \\
    \mathrm{Var}[o_i] & = E[o_i^2] - (E[o_i])^2 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij} x^2_j] - 0 \\
        & = \sum_{j=1}^{n_\mathrm{in}} E[w^2_{ij}] E[x^2_j] \\
        & = n_\mathrm{in} \sigma^2 \gamma^2.
\end{aligned}
$$

保持方差固定的一种方法是设置nin jsx2 =1nin jsx2 =1。现在考虑反向传播。在这里我们面临一个类似的问题，尽管梯度是从更接近输出的层传播的。使用与前向传播相同的推理，我们可以看到梯度的方差可能会放大，除非nout jsx2 =1nout js2 =1，其中noutnout是这一层的输出数量。这让我们陷入了两难境地:我们不可能同时满足这两个条件。相反，我们只是试图满足:

$$
\begin{aligned}
\frac{1}{2} (n_\mathrm{in} + n_\mathrm{out}) \sigma^2 = 1 \text{ or equivalently }
\sigma = \sqrt{\frac{2}{n_\mathrm{in} + n_\mathrm{out}}}.
\end{aligned}
$$

这就是现在标准的、实际上有益的Xavier初始化背后的原因，Xavier初始化是以其创建者的第一作者命名的:cite:`Glorot.Bengio.2010`。典型地，Xavier初始化从一个均值和方差为零的高斯分布中采样权值，该分布的均值和方差为零:我们也可以利用Xavier的直觉，在对均匀分布的权重进行抽样时选择方差。注意，均匀分布U(−a,a)U(−a,a)有方差a23a23。将a23a23插入到我们的条件中，就会得到根据的初始化建议

$$U\left(-\sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}, \sqrt{\frac{6}{n_\mathrm{in} + n_\mathrm{out}}}\right).$$

虽然上述数学推理中非线性不存在的假设在神经网络中很容易被违背，但Xavier初始化方法在实际应用中表现良好。

## 超越

上面的推理仅仅触及了现代参数初始化方法的表面。深度学习框架通常实现十几种不同的启发式。此外，参数初始化仍然是深度学习基础研究的热点。其中有专门针对绑定(共享)参数、超分辨率、序列模型和其他情况的启发式方法。例如，Xiao et al.证明了使用精心设计的初始化方法训练10000层神经网络而不使用结构技巧的可能性:cite:`Xiao.Bahri.Sohl-Dickstein.ea.2018`。

如果您对该主题感兴趣，我们建议您深入研究本模块提供的内容，阅读提出和分析每个启发式的论文，然后探索有关该主题的最新出版物。也许您会偶然发现甚至发明一个聪明的想法，并将实现贡献给深度学习框架。

## 总结

* 梯度的消失和爆发是深网络中常见的问题。参数初始化需要非常小心，以确保梯度和参数保持良好的控制。
* 需要初始化启发式来确保初始梯度不太大也不太小。
* ReLU激活函数缓解了梯度消失问题。这可以加速收敛。
* 在优化之前，随机初始化是保证对称性被打破的关键。
* Xavier初始化表明，对于每一层，任何输出的方差不受输入数量的影响，任何梯度的方差不受输出数量的影响。

## 练习

1. 除了MLP层的排列对称之外，你还能设计其他神经网络需要破坏对称的情况吗?
1. 我们是否可以在线性回归或softmax回归中将所有的权重参数初始化为相同的值?
1. 查找两个矩阵乘积特征值的解析界。这告诉了你什么关于确保梯度是良好的条件?
1. 如果我们知道某些项是发散的，我们能在事后修正吗?你可以参考一篇关于分层自适应率缩放的论文来获得灵感:cite:`You.Gitman.Ginsburg.2017`。
