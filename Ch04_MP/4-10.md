

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-23 00:50:28
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-18 02:03:24
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_multilayer-perceptrons/kaggle-house-price.html
-->

# 在Kaggle预测房价

现在我们已经介绍了一些基本的工具来建立和训练深度网络，并使用包括重量衰减和退出在内的技术来规范它们，我们准备通过参加一个Kaggle比赛将所有这些知识付诸实践。房价预测竞赛是一个很好的开始。这些数据是相当通用的，没有显示可能需要特殊模型(如音频或视频)的奇异结构。这个数据集是由Bart de Cock在2011年收集的[DeCock, 2011]，涵盖了从2006年到2010年在IA的Ames的房价。它比著名的Harrison和Rubinfeld(1978)的波士顿住宅数据集大得多，拥有更多的例子和更多的特征。

在本节中，我们将详细介绍数据预处理、模型设计和超参数选择。我们希望通过实际操作的方法，您将获得一些直觉，这些直觉将指导您作为数据科学家的职业生涯。

## 下载和缓存数据集

在整本书中，我们将在各种下载的数据集上训练和测试模型。在这里，我们实现了几个实用函数，以方便数据下载。首先，我们维护一个字典 DATA _ hub，它将一个字符串(数据集的名称)映射到一个元组，该元组包含定位数据集的 URL 和验证文件完整性的 SHA-1键。所有这些数据集都托管在地址为 DATA _ url 的站点上。

下面的下载函数将下载数据集，并将其缓存在本地目录中(。./data) ，并返回下载文件的名称。如果缓存目录中已经存在与此数据集对应的文件，且其 SHA-1与存储在 DATA _ hub 中的文件匹配，我们的代码将使用缓存文件，以避免冗余下载堵塞您的互联网。

TODO:CODE

我们还实现了另外两个实用函数: 一个是下载并解压 zip 或 tar 文件，另一个是将本书中使用的所有数据集从 DATA _ hub 下载到缓存目录中。

TODO:CODE

## Kaggle

Kaggle是一个很受欢迎的机器学习比赛平台。每个竞赛都以一个数据集为中心，许多竞赛是由利益相关者赞助的，他们为获胜的解决方案提供奖品。平台帮助用户通过论坛和共享代码进行互动，促进合作和竞争。尽管排行榜追逐经常失控,研究者只关注预处理步骤,而不是问基本问题,也有巨大的价值客观性的一个平台,可以促进竞争之间的直接定量比较方法以及代码共享,这样每个人都可以学习,没有什么工作。如果你想参加一个Kaggle比赛，你首先需要注册一个账户(见图4.10.1)。

TODO:IMG

在房价预测比赛页面，如图4.10.2所示，你可以找到数据集(在“Data”标签下)，提交预测，然后看到你的排名，URL就在这里:

TODO:IMG

## 访问和读取数据集

请注意，比赛数据分为训练集和测试集。 每个记录都包括房屋的属性值和属性，例如街道类型，建造年份，屋顶类型，地下室状况等。这些特征包括各种数据类型。 例如，施工年份用整数表示，屋顶类型用离散的类别分配表示，其他特征用浮点数表示。 这就是现实使事情复杂化的地方：对于某些示例，某些数据完全丢失，而缺失值仅标记为“ na”。 每个房屋的价格仅包含在培训中（毕竟这是一场比赛）。 我们将要划分训练集以创建一个验证集，但是只有在将预测上传到Kaggle之后才能在官方测试集上评估我们的模型。 图4.10.2中比赛标签上的“数据”标签具有下载数据的链接。

首先，我们将使用在2.2节中介绍的熊猫读入并处理数据。 因此，在继续操作之前，您需要确保已安装熊猫。 幸运的是，如果您正在阅读Jupyter，我们甚至可以在不离开笔记本电脑的情况下安装pandas。

TODO:CODE

为了方便起见，我们可以使用上面定义的脚本下载和缓存Kaggle住房数据集。

TODO:CODE

我们使用pandas来加载分别包含训练和测试数据的两个csv文件。

TODO:CODE

训练数据集包括1460个例子，80个特征，1个标签，测试数据包括1459个例子，80个特征。

TODO:CODE

让我们看看前四个和最后两个特性以及前四个示例中的标签(SalePrice)。

TODO:CODE

我们可以看到，在每个示例中，第一个特性是ID。这有助于模型识别每个训练示例。虽然这很方便，但它不携带任何用于预测目的的信息。因此，在将数据输入到模型之前，我们将其从数据集中删除。

TODO:CODE

## 数据预处理

如上所述，我们有各种各样的数据类型。在开始建模之前，我们需要对数据进行预处理。让我们从数字特征开始。首先，我们应用启发式方法，用相应特征的平均值替换所有缺失的值。然后，为了将所有的特征放在一个共同的尺度上，我们将数据标准化，将特征调整为零均值和单位方差:

TODO:MATH

为了验证这确实可以变换特征（变量），使其均值和单位方差为零，请注意E [x-μσ] =μ-μσ= 0E [x-μσ] =μ-μσ= 0且E [ （x-μ）2] =（σ2+μ2）-2μ2+μ2=σ2E[（x-μ）2] =（σ2+μ2）-2μ2+μ2=σ2。 直观上，我们将数据标准化有两个原因。 首先，它证明优化很方便。 其次，因为我们不知道先验哪些特征将是相关的，所以我们不想对分配给一个特征的系数的惩罚要比对任何其他特征的惩罚更大。

接下来我们处理离散值。其中包括“ms分区”等功能。我们用单一热点编码代替它们，就像我们以前把多类标签转换成向量一样(参见3.4.1节)。例如，“ms分区”假设值为“RL”和“RM”。删除“ms分区”特性后，将创建两个新的指示符特性“MSZoning_RL”和“MSZoning_RM”，其值为0或1。根据one-hot编码，如果“ms分区”的原始值为“RL”，则“MSZoning_RL”为1，“MSZoning_RM”为0。pandas包会自动为我们完成这项工作。

您可以看到，这种转换将特性的数量从79个增加到331个。最后，通过values属性，我们可以从panda格式中提取NumPy格式，并将其转换为张量表示进行训练。

TODO:CODE

## 训练

首先，我们训练具有平方损失的线性模型。 毫不奇怪，我们的线性模型不会导致竞赛获奖，但是它提供了健全性检查，以查看数据中是否包含有意义的信息。 如果我们不能做得比随机猜测要好，那么很有可能出现数据处理错误。 如果一切正常，线性模型将作为基线，使我们对简单模型与最佳报告模型的接近程度有所了解，从而使我们了解到我们应该从更先进的模型中获得多少收益。

TODO:CODE

对于房价，和股票价格一样，我们更关心相对数量而不是绝对数量。因此，我们往往更关心相对误差(y - y^yy - y^y)，而不是绝对误差(y - y^y - y^y)。例如，如果我们在估算俄亥俄州农村的一所房子的价格时，预测相差10万美元，而那里一所普通的房子的价值是12.5万美元，那么我们可能做了一件可怕的工作。另一方面，如果我们在加州的洛斯阿尔托斯山(Los Altos Hills)弄错了这个数字，这可能代表了一个惊人的准确预测(那里的房价中值超过了400万美元)。

解决这个问题的一种方法是用价格估计的对数来衡量差异。事实上，这也是比赛用来评估参赛作品质量的官方误差衡量标准。毕竟,一个小值δδ为|呆呆的−呆呆的^ |≤δ|日志⁡−日志⁡y ^ |≤δ转化为e−δy e ^ y≤≤δe−δδy ^ y≤≤e。这就导致了预测价格的对数与标签价格的对数之间的均方根误差:

TODO:MATH

TODO:CODE

与前几节不同，我们的训练函数将依赖于Adam优化器(我们将在后面更详细地描述它)。这个优化器的主要吸引力在于，尽管在提供无限资源进行超参数优化方面没有做得更好(有时更差)，但人们往往会发现它对初始学习率的敏感性明显降低。

TODO:CODE

## K-fold交叉验证

您可能还记得，在讨论如何处理模型选择的章节(4.4节)中，我们介绍了KK-fold交叉验证。我们将很好地利用它来选择模型设计和调整超参数。我们首先需要一个函数，它在K-fold交叉验证过程中返回数据的ithith fold。它通过分割ithith段作为验证数据并返回其余的训练数据来继续。请注意，这并不是处理数据的最有效方式，如果我们的数据集相当大，我们肯定会做一些更聪明的事情。但是这种增加的复杂性可能会不必要地混淆我们的代码，因此由于问题的简单性，我们可以安全地省略它。

TODO:CODE

当我们在K-fold交叉验证中训练K次时，会返回训练和验证的平均误差。

TODO:CODE

## 模型选择

在本例中，我们选择了一组未调优的超参数，并将其留给读者来改进模型。找到一个好的选择可能会花费时间，这取决于一个人优化了多少变量。有了足够大的数据集和正常种类的超参数， K-fold交叉验证对于多重测试具有相当的弹性。但是，如果我们尝试了不合理的大量选项，我们可能会幸运地发现验证性能不再代表真正的错误。

TODO:CODE

请注意，有时一组超参数的训练错误数量可能非常低，即使K-fold交叉验证的错误数量相当高。这表明我们是过拟合。在整个培训过程中，您将需要监控这两个数字。较少的过拟合可能表明我们的数据可以支持更强大的模型。大规模过拟合可能表明，我们可以通过合并正则化技术获益。

## 在Kaggle上提交预测

既然我们知道了什么是一个好的超参数选择，我们不妨使用所有的数据对其进行训练（而不是仅仅使用交叉验证片段中使用的数据的1−1/K）。通过这种方式得到的模型可以应用到测试集。将预测保存在csv文件中可以简化将结果上传到Kaggle。

TODO:CODE

接下来，如图4.10.3所示，我们可以提交我们在Kaggle上的预测，看看它们与测试集上的实际房价(标签)的对比情况。步骤非常简单:

TODO:CODE

* 登录Kaggle网站，访问房价预测比赛页面。
* 点击“提交预测”或“延迟提交”按钮(在撰写本文时，该按钮位于右侧)。
* 点击页面底部虚线框中的“上传提交文件”按钮，选择要上传的预测文件。
* 点击页面底部的“提交”按钮可以查看您的结果。

## 小结

* 实际数据通常包含不同数据类型的混合，需要进行预处理。
* 将实值数据重新缩放为零均值和单位方差是一个很好的默认设置。用平均值代替缺失值也是如此。
* 将分类特征转换为指标特征，使我们可以将它们像一键矢量一样对待。
* 我们可以使用K-fold交叉验证来选择模型并调整超参数。
* 对数对于相对误差很有用。

## 练习

1. 将您对该部分的预测提交给Kaggle。您的预测有多好？
1. 您能否通过直接最小化价格的对数来改进模型？ 如果您尝试预测价格的对数而不是价格，会发生什么？
1. 用平均值代替缺失值总是一个好主意吗？ 提示：您能构造一个值不随机丢失的情况吗？
1. 通过K-fold交叉验证调整超参数，从而提高Kaggle的得分。
1. 通过改进模型（例如，层数，重量衰减和跌落）来提高分数。
1. 如果我们不像本节中那样对连续数值特征进行标准化，会发生什么？
