

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-05 22:30:47
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-05 23:03:44
 * @Description:MT
 * @TODO::
 * @Reference:https://zh.d2l.ai/chapter_deep-learning-basics/dropout.html
 * http://preview.d2l.ai/d2l-en/master/chapter_multilayer-perceptrons/dropout.html
-->


# 丢弃法(Dropout)

在第4.5节中，我们介绍了通过惩罚权重的L2范数来规范化统计模型的经典方法。 用概率的话，我们可以通过证明我们已经假定先验的权重取平均值为零的高斯分布中的值来证明这种技术的合理性。 更直观地说，我们可能会认为我们鼓励模型在许多功能中分散其权重，而不是过多地依赖少数潜在的虚假关联。

## 过度拟合

面对比示例更多的功能，线性模型倾向于过度拟合。 但是给出的例子比功能更多，我们通常可以指望线性模型不会过拟合。 不幸的是，线性模型推广的可靠性是有代价的。 天真的应用，线性模型没有考虑要素之间的相互作用。 对于每个功能，线性模型都必须分配正权或负权，而忽略上下文。

在传统文献中，可概括性和灵活性之间的这种基本张力被描述为偏差方差折衷。 线性模型具有高偏差：它们只能代表一小类函数。 但是，这些模型的方差低：它们在数据的不同随机样本中给出相似的结果。

深度神经网络位于偏差方差频谱的另一端。 与线性模型不同，神经网络不限于单独查看每个特征。 他们可以学习功能组之间的交互。 例如，他们可能推断出，在电子邮件中同时出现的“尼日利亚”和“西联汇款”表明是垃圾邮件，但它们分别不是。

即使我们的例子比功能多得多，深度神经网络仍具有过度拟合的能力。 2017年，一组研究人员通过在随机标记的图像上训练深层网络来展示神经网络的极端灵活性。 尽管没有将输入链接到输出的任何真实模式，他们发现通过随机梯度下降优化的神经网络可以完美地标记训练集中的每幅图像。 考虑一下这意味着什么。 如果标签是随机随机分配的，并且有10个类别，则分类器对保持数据的准确度不能超过10％。 这里的泛化差距高达90％。 如果我们的模型表现力很强，以至于它们可能严重拟合过度，那么我们何时应该期望它们不会过度拟合呢？

深层网络令人困惑的泛化特性的数学基础仍然是开放的研究问题，我们鼓励以理论为导向的读者深入研究这个主题。现在，我们转而研究一些实用工具，这些工具往往能从经验上提高深网的普遍性。

## 通过扰动的鲁棒性

让我们简要地思考一下，我们期望从一个好的预测模型中得到什么。我们希望它能在看不见的数据上运行良好。经典的概化理论建议，为了缩小列车和测试之间的差距，我们应该致力于建立一个简单的模型。简单可以以少量维度的形式出现。我们在第4.4节讨论线性模型的单项式基函数时探讨了这个问题。此外，正如我们在4.5节中讨论权重衰减(L2正则化)时看到的，参数的(逆)范数也代表了简单性的有用度量。另一个简单的有用概念是平滑性，也就是说，函数不应该对输入的微小变化敏感。例如，当我们对图像进行分类时，我们会期望在像素上添加一些随机噪声基本上是无害的。

1995年，Christopher Bishop将这个想法形式化，他证明了用输入噪声进行训练等价于Tikhonov正则化[Bishop, 1995]。这项工作在要求函数平滑(因此简单)和要求函数对输入中的干扰有弹性之间画出了一个清晰的数学联系。

然后，在2014年，Srivastava等人[Srivastava et al.， 2014]开发了一个聪明的想法，如何将Bishop的想法应用到网络的内部层。即，他们提出在训练时先将噪声注入到网络的每一层，然后再计算后续的一层。他们意识到，当训练一个多层深度网络时，注入噪声只会增强输入-输出映射的平滑性。

他们的想法叫做dropout，涉及到在前向传播过程中计算每个内层时注入噪音，这已经成为训练神经网络的标准技术。这种方法被称为“中途退出”(dropout)，因为在训练过程中，我们实际上会丢失一些神经元。在整个训练过程中，在每次迭代中，标准dropout包括在计算后续层之前对每一层中的一些节点进行归零。

澄清一下，我们把我们自己的叙述与毕晓普的联系强加于人。关于辍学的原始论文通过一个与有性繁殖惊人的类比提供了直觉。作者认为，神经网络过拟合的特点是，每一层依赖于前一层特定的激活模式，称之为协同适应。他们声称，辍学会破坏协同适应，就像有性繁殖被认为会破坏协同适应基因一样。

关键的挑战是如何注入这些噪音。一种方法是以无偏的方式注入噪声，这样每一层的期望值(同时固定其他层)就等于没有噪声时的期望值。

在Bishop的工作中，他将高斯噪声添加到了线性模型的输入中。 在每次训练迭代中，他将平均零为ϵ〜N（0，σ2）ϵ〜N（0，σ2）的分布采样的噪声添加到输入xx，从而得到一个扰动点x'= x + ϵx'= x + ϵ。 期望地，E [x'] = xE [x'] = x。

在标准的遗漏规范化中，通过对保留的（未丢弃）节点的分数进行归一化来对每一层进行去偏移。 换句话说，每个中间激活hh都以丢失概率pp替换为随机变量h'h'，如下所示：

（4.6.1）

h′= {0h1-p，否则为p

h′= {0，否则为ph1-p

通过设计，期望保持不变，即E [h'] = hE [h'] = h。

## 实践中的dropout

调用图4.1.1中带有隐藏层和5个隐藏单元的MLP。 当我们将缺失应用于隐藏层时，以概率pp将每个隐藏单元归零，结果可以视为仅包含原始神经元子集的网络。 在图4.6.1中，将h2h2和h5h5删除。 因此，输出的计算不再取决于h2h2或h5h5，并且在执行反向传播时，它们各自的梯度也将消失。 这样，输出层的计算不能过分依赖于

通常，我们在测试时禁用dropout。对于一个训练过的模型和一个新的例子，我们不需要删除任何节点，因此不需要进行规范化。然而，也有一些例外:一些研究人员在测试时使用dropout作为估算神经网络预测的不确定性的启发式:如果预测在许多不同的dropout mask上一致，那么我们可以说网络更有信心。

## 从头开始实现

为了实现单个层的dropout函数，我们必须从一个伯努利(二进制)随机变量中抽取与我们的层有维数一样多的样本，其中随机变量以1 - p1 - p的概率取值11 (keep)，以pp的概率取值00 (drop)。实现这一点的一个简单方法是首先从均匀分布U[0,1]U[0,1]中抽取样本。然后我们可以保留那些对应样本大于pp的节点，去掉其余节点。

在下面的代码中，我们实现了一个dropout_layer函数，它以dropout的概率去除张量输入X中的元素，并按照上面描述的那样重新调整剩余的元素:幸存者除以1.0-dropout。

TODO:CODE

我们可以在几个例子中测试dropout_layer函数。在下面的代码行中，我们通过dropout操作传递输入X，概率分别为0、0.5和1。

TODO:CODE

### 定义模型参数

同样，我们使用3.5节中介绍的Fashion-MNIST数据集。我们定义了一个MLP，它有两个隐藏层，每个层包含256个单位。

TODO:CODE

### 定义模型

下面的模型将dropout应用于每个隐藏层的输出(在激活函数之后)。我们可以为每一层分别设置dropout概率。一种常见的趋势是在靠近输入层的地方设置较低的dropout概率。下面我们将它设置为0.2和0.5的第一和第二个隐藏层,分别。我们确保辍学生只在训练期间活动。

TODO:CODE

### 训练和测试

这类似于前面所述的MLP训练和测试。

TODO:CODE

## 简洁的实现

对于高级api，我们所需要做的就是在每个完全连接的层之后添加一个退出层，将退出概率作为唯一的参数传递给它的构造函数。在训练过程中，drop - out层会按照指定的drop - out概率随机的drop - out前一层的输出(或者相当于后面一层的输入)。当不在训练模式时，Dropout层只是在测试过程中通过数据。

深度网络的令人困惑的泛化特性的数学基础仍然是开放的研究问题，因此我们鼓励理论性读者深入研究该主题。 目前，我们转向对实际工具的研究，这些工具往往会根据经验提高深网的泛化性。

TODO:CODE

接下来，我们训练和测试模型。

TODO:CODE

## 小结

* 除了控制维数和权值向量的大小之外，dropout是另一种避免过拟合的工具。通常它们被联合使用。
* Dropout将激活h替换为期望值为h的随机变量。
* Dropout只在训练期间使用。

## 练习

1. 如果你改变了第一层和第二层的概率，会发生什么?特别是，如果你切换两个层的那些会发生什么?设计一个实验来回答这些问题，定量地描述你的结果，并总结定性的结论。
1. 增加epoch的数量，比较使用dropout和不使用dropout时的结果。
1. 当dropout被应用和没有被应用时，每个隐藏层中激活的方差是多少?画一个图来显示这两个模型的数量是如何随时间变化的。
1. 为什么在测试时不使用dropout ?
1. 以本节中的模型为例，比较使用dropout和权重衰减的效果。当失重和重量衰减同时使用时，会发生什么?结果是相加的吗?回报是否减少(或更糟)?它们互相抵消了吗?
1. 如果我们将dropout应用于权重矩阵的单个权重而不是激活，会发生什么?
1. 发明另一种技术，以注入随机噪声在每层不同于标准的dropout技术。你能在Fashion-MNIST数据集(针对固定架构)上开发一种优于dropout的方法吗?
