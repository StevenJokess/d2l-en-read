

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-16 23:55:34
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-28 16:44:00
 * @Description:MT， improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/gru.html
-->

# 门控循环单元

在前面的部分中，我们讨论了梯度是如何在递归神经网络中计算的。特别地，我们发现矩阵的长积会导致渐变的消失或发散。让我们简单地思考一下这种梯度异常在实践中意味着什么:

* 我们可能会遇到这样一种情况，即早期观测对于预测未来所有的观测具有非常重要的意义。考虑这样一个有些人为的情况: 第一个观察值包含一个校验和，目的是确定在序列末尾的校验和是否正确。在这种情况下，第一个标记的影响是至关重要的。我们希望有一些在记忆细胞中存储重要早期信息的机制。如果没有这样一个机制，我们将不得不给这个观测指定一个非常大的梯度，因为它影响所有随后的观测。
* 我们可能会遇到一些符号没有相关观察的情况。例如，在解析一个网页时，可能会有一些与评估网页上传达的情绪无关的辅助 HTML 代码。我们希望有一些机制跳过这样的符号在潜在的状态表示。
* 我们可能会遇到这样的情况: 一个序列的各个部分之间存在逻辑断裂。例如，在一本书的章节之间可能有一个过渡，或者在证券市场的熊市和牛市之间有一个过渡。在这种情况下，最好能有一种重置内部状态表示的方法。

已经提出了一些方法来解决这个问题。其中最早的是长期短期记忆(LSTM)[ Hochreiter & Schmidhuber，1997] ，我们将在第9.2节中讨论。门控循环单元(GRU)[ Cho 等人，2014]是一个稍微更精简的变量，往往提供可比性能和显着更快的计算。详情请参阅[ Chung 等人，2014]。由于它的简单性，让我们从 GRU 开始。

## 控制隐藏状态

常规 rnn 和 gru 之间的关键区别在于后者支持控制隐藏状态。这意味着我们有专门的机制来确定何时应该更新隐藏状态，以及何时应该重置隐藏状态。这些机制是学到的，它们解决了上面列出的问题。例如，如果第一个符号非常重要，我们将学习不更新第一次观察后的隐藏状态。同样，我们将学会跳过不相关的临时观察。最后，我们将学习在任何需要的时候重置潜在状态。我们将在下面详细讨论这个问题。

### 重置门和更新门

我们需要引入的第一件事是重置和更新门。我们把它们设计成具有(0,1)(0,1)项的向量，这样我们就可以执行凸组合。例如，重置变量将允许我们控制需要记忆的以前状态的数量。同样，更新变量允许我们控制新状态中有多少只是旧状态的副本。

我们从工程门开始生成这些变量。图9.1.1示出了一个 GRU 中复位门和更新门的输入，给出了当前时间步骤输入 x t Xt 和前一个时间步骤 h t-1 Ht-1的隐藏状态。输出是由一个完全连接的带有 s 形激活函数的层给出的。

图9.1.1 GRU 中的重置和更新门。

对于给定的时间步长 t，小批量输入为 x t ∈ Rn d Xt ∈ Rn d (实例数: n，输入数: d) ，最后一个时间步长的隐状态为 h t-1∈ Rn h (隐状态数: h)。然后，重置门 r t ∈ Rn h Rt ∈ Rn h，更新门 z t ∈ Rn h Zt ∈ Rn h，计算如下:

TODO:MATH

这里，w xr，w xz ∈ Rd h Wxr，Wxz ∈ Rd h 和 w hr，w hz ∈ rhwhr，w ∈ rhh 是权参数，b r，b z ∈ r1h br，bz ∈ r1h 是偏参数。我们使用一个 S形函数(如第4.1节所介绍的)来将输入值转换为间隔(0,1)。

## 实践中的重置门

我们首先将复位门和一个规则的潜在状态更新机制集成起来。在传统的 RNN 中，我们会有一个表单的隐藏状态更新

TODO:MATH

这与上一节的讨论本质上是相同的，尽管采用tanh形式的非线性来确保隐态的值保持在区间(−1,1)(−1,1)。如果我们想要减少之前状态的影响，我们可以在元素方面用Ht - 1Ht - 1乘以RtRt。每当复位门RtRt中的项接近11时，我们恢复一个常规RtRt。对于复位门RtRt中接近00的所有条目，隐藏状态是MLP以XtXt作为输入的结果。因此，任何预先存在的隐藏状态都被重置为默认值。这将导致以下候选隐藏状态(它是候选状态，因为我们仍然需要合并更新门的操作)。

TODO:MATH

图9.1.2示出了应用复位门后的计算流程。符号⊙表示张量之间的点乘。

图9.1.2 GRU中的候选隐藏状态计算。乘法是逐元素进行的。

## 实践中的更新门

接下来，我们需要合并更新门Zt的效果，如：numref：fig_gru_3所示。这确定了新状态HtHt刚好是旧状态Ht-1Ht-1的程度以及使用了多少新候选状态H〜t。选通变量ZtZt可以用于此目的，只需在两个候选对象之间采用元素逐行凸组合即可。这导致了GRU的最终更新方程式。

TODO:MATH

每当更新门Zt接近1时，我们仅保留旧状态。在这种情况下，来自Xt的信息实际上将被忽略，从而有效地跳过了依赖链中的时间步tt。相反，每当Zt接近0时，新的潜在状态Ht就会接近候选潜在状态H〜t。这些设计可以帮助我们解决RNN中消失的梯度问题，并更好地捕获具有较大时间步距的时间序列的依存关系。总之，GRU具有以下两个区别功能：

* 重置门有助于捕获时间序列中的短期依存关系。
* 更新门有助于捕获时间序列中的长期依赖性。

## 从零实现GRU

为了更好地理解这个模型，让我们从头开始实现GRU。

### 初始化模型参数

下一步是初始化模型参数。我们从方差为0.01的高斯绘制权重，并将偏差设置为0。超参数`num_hiddens`定义隐藏单元的数量。我们实例化与更新门，重置门和候选隐藏状态本身有关的所有权重和偏差。随后，我们将渐变附加到所有参数。

TODO:CODE

### 定义模型

现在，我们将定义隐藏状态初始化函数init_gru_state。就像第8.5节中定义的init_rnn_state函数一样，该函数返回一个张量，该张量的形状（批大小，隐藏单位数）的值均为零。

TODO:CODE

现在我们准备定义GRU模型。它的结构与基本RNN单元相同，除了更新方程更为复杂。

TODO:CODE

## 训练与预测

训练和预测的工作方式与以前完全相同。训练一个纪元后，困惑和输出句子将如下所示。

TODO:CODE

## 简洁实现

在Gluon中，我们可以直接在rnn模块中调用GRU类。这封装了上面我们明确说明的所有配置细节。该代码明显更快，因为它使用编译运算符而不是Python来处理许多我们之前详细说明的细节。

TODO:CODE

## 小结

* 门控递归神经网络更擅长捕获具有较大时间步距的时间序列的依存关系。
* 重置门有助于捕获时间序列中的短期依存关系。
* 更新门有助于捕获时间序列中的长期依赖性。
* 每当复位门打开时，GRU都将基本RNN作为极端情况。他们可以根据需要忽略序列。

## 练习

1. 比较rnn.RNN,rnn.GRU的运行时、perplexity和输出字符串。RNN RNN。GRU实现彼此。
1. 假设我们只想使用时间步长t '的输入来预测时间步长t>t不>t '的输出。每个时间步骤的重置和更新闸门的最佳值是什么?
1. 调整超参数，观察分析对运行时间、perplexity、书面歌词的影响。
1. 如果只实现GRU的一部分，会发生什么?也就是说，实现一个只有复位门的循环单元。同样，只使用更新门实现周期性单元格。
