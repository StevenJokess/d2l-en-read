

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-17 18:27:47
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-17 22:45:20
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/lstm.html
-->

# 长短期记忆（LSTM）

在潜在变量模型中解决长期信息保存和短期输入跳过的挑战已经存在很长时间了。解决这个问题的最早方法之一是LSTM [Hochreiter＆Schmidhuber，1997]。它具有门控循环单元（GRU）的许多属性。有趣的是，LSTM的设计比GRU稍微复杂一些，但比GRU早了近二十年。

可以说，它是受计算机逻辑门启发的。为了控制存储单元，我们需要多个门。需要一个门来从单元中读出条目（与读取任何其他单元相反）。我们将其称为输出门。需要第二个门来决定何时将数据读取到单元中。我们将此称为输入门。最后，我们需要一种机制来重置单元格的内容，该机制由“忘记门”控制。这种设计的动机与以前相同，即能够通过专用机制决定何时记住和何时忽略潜在状态下的输入。让我们看看这在实践中是如何工作的。

## 门控存储单元

LSTM中引入了三个门：输入门，忘记门和输出门。除此之外，我们还将介绍形状与隐藏状态相同的存储单元。严格来说，这只是隐藏状态的一种变体，旨在记录其他信息。

### 输入门，忘记门和输出门

就像GRU一样，馈送到LSTM门的数据是当前时间步XtXt的输入和前一个时间步Ht-1Ht-1的隐藏状态。这些输入由一个完全连接的层和一个S型激活函数处理，以计算输入，遗忘和输出门的值。结果，这三个门的所有输出值都在[0,1] [0,1]范围内。图9.2.1说明了输入，遗忘和输出门的数据流。

TODO:PIC

我们假设有hh个隐藏单元，最小批量的大小为nn，输入的数量为dd。因此，输入为Xt∈Rn×dXt∈Rn×d，最后一个时间步的隐藏状态为Ht-1∈Rn×hHt-1∈Rn×h。相应地，门定义如下：输入门为It∈Rn×hIt∈Rn×h，忘记门为Ft∈Rn×hFt∈Rn×h，输出门为Ot∈Rn×hOt∈Rn× H 。它们的计算如下：

（9.2.1）

ItFtOt =σ（XtWxi + Ht-1Whi + bi），=σ（XtWxf + Ht-1Whf + bf），=σ（XtWxo + Ht-1Who + bo），
，

其中Wxi，Wxf，Wxo∈Rd×hWxi，Wxf，Wxo∈Rd×h和Whi，Whf，Who∈Rh×hWhi，Whf，Who∈Rh×h是权重参数，bi，bf，bo∈R1×hbi， bf，bo∈R1×h是偏差参数。

### 隐状态

最后，我们需要定义如何计算隐藏状态Ht∈Rn x hHt∈Rn x h。这就是输出门发挥作用的地方。在LSTM中，它只是内存单元tanhtanh的门控版本。这确保了HtHt的值总是在区间(−1,1)(−1,1)内。每当输出门是11，我们有效地传递所有的记忆信息通过预测器，而对于输出00，我们保留所有的信息仅在记忆细胞内，不执行进一步的处理。图9.2.4给出了数据流的图形说明。

TODO:TRANS

## 从零开始实施

现在，让我们从头开始实现LSTM。与前面几节中的实验相同，我们首先加载The Time Machine的数据。

让我们通过调用8.5节中介绍的RNNModelScratch函数来训练与9.1节中相同的LSTM。

TODO:CODE

### 初始化模型参数

接下来，我们需要定义和初始化模型参数。如前所述，超参数num_hiddens定义隐藏单元的数量。我们按照0.01标准偏差的高斯分布初始化权重，并将偏差设置为0。

TODO:CODE

### 定义模型

在初始化函数中，LSTM的隐藏状态需要返回一个值为00且形状为（批大小，隐藏单元数）的附加存储单元。因此，我们得到以下状态初始化。

TODO:CODE

实际模型的定义就像我们之前讨论的那样：提供三个门和一个辅助存储单元。请注意，只有隐藏状态才传递到输出层。存储单元CtCt不直接参与输出计算。

TODO:CODE

## 训练和预测

让我们通过调用8.5节中介绍的RNNModelScratch函数来训练与9.1节中相同的LSTM。

TODO:CODE

## 简洁实现

在Gluon中，我们可以直接在rnn模块中调用LSTM类。这封装了上面我们明确说明的所有配置细节。该代码明显更快，因为它使用编译运算符而不是Python来处理许多我们之前详细说明的细节。

TODO:CODE

在许多情况下，LSTM的性能比GRU稍好，但是由于较大的潜在状态，它们的训练和执行成本更高。LSTM是具有非平凡状态控制的原型潜在变量自回归模型。这些年来，已经提出了许多变体，例如，多层，残余连接，不同类型的正则化。但是，由于序列的长期依赖性，训练LSTM和其他序列模型（例如GRU）的成本很高。稍后，我们将遇到在某些情况下可以使用的替代模型，例如变压器。

## 小结

* LSTM具有三种类型的门：输入门，忘记门和控制信息流的输出门。
* LSTM的隐藏层输出包括隐藏状态和存储单元。仅隐藏状态被传递到输出层。存储单元完全是内部的。
* LSTM可以应对逐渐消失的梯度。

## 练习

1. 调整超参数。观察并分析对运行时，困惑和生成的输出的影响。
1. 您将如何更改模型以生成正确的单词而不是字符序列？
1. 比较给定隐藏维度的GRU，LSTM和常规RNN的计算成本。要特别注意训练和推理成本。
1. 由于候选存储单元通过使用tanhtanh函数确保值范围在1-1和11之间，为什么隐藏状态需要再次使用tanh函数来确保输出值范围在-1和1之间。11？
1. 为时间序列预测而不是字符序列预测实现LSTM。
