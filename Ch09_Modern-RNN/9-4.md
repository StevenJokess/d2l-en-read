

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-29 21:17:01
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-29 21:33:40
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/bi-rnn.html
 * https://zh.d2l.ai/chapter_recurrent-neural-networks/bi-rnn.html
-->

# 双向RNN

到目前为止，我们假设我们的目标是建模下一个单词，例如，在一个时间序列的上下文中或在一个语言模型的上下文中。虽然这是一个典型的场景，但它不是我们可能遇到的唯一的场景。为了说明这个问题，考虑以下三个填空任务:

1. I am _____
1. I am _____ very hungry.
1. I am _____ very hungry, I could eat half a pig.

根据可用信息的数量，我们可能会用非常不同的单词来填空，比如“happy”、“not”和“very”。显然，短语的结尾(如果可能的话)传达了选择哪个词的重要信息。不能利用这一优势的序列模型将在相关任务上表现不佳。例如，要做好命名实体识别(例如，识别“Green”是否指“Mr。”更长的背景也同样重要。为了获得解决这个问题的一些灵感，让我们转向图形模型。

## 动态编程

本节将说明动态规划问题。具体的技术细节对于理解深度学习并不重要，但它们有助于激发人们为什么会使用深度学习，以及为什么会选择特定的架构。

如果我们想用图形模型来解决这个问题，我们可以设计一个潜在变量模型，如下所示。我们假设存在一些潜在的变量ht，支配我们通过p(xt∣ht)p(xt∣ht)观察到的排放xtxt。此外，ht→ht+1ht→ht+1的跃迁由状态跃迁概率p(ht+1∣ht)给出。图9.4.1所示的图形模型为隐马尔可夫模型(HMM)。
图9.4.1隐马尔可夫模型。

因此，对于一个TT观测序列，我们在观察态和隐藏态上的联合概率分布如下:

(9.4.1)

h p (x) = p (h1) p (x1∣h1)∏t = 2 tp (ht∣ht−1) p (xt∣ht)。

现在假设我们观察了除某些xjxj之外的所有xi，我们的目标是计算p(xj∣x−j)p(xj∣x−j)，其中x−j=(x1,x2，…，xj−1)x−j=(x1,x2，…，xj−1)。为了做到这一点，我们需要对h=(h1，…，hT)h=(h1，…，hT)的所有可能选择进行求和。如果hihi可以取kk个不同的值，这意味着我们需要对kTkT项求和——不可能完成的任务!幸运的是，对于这个问题有一个优雅的解决方案:动态编程。要了解它是如何工作的，考虑对前两个隐藏变量h1和h2求和。这个收益率:

TODO:MATH

因此，我们可以把反向递归写成

ρt−1 (ht−1) =∑htp (ht∣ht−1) p (xt∣ht)ρt (ht),

初始化T(hT)=1，初始化T(hT)=1。这两个递归允许我们在O(kT)O(kT)(线性)时间内对TT变量求和(h1，…，hT)(h1，…，hT)，而不是在指数时间内。这是图形化模型的概率推理的最大好处之一。这是Aji和McEliece在2000年提出的[Aji和McEliece, 2000]的一个非常特殊的例子。结合前传和后传，我们就可以计算了

TODO:MATH

注意，在抽象的术语中，逆向递归可以写成:gg是一个可学习的函数。这看起来很像一个更新方程，只是向后运行，不像我们目前在RNNs中看到的那样。事实上，当未来数据可用时，HMMs可以从了解未来数据中获益。信号处理科学家将知道和不知道未来观测的两种情况区分为插值和外推。详见[Doucet et al.， 2001]关于顺序蒙特卡洛算法的介绍章节。

## 双向模型

如果我们想在RNNs中有一种机制，提供与HMMs中类似的前瞻性能力，我们需要修改我们目前看到的循环网络设计。幸运的是，这在概念上很简单。我们不再只在前向模式下从第一个符号开始运行RNN，而是从最后一个符号开始从后向前运行另一个RNN。双向递归神经网络增加了反向传递信息的隐藏层，使信息处理更加灵活。图9.4.2说明了具有单一隐含层的双向递归神经网络的结构。

图9.4.2双向递归神经网络的结构。

事实上，这与我们上面遇到的前向递归和后向递归并没有太大的不同。主要的区别是，在前面的例子中，这些方程有一个特定的统计意义。现在它们没有了这样容易理解的解释，我们可以把它们当作通用函数。这种转变集中体现了许多指导现代深度网络设计的原则:首先，使用经典统计模型的功能依赖类型，然后使用通用形式的模型。

### 定义

双向神经网络是由[Schuster & Paliwal, 1997]提出的。有关各种建筑的详细讨论，请参阅[Graves & Schmidhuber, 2005]的论文。让我们看看这种网络的细节。

对于给定的步伐tt, minibatch输入Xt∈Rn×dXt∈Rn×d(数量的例子:神经网络、输入:dd)和隐藏层的激活函数是ϕϕ。在双向架构中，我们假设该时间步的正向隐藏状态为H→t∈Rn * hH→t∈Rn * H, H←t∈Rn * hH←t∈Rn * H。这里hh表示隐藏单位的数量。我们计算正向和逆向隐藏状态更新如下:

(9.4.7)

H→tH←t =ϕ(XtW (f) xh + H t→−1 w (f) hh + b (f) H), =ϕ(XtW (b) xh + H←t + 1 w (b) hh + b (b) H)。

其中权重参数W(f)xh∈Rd×h,W(f)hh∈Rh×h,W(b)xh∈Rd×h, W(b)xh∈Rd×h, W(b)hh∈Rh×h,W(b) hh∈Rh×h,W(b) hh∈Rh×h, Wxh(b)∈Rd×h, Whh(b)∈Rh×h, h(b) h∈R1×h, b(b)h∈R1×h, bh(b)∈R1×h，都是模型参数。

然后将正向隐藏状态H→tH→t和向后隐藏状态H←tH←t串联，得到隐藏状态Ht∈Rn×2hHt∈Rn×2h，并馈给输出层。在深度双向网络中，信息作为输入被传递到下一个双向层。最后，输出层计算输出Ot∈Rn×qOt∈Rn×q(输出数量:qq):

(9.4.8)

= HtWhq + bq。

其中权值参数Whq∈R2h xqwhq∈R2h xq，偏置参数bq∈R1 xqbq∈R1 xq为输出层的模型参数。这两个方向可以有不同数量的隐藏单位。

## 计算成本和应用

双向RNN的关键特征之一是，来自序列两端的信息都用于估计输出。也就是说，我们使用来自未来和过去观察的信息来预测当前（平滑情况）。对于语言模型，这不是我们想要的。毕竟，在预测下一个符号时，我们并不知道知道下一个符号。因此，如果天真地使用双向RNN，我们将不会获得非常好的准确性：在训练过程中，我们将使用过去和将来的数据来估计当前时间。在测试期间，我们只有过去的数据，因此准确性较差（我们将在下面的实验中对此进行说明）。

更糟的是，双向RNN也非常慢。这样做的主要原因是，它们既需要前进又需要后退，并且后退取决于前向传播的结果。因此，梯度将具有非常长的依赖性链。

实际上，双向层很少使用，并且仅用于狭窄的一组应用程序，例如填写缺失的单词，注释令牌（例如，用于命名实体识别）或将序列批发编码为序列处理管道中的步骤（例如， 用于机器翻译）。总之，要小心处理！

## 为错误的应用训练一个双向RNN

由于上述原因，输出显然不能令人满意。关于如何更有效地使用双向模型的讨论，请参见15.2节中的情感分类。

TODO:CODE

由于上述原因，输出显然不能令人满意。关于如何更有效地使用双向模型的讨论，请参见15.2节中的情感分类。

## 小结

* 在双向递归神经网络中，每个时间步长的隐藏状态由当前时间步前和后的数据同时确定。
* 双向神经网络与图形模型中的前向后向算法有着惊人的相似之处。
* 双向神经网络主要用于序列嵌入和在双向背景下估计观测值。
* 由于双向神经网络的梯度链很长，因此训练成本很高。

## 练习

1. 如果不同的方向使用不同数量的隐藏单元，HtHt的形状会有什么变化?
1. 设计了一种具有多重隐层的双向递归神经网络。
1. 利用双向神经网络实现序列分类算法。提示:使用RNN嵌入每个单词，然后在将输出发送到MLP进行分类之前聚合(平均)所有嵌入输出。例如，如果我们有(o1,o2,o3)(o1,o2,o3)，我们首先计算o¯=13∑ioio¯=13∑ioi，然后使用后者进行情感分类。
