

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-29 21:45:51
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-05 22:46:38
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/seq2seq.html
-->

# 序列到序列

序列到序列(seq2seq)模型基于编码器-解码器架构，为序列输入生成序列输出，如图9.7.1所示。编码器和解码器都使用递归神经网络(RNNs)来处理可变长度的序列输入。编码器的隐藏状态直接用于初始化解码器的隐藏状态，以便将信息从编码器传递到解码器。

图9.7.1序列到序列模型的体系结构

编码器和解码器中的层如图9.7.2所示。

图9.7.2编码器和解码器的层。

在本节中，我们将解释并实现seq2seq模型，以在机器翻译数据集上进行训练。

## 编码器

回想一下seq2seq的编码器可以通过将序列信息编码为c，将可变长度的输入转换为固定长度的上下文向量cc。 我们通常在编码器中使用RNN层。 假设我们有一个输入序列x1，...，xT，其中xtxt是第t字。 在时间步tt处，RNN将有两个向量作为输入：xtxt的特征向量xtxt和最后一个时间步ht-1ht-1的隐藏状态。 让我们用函数ff表示RNN的隐藏状态的转换：

ht = f（xt，ht-1）。

接下来，编码器捕获所有隐藏状态的信息，并使用函数q将其编码为上下文向量c：

c = q（h1，...，hT）。

例如，如果我们选择qq作为q（h1，...，hT）= hT，则上下文向量将是最终的隐藏状态hThT。

到目前为止，我们上面描述的是单向RNN，其中每个时间步的隐藏状态仅取决于先前的时间步。 我们还可以使用其他形式的RNN（例如GRU，LSTM和双向RNN）对顺序输入进行编码。

现在，让我们实现seq2seq的编码器。 在这里，我们使用词嵌入层根据输入语言的词索引获得特征向量。 这些特征向量将被馈送到多层LSTM。 编码器的输入是一批序列，它是具有形状（批大小，序列长度）的二维张量。 编码器返回LSTM输出，即所有时间步长的隐藏状态，以及最终时间步长的隐藏状态和存储单元。

TODO:CODE

接下来，我们将创建一个批大小为4和7个时间步骤的minibatch序列输入。我们假设LSTM单元的隐藏层数为2，隐藏层数为16。编码器对输入执行前向计算后返回的输出形状为(时间步数、批大小、隐藏单元数)。门控递归单元在最终时间步的多层隐藏状态形状为(隐藏层数、批大小、隐藏单元数)。对于门控循环单元，状态列表只包含一个元素，即隐藏状态。如果使用长短期内存，则状态列表还将包含另一个元素，即内存单元格。

TODO:CODE

由于使用了LSTM，状态列表将包含隐藏状态和具有相同形状的内存单元(隐藏层的数量、批大小、隐藏单元的数量)。但是，如果使用GRU，状态列表将只包含一个元素——最终时间步中的隐藏状态(隐藏层的数量、批大小、隐藏单元的数量)。

TODO:CODE

## 解码器

正如我们刚刚介绍的那样，上下文向量cc对来自整个输入序列x1，...，xTx1，...，xT的信息进行编码。 假设训练集中的给定输出为y1，...，yT'y1，...，yT'。 在每个时间步t't'，输出yt'yt'的条件概率将取决于先前的输出序列y1，...，yt'-1y1，...，yt'-1和上下文向量cc，即

TODO:MATH

因此，我们可以使用另一个RNN作为解码器。 在时间步t'，解码器将使用以下三个输入更新其隐藏状态st'：yt'-1的特征向量yt'-1，上下文向量c和隐藏状态 最后时间步长st'-1的时间。 让我们用函数gg表示解码器中RNN隐藏状态的转换：

TODO:MATH

在实现解码器时，我们直接将最终时间步中编码器的隐藏状态用作解码器的初始隐藏状态。 这就要求编码器和解码器RNN具有相同数量的层和隐藏单元。 解码器的LSTM前向计算类似于编码器。 唯一的不同是，我们在LSTM层之后添加了一个密集层，其中隐藏的大小是词汇量。 密集层将预测每个单词的置信度得分。

TODO:CODE

我们创建了一个与编码器具有相同超参数的解码器。如我们所见，输出形状更改为(批处理大小、序列长度、词汇表大小)。

TODO:CODE

## 损失函数

对于每个时间步，解码器都会输出词汇量的置信度分数矢量来预测单词。 与语言建模相似，我们可以应用softmax来获得概率，然后使用交叉熵损失来计算损失。 注意，我们填充了目标句子以使它们具有相同的长度，但是我们不需要计算填充符号上的损失。

为了实现过滤掉某些条目的损失函数，我们将使用一个称为SequenceMask的运算符。 它可以指定掩盖第一个尺寸（axis= 0）或第二个尺寸（axis= 1）。 如果选择第二个，给定有效的长度向量len和2-dim输入X，则此运算符为所有i设置X[i, len[i]:] = 0。

TODO:CODE

应用于n-dim张量X，它设置X[i, len[i]:, :, ..., :] = 0。此外，我们可以指定填充值，例如-1，如下所示。

TODO:CODE

现在，我们可以实现softmax交叉熵损失的屏蔽版本。 请注意，每个Gluon损失函数都允许指定每个示例的权重，默认情况下为1s。 然后，对于每个要删除的示例，我们都可以使用零权重。 因此，我们的自定义损失函数接受一个额外的valid_len参数来忽略每个序列中的某些失败元素。

TODO:CODE

为了进行完整性检查，我们创建了相同的三个序列，为第一个序列保留4个元素，为第二个序列保留2个元素，为最后一个序列保留一个元素。那么第一个例子的损失应该是第二个的2倍，最后一个损失应该是0。

## 训练

在训练时，如果目标序列有长度n，我们将前n−1个令牌作为输入输入到译码器，后n−1令牌作为ground truth label。

TODO:CODE

接下来，我们创建一个模型实例并设置超参数。然后，我们可以训练模型。

TODO:CODE

## 预测

这里我们实现了最简单的方法，贪婪搜索，以生成一个输出序列。如图9.7.3所示，在预测过程中，我们在时间步长0时将相同的“<bos>”标记输入译码器作为训练。但是后面一个时间步的输入令牌是前一个时间步的预测令牌。

图9.7.3用贪婪搜索预测序列到序列模型

TODO：CODE

试几个例子：

TODO：CODE

## 小结

* 序列到序列(seq2seq)模型基于编码器-解码器架构，从序列输入生成序列输出。
* 我们为编码器和解码器使用多个LSTM层。


## 练习

1. 除了神经机器翻译，你还能想到seq2seq的其他用例吗?
1. 如果本节示例中的输入序列较长，该怎么办?
1. 如果我们在损失函数中不使用序列运算，会发生什么?
