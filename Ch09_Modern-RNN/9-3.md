

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-29 21:11:09
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-29 21:16:22
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-modern/deep-rnn.html
-->

# 深度递归神经网络

到目前为止，我们仅讨论了具有单个单向隐藏层的递归神经网络。在其中，潜在变量和观测值如何相互作用的特定功能形式相当随意。只要我们有足够的灵活性来对不同类型的交互进行建模，这就不是大问题。但是，对于单层来说，这可能是非常具有挑战性的。对于感知器，我们通过添加更多层来解决此问题。在RNN中，这有点棘手，因为我们首先需要确定如何以及在何处添加额外的非线性。下面我们的讨论主要集中在LSTM上，但是它也适用于其他序列模型。

* 我们可以为门控机制增加额外的非线性。也就是说，我们可以使用多个层来代替单个感知器。这使LSTM的机制保持不变。相反，它使它更加复杂。如果使我们相信LSTM机制描述了潜在变量自回归模型如何工作的某种形式的普遍真理，这将是有道理的。
* 我们可以将LSTM的多层堆叠在一起。由于几个简单层的组合，因此导致机制更加灵活。特别是，数据可能在堆栈的不同级别相关。例如，我们可能想保留有关金融市场状况（熊市或牛市）的高级数据，而在较低的水平上，我们仅记录短期的时间动态。

除了所有这些抽象讨论之外，通过回顾图9.3.1可能最容易理解我们感兴趣的模型系列。它描述了具有LL隐藏层的深度递归神经网络。每个隐藏状态都连续传递到当前层的下一个时间步和下一层的当前时间步。

图9.3.1深度递归神经网络的架构。

## 功能依赖

在时间步tt，我们假设我们有一个小批量Xt∈Rn×dXt∈Rn×d（示例数：nn，输入数：dd）。隐藏层layer（ℓ= 1，…，Lℓ= 1，…，L）的隐藏状态为H（ℓ）t∈Rn×hHt（ℓ）∈Rn×h（隐藏单位数：hh），输出 层变量为Ot∈Rn×qOt∈Rn×q（输出数量：qq）和层ll的隐藏层激活函数flfl。我们像以前一样使用XtXt作为输入来计算第11层的隐藏状态。对于所有后续层，将使用前一层的隐藏状态。

（9.3.1）

H（1）tH（l）t = f1（Xt，H（1）t-1），= fl（H（1-1）t，H（l）t-1）。

最后，输出层仅基于隐藏层LL的隐藏状态。我们使用输出函数gg来解决此问题：

（9.3.2）

Ot ＝ g（H（L）t）。

与多层感知器一样，隐藏层LL的数量和隐藏单元hh的数量也是超参数。特别是，我们可以选择常规的RNN，GRU或LSTM来实现模型。

## 简洁的实现

幸运的是，在Gluon中可以很容易地获得实现多层RNN所需的许多后勤细节。为了保持简单，我们只使用这样的内置功能来说明实现。该代码与我们之前在LSTMs中使用的代码非常相似。实际上，唯一的区别是我们显式地指定层的数量，而不是选择单个层的默认值。让我们从导入适当的模块和加载数据开始。

TODO:CODE

体系结构决策（例如选择参数）与前面的部分非常相似。我们选择相同数量的输入和输出，因为我们拥有不同的令牌，即vocab_size。隐藏单位的数量仍然是256。唯一的区别是，我们现在选择了不平凡的层数num_layers = 2。

TODO:CODE

## 训练

实际的调用逻辑与之前相同。唯一的区别是，我们现在使用LSTM实例化两层。这种相当复杂的体系结构和大量的纪元大大减慢了训练的速度。

TODO:CODE

## 小结

* 在深度递归神经网络中，隐藏状态信息被传递到当前层的下一个时间步长和下一层的当前时间步长。
* 深层神经网络存在许多不同的类型，如LSTMs、GRUs或常规的神经网络。这些模型都可以方便地作为胶子rnn模块的一部分。
* 模型的初始化需要小心。总的来说，深度神经网络需要大量的工作(比如学习速率和裁剪)来确保适当的收敛。


## 练习

1. 尝试使用我们在8.5节中讨论的单层实现从头开始实现一个两层RNN。
1. 用GRU代替LSTM，比较其精度。
1. 增加培训数据以包括多本书。你能把困惑的程度降到多低?
1. 在建模文本时，您是否希望合并不同作者的资源?为什么这是个好主意?会出什么问题呢?
