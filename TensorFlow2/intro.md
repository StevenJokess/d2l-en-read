

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-17 15:10:25
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-17 22:38:55
 * @Description:
 * @TODO::
 * @Reference:
-->
```python
import tensorflow as tf
# æ­¤å¤„ä»£ç éœ€è¦ä½¿ç”¨ tf 2 ç‰ˆæœ¬è¿è¡Œ # 1.åˆ›å»ºè¾“å…¥å¼ é‡ï¼Œå¹¶èµ‹åˆå§‹å€¼ a = tf.constant(2.) b = tf.constant(4.)
# 2.ç›´æ¥è®¡ç®—ï¼Œå¹¶æ‰“å°ç»“æœ
print('a+b=',a+b)
```

è¿™ç§è¿ç®—æ—¶åŒæ—¶åˆ›å»ºè®¡ç®—å›¾ğ‘ = ğ‘ + ğ‘å’Œæ•°å€¼ç»“æœ6.0 = 2.0 + 4.0çš„æ–¹å¼å«åšå‘½ä»¤å¼ç¼– ç¨‹ï¼Œä¹Ÿç§°ä¸ºåŠ¨æ€å›¾æ¨¡å¼ã€‚TensorFlow 2 å’Œ PyTorch éƒ½æ˜¯é‡‡ç”¨åŠ¨æ€å›¾(ä¼˜å…ˆ)æ¨¡å¼å¼€å‘ï¼Œè°ƒè¯•æ–¹ ä¾¿ï¼Œæ‰€è§å³æ‰€å¾—ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒåŠ¨æ€å›¾æ¨¡å¼å¼€å‘æ•ˆç‡é«˜ï¼Œä½†æ˜¯è¿è¡Œæ•ˆç‡å¯èƒ½ä¸å¦‚é™æ€å›¾æ¨¡ å¼ã€‚TensorFlow 2 ä¹Ÿæ”¯æŒé€šè¿‡ tf.function å°†åŠ¨æ€å›¾ä¼˜å…ˆæ¨¡å¼çš„ä»£ç è½¬åŒ–ä¸ºé™æ€å›¾æ¨¡å¼ï¼Œå®ç° å¼€å‘å’Œè¿è¡Œæ•ˆç‡çš„åŒèµ¢

```python
# åˆ›å»ºåœ¨ CPU ç¯å¢ƒä¸Šè¿ç®—çš„ 2 ä¸ªçŸ©é˜µ
with tf.device('/cpu:0'):
    cpu_a = tf.random.normal([1, n])
    cpu_b = tf.random.normal([n, 1])
    print(cpu_a.device, cpu_b.device)
# åˆ›å»ºä½¿ç”¨ GPU ç¯å¢ƒè¿ç®—çš„ 2 ä¸ªçŸ©é˜µ
with tf.device('/gpu:0'):
    gpu_a = tf.random.normal([1, n])
    gpu_b = tf.random.normal([n, 1])
    print(gpu_a.device, gpu_b.device)
```

TensorFlow åœ¨è¿è¡Œæ—¶ï¼Œé»˜è®¤ä¼šå ç”¨æ‰€æœ‰ GPU æ˜¾å­˜èµ„æºï¼Œè¿™æ˜¯éå¸¸ä¸å‹å¥½çš„è¡Œä¸ºï¼Œå°¤å…¶ æ˜¯å½“è®¡ç®—æœºåŒæ—¶æœ‰å¤šä¸ªç”¨æˆ·æˆ–è€…ç¨‹åºåœ¨ä½¿ç”¨ GPU èµ„æºæ—¶ï¼Œå ç”¨æ‰€æœ‰ GPU æ˜¾å­˜èµ„æºä¼šä½¿å¾— å…¶ä»–ç¨‹åºæ— æ³•è¿è¡Œã€‚å› æ­¤ï¼Œä¸€èˆ¬æ¨èè®¾ç½® TensorFlow çš„æ˜¾å­˜å ç”¨æ–¹å¼ä¸ºå¢é•¿å¼å ç”¨æ¨¡å¼ï¼Œ å³æ ¹æ®å®é™…æ¨¡å‹å¤§å°ç”³è¯·æ˜¾å­˜èµ„æºï¼Œä»£ç å®ç°å¦‚ä¸‹ï¼š

```python
# è®¾ç½® GPU æ˜¾å­˜ä½¿ç”¨æ–¹å¼
# è·å– GPU è®¾å¤‡åˆ—è¡¨
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
    # è®¾ç½® GPU ä¸ºå¢é•¿å¼å ç”¨
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)    except RuntimeError as e:
    # æ‰“å°å¼‚å¸¸
    print(e)
```


```python
with tf.GradientTape() as tape: # æ„å»ºæ¢¯åº¦è®°å½•ç¯å¢ƒ
    # æ‰“å¹³æ“ä½œï¼Œ[b, 28, 28] => [b, 784]
    x = tf.reshape(x, (-1, 28*28))
    # Step1. å¾—åˆ°æ¨¡å‹è¾“å‡º
    output [b, 784] => [b, 10]
    out = model(x)
    # [b] => [b, 10]
    y_onehot = tf.one_hot(y, depth=10)
    # è®¡ç®—å·®çš„å¹³æ–¹å’Œï¼Œ[b, 10]
    loss = tf.square(out-y_onehot)
    # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¹³å‡è¯¯å·®ï¼Œ[b]
    loss = tf.reduce_sum(loss) / x.shape[0]
å†åˆ©ç”¨ TensorFlow æä¾›çš„è‡ªåŠ¨æ±‚å¯¼å‡½æ•° tape.gradient(loss, model.trainable_variables)æ±‚å‡ºæ¨¡ å‹ä¸­æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦ä¿¡æ¯ğœ•â„’ ğœ•ğœƒ ,ğœƒ âˆˆ {ğ‘¾1,ğ’ƒ1,ğ‘¾2,ğ’ƒ2,ğ‘¾3,ğ’ƒ3}ã€‚
    # Step3. è®¡ç®—å‚æ•°çš„æ¢¯åº¦ w1, w2, w3, b1, b2, b3
    grads = tape.gradient(loss, model.trainable_variables)
```

è®¡ç®—è·å¾—çš„æ¢¯åº¦ç»“æœä½¿ç”¨ grads åˆ—è¡¨å˜é‡ä¿å­˜ã€‚å†ä½¿ç”¨ optimizers å¯¹è±¡è‡ªåŠ¨æŒ‰ç…§æ¢¯åº¦æ›´æ–°æ³• åˆ™å»æ›´æ–°æ¨¡å‹çš„å‚æ•°ğœƒã€‚
ğœƒâ€² = ğœƒ âˆ’ğœ‚ âˆ™ğœ•â„’ /ğœ•ğœƒ

å®ç°å¦‚ä¸‹ã€‚

```py
# è‡ªåŠ¨è®¡ç®—æ¢¯åº¦
grads = tape.gradient(loss, model.trainable_variables)
# w' = w - lr * gradï¼Œæ›´æ–°ç½‘ç»œå‚æ•°
optimizer.apply_gradients(zip(grads, model.trainable_variables))
```

[1]: https://github.com/dragen1860/Deep-Learning-with-TensorFlow-book/blob/master/%E3%80%90%E3%80%8ATensorFlow%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E3%80%8B%E3%80%91.pdf
[2]: https://medium.com/coinmonks/8-things-to-do-differently-in-tensorflows-eager-execution-mode-47cf429aa3ad
