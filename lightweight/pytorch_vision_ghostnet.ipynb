{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_vision_ghostnet.ipynb",
      "provenance": []
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fed3aa65054449e8030de443f8ac022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23c46194bf944a46994c529802b009a5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5a449ca6c2ca46099c6588ab1bc82437",
              "IPY_MODEL_f9cf5b2e436b4b30828bc383d30b030f"
            ]
          }
        },
        "23c46194bf944a46994c529802b009a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a449ca6c2ca46099c6588ab1bc82437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1dda0067bb0349cb861641018c96e45a",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 20923722,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20923722,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_db463835194e4cbbb327891f456b4a1c"
          }
        },
        "f9cf5b2e436b4b30828bc383d30b030f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1cb353ae41ed409cb2a1a31bfb5e3bf8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20.0M/20.0M [00:00&lt;00:00, 70.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e26c37756c2433abcc65eb9f3259f12"
          }
        },
        "1dda0067bb0349cb861641018c96e45a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "db463835194e4cbbb327891f456b4a1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1cb353ae41ed409cb2a1a31bfb5e3bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e26c37756c2433abcc65eb9f3259f12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-UpgkzuFMX4"
      },
      "source": [
        "### This notebook is optionally accelerated with a GPU runtime.\n",
        "### If you would like to use this acceleration, please select the menu option \"Runtime\" -> \"Change runtime type\", select \"Hardware Accelerator\" -> \"GPU\" and click \"SAVE\"\n",
        "\n",
        "----------------------------------------------------------------------\n",
        "\n",
        "# GhostNet\n",
        "\n",
        "*Author: Huawei Noah's Ark Lab*\n",
        "\n",
        "**Efficient networks by generating more features from cheap operations**\n",
        "\n",
        "<img src=\"https://pytorch.org/assets/images/ghostnet.png\" alt=\"alt\" width=\"50%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_3RGbxXFMYD",
        "outputId": "fd3bb3f7-e98b-474c-c353-7cf7e07477f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8fed3aa65054449e8030de443f8ac022",
            "23c46194bf944a46994c529802b009a5",
            "5a449ca6c2ca46099c6588ab1bc82437",
            "f9cf5b2e436b4b30828bc383d30b030f",
            "1dda0067bb0349cb861641018c96e45a",
            "db463835194e4cbbb327891f456b4a1c",
            "1cb353ae41ed409cb2a1a31bfb5e3bf8",
            "8e26c37756c2433abcc65eb9f3259f12"
          ]
        }
      },
      "source": [
        "import torch\n",
        "model = torch.hub.load('huawei-noah/ghostnet', 'ghostnet_1x', pretrained=True)\n",
        "model.eval()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/huawei-noah/ghostnet/archive/master.zip\" to /root/.cache/torch/hub/master.zip\n",
            "Downloading: \"https://github.com/huawei-noah/ghostnet/raw/master/ghostnet_pytorch/models/state_dict_73.98.pth\" to /root/.cache/torch/hub/checkpoints/state_dict_73.98.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fed3aa65054449e8030de443f8ac022",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=20923722.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GhostNet(\n",
              "  (conv_stem): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (act1): ReLU(inplace=True)\n",
              "  (blocks): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
              "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=8, bias=False)\n",
              "            (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
              "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (conv_dw): Conv2d(48, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=48, bias=False)\n",
              "        (bn_dw): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(48, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
              "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
              "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
              "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(72, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=12, bias=False)\n",
              "            (1): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(24, 36, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(36, 36, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=36, bias=False)\n",
              "            (1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (conv_dw): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "        (bn_dw): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(20, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(72, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
              "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(24, 24, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=24, bias=False)\n",
              "          (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(40, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=60, bias=False)\n",
              "            (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(120, 20, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=20, bias=False)\n",
              "            (1): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=120, bias=False)\n",
              "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (conv_dw): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "        (bn_dw): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=40, bias=False)\n",
              "          (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(80, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
              "            (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(200, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (1): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
              "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(80, 92, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(92, 92, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=92, bias=False)\n",
              "            (1): BatchNorm2d(92, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(184, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(40, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=40, bias=False)\n",
              "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (3): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(80, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=240, bias=False)\n",
              "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(480, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
              "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "          (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(80, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (4): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
              "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(672, 56, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(56, 56, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=56, bias=False)\n",
              "            (1): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(112, 336, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(336, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=336, bias=False)\n",
              "            (1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (conv_dw): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "        (bn_dw): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential(\n",
              "          (0): Conv2d(112, 112, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=112, bias=False)\n",
              "          (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): Conv2d(112, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (1): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (2): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "      (3): GhostBottleneck(\n",
              "        (ghost1): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(160, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "        )\n",
              "        (se): SqueezeExcite(\n",
              "          (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (conv_reduce): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (act1): ReLU(inplace=True)\n",
              "          (conv_expand): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (ghost2): GhostModule(\n",
              "          (primary_conv): Sequential(\n",
              "            (0): Conv2d(960, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "          (cheap_operation): Sequential(\n",
              "            (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
              "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "            (2): Sequential()\n",
              "          )\n",
              "        )\n",
              "        (shortcut): Sequential()\n",
              "      )\n",
              "    )\n",
              "    (9): Sequential(\n",
              "      (0): ConvBnAct(\n",
              "        (conv): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (act1): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (conv_head): Conv2d(960, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "  (act2): ReLU(inplace=True)\n",
              "  (classifier): Linear(in_features=1280, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POD0odHCFMYE"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(3 x H x W)`, where `H` and `W` are expected to be at least `224`.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "Here's a sample execution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7JwltF8FMYF"
      },
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk2dtbdvFMYF",
        "outputId": "0a3429af-c921-4199-be6f-33e0ed802b04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "print(output[0])\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "print(torch.nn.functional.softmax(output[0], dim=0))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 1.2017e-01, -1.2157e+00, -8.6316e-01, -4.4031e-01, -1.4798e-01,\n",
            "        -6.2174e-02,  1.0053e+00,  1.4616e+00,  1.4725e+00, -5.8533e-01,\n",
            "        -7.5813e-01,  2.0558e-01, -4.3331e-01, -6.5629e-01, -6.7759e-01,\n",
            "        -2.2912e-01,  6.6753e-01, -3.9376e-02,  9.1357e-01, -5.2970e-02,\n",
            "        -4.8870e-01,  9.5270e-02,  8.0659e-01, -4.3592e-01, -8.0729e-01,\n",
            "         2.0196e-01, -4.8348e-01, -1.7261e-01,  4.1925e-02,  1.5424e+00,\n",
            "        -3.2623e-01,  3.5553e-01,  3.0908e-02,  3.2748e-01, -5.1211e-01,\n",
            "        -7.7316e-01, -5.4865e-01, -2.5402e-01,  3.6358e-01,  9.5621e-01,\n",
            "         8.4304e-02,  4.8824e-01, -5.9983e-01, -3.5944e-01,  2.0398e-01,\n",
            "         4.2388e-01,  1.1903e-02,  1.3735e-01, -5.5075e-01, -6.8120e-01,\n",
            "         8.5751e-03,  1.3971e-01,  1.0920e+00, -6.9391e-01,  2.5174e-01,\n",
            "         4.8245e-01, -6.6205e-01, -1.8027e-02, -3.6690e-01,  6.3752e-01,\n",
            "         1.2044e+00, -3.8001e-01, -4.2095e-01, -2.1851e-02, -5.1857e-01,\n",
            "        -4.9487e-01, -2.6857e-01, -2.9963e-01,  1.6039e-01, -3.8033e-01,\n",
            "         2.2654e-01,  5.3429e-01,  7.9743e-02, -8.6469e-02,  1.1860e-01,\n",
            "        -3.1701e-01, -1.1469e+00,  6.3420e-02,  3.9594e-01,  6.6800e-02,\n",
            "        -1.1721e+00,  5.1619e-01, -4.3927e-01, -8.3999e-01,  1.1256e+00,\n",
            "         2.0557e-01, -4.0667e-01, -1.7489e+00, -5.3319e-01,  5.9000e-01,\n",
            "         6.9989e-01, -3.2474e-01, -9.3633e-02, -5.9179e-01, -4.6837e-01,\n",
            "        -1.0508e+00, -2.6829e-01,  6.2305e-02, -1.8205e-01,  3.5111e-01,\n",
            "         5.0944e-01, -1.1524e+00, -8.4985e-01, -7.9732e-01,  3.0599e+00,\n",
            "        -7.5720e-01, -2.8710e-01, -1.2391e+00,  8.6377e-02, -4.3255e-01,\n",
            "        -3.5158e-01, -9.7633e-01,  5.4875e-01, -1.6900e-01, -1.0227e-02,\n",
            "         1.3449e+00, -5.6420e-01, -1.4246e-01,  7.9158e-02,  5.1798e-01,\n",
            "        -7.0587e-02,  1.0076e+00, -8.1908e-02,  4.3722e-01,  4.3771e-01,\n",
            "        -9.1766e-02, -7.8374e-01,  3.0597e-01, -8.7130e-01, -8.0730e-02,\n",
            "        -4.9732e-01, -1.2754e+00,  1.8549e-01, -1.9977e+00,  2.6067e-01,\n",
            "        -8.5437e-01, -1.3460e+00, -9.8489e-01,  3.0439e-01, -1.4841e-02,\n",
            "        -1.2336e+00, -7.0869e-01, -8.0205e-02,  2.0444e-01,  9.8448e-01,\n",
            "        -1.2539e+00,  1.7664e-01, -6.6280e-01,  8.1106e-02, -7.2238e-01,\n",
            "         1.1343e-02,  2.4255e+00,  2.6685e+00,  8.5475e-01,  1.1361e+00,\n",
            "        -9.8677e-01, -5.3606e-01,  3.0512e+00, -4.6890e-01, -1.3326e-01,\n",
            "        -1.1229e+00, -1.5741e+00, -1.4157e+00, -9.9575e-01,  1.1555e-02,\n",
            "        -1.1182e+00,  2.2758e-02, -1.3546e+00, -3.7008e-01,  6.7553e-01,\n",
            "         1.3640e+00, -2.3757e-01, -7.5715e-01,  1.2876e+00,  7.8723e-01,\n",
            "        -2.5627e-01, -2.0321e+00, -5.4393e-01, -1.0203e+00,  3.4458e-01,\n",
            "         2.7225e-01, -1.8749e+00, -1.3628e-01, -2.1319e-01, -1.8908e-01,\n",
            "        -3.0451e-01,  1.1086e+00, -7.1538e-01,  8.2762e-02, -1.0729e+00,\n",
            "        -6.7088e-01, -6.5230e-01,  9.9349e-01,  9.4271e-03, -7.1489e-01,\n",
            "         1.2818e+00, -9.0179e-01,  1.4972e-01, -1.8149e+00,  6.7461e-01,\n",
            "        -8.1809e-02, -5.8968e-01, -6.5178e-01,  2.7910e+00, -9.0069e-01,\n",
            "        -7.2509e-01, -1.1132e+00,  1.3348e-01, -3.5568e-01, -8.5955e-01,\n",
            "         1.9465e-01, -4.8989e-01,  1.0186e+00, -5.7190e-01, -7.4916e-01,\n",
            "        -2.5320e-02, -1.0283e+00, -1.1605e-01, -5.8719e-01, -9.3257e-01,\n",
            "        -1.1759e+00, -7.7403e-01,  2.1108e+00,  2.9527e+00,  4.4403e-01,\n",
            "         1.7608e-01,  1.3174e+00, -2.0846e-01,  9.0070e-01,  1.0179e+00,\n",
            "         2.9025e+00,  2.7438e+00,  2.6912e+00,  1.2854e-01, -7.6156e-01,\n",
            "         6.9002e-01,  3.9925e-01, -3.2534e-01,  4.6264e-01,  2.6825e-02,\n",
            "         1.5998e-02,  6.4680e-01,  9.2224e-01, -2.7440e-01, -6.4295e-01,\n",
            "        -1.6551e-01,  9.6117e-01,  1.0529e+00,  4.6087e+00,  2.9698e+00,\n",
            "         3.6836e+00,  1.3276e+00, -8.6419e-02,  9.0974e-02, -7.7997e-01,\n",
            "        -2.5782e-01,  1.1500e+00,  3.1696e+00,  8.2066e+00,  5.7784e+00,\n",
            "         2.2777e+00,  2.9714e+00,  3.0526e-01,  1.0166e+00,  6.8363e-01,\n",
            "        -1.5445e-01, -3.0946e-01, -8.6976e-02, -1.1470e+00,  8.6764e-01,\n",
            "         4.1018e+00, -1.4165e-01, -4.1080e-01,  9.5332e-01,  4.4251e-01,\n",
            "        -1.0644e+00,  3.0179e-01,  6.2277e-01,  3.6930e-01,  4.0906e+00,\n",
            "         7.5843e-01,  2.9590e-01, -5.3701e-01,  1.8653e+00,  6.0583e-01,\n",
            "         1.1813e+00, -1.3188e+00, -5.7879e-01,  9.3689e-02,  7.7287e-01,\n",
            "        -7.3580e-02, -6.4409e-01,  3.4728e-02, -3.1962e-01, -7.6585e-01,\n",
            "        -9.5879e-01, -6.1616e-01, -1.3792e+00, -1.3853e+00, -1.0273e+00,\n",
            "        -1.4351e-02, -1.1270e-01, -6.2315e-01, -7.5125e-01,  7.0681e-01,\n",
            "         2.5937e-02,  4.6258e-01, -6.6152e-01,  3.0527e-01, -8.4535e-01,\n",
            "         4.7529e-01, -3.7113e-01,  6.5129e-01,  6.6975e-01,  2.6050e-01,\n",
            "         5.0292e-01,  2.3346e-01,  1.3402e-03,  1.2049e-02, -7.3330e-01,\n",
            "        -2.9284e-01, -1.2774e-01, -8.5649e-01,  2.3746e-01, -1.2287e+00,\n",
            "        -3.7398e-02, -6.2317e-01,  8.0352e-01,  2.3826e-01,  3.5365e-03,\n",
            "        -1.1448e-01,  1.1633e+00,  2.5971e+00,  2.3974e-01, -6.9833e-02,\n",
            "         9.9355e-01, -4.9668e-01, -1.5966e-01,  1.3216e-01,  2.8089e-01,\n",
            "        -3.8069e-01, -4.7902e-01, -1.9019e-01, -8.7585e-02, -3.7155e-01,\n",
            "        -7.6740e-01, -3.5827e-02,  6.8168e-02,  9.2462e-01, -6.5968e-02,\n",
            "        -1.2037e+00,  1.4997e-01, -1.1069e+00, -8.5125e-01, -6.8483e-01,\n",
            "         2.4256e+00,  1.1067e-01, -4.9788e-01,  1.7368e+00,  9.6779e-01,\n",
            "        -1.9372e+00,  2.4017e+00,  8.4426e-01,  5.0100e-01, -8.1819e-01,\n",
            "        -8.0031e-02, -1.1224e+00, -5.0102e-01, -9.7104e-02, -3.3890e-01,\n",
            "        -8.7205e-01, -1.5933e+00, -8.1783e-01, -4.3868e-01,  3.8633e-01,\n",
            "        -2.1416e-01, -4.3426e-01, -3.8431e-01,  1.8776e-01, -5.7102e-01,\n",
            "        -7.2574e-01, -6.4480e-01, -2.1250e-01,  1.3918e-01, -1.1330e+00,\n",
            "        -5.6699e-01, -1.0275e+00,  4.0249e-01,  1.3042e-01, -6.0557e-01,\n",
            "        -1.5578e-01, -6.1526e-01, -4.5421e-01, -1.5121e-01, -6.5866e-01,\n",
            "        -6.9439e-01,  4.5021e-01, -8.5426e-01, -1.0552e+00, -8.4637e-01,\n",
            "        -2.6393e-01, -4.1703e-02, -6.8781e-01,  1.2218e-01,  5.1542e-01,\n",
            "         1.2646e-01, -3.7623e-01,  2.5075e-01, -7.2171e-01,  5.5707e-02,\n",
            "        -5.4113e-02, -6.2859e-01,  1.6612e-01, -4.3532e-02, -2.2010e-01,\n",
            "         8.9033e-01, -1.9928e-01, -2.4060e-01, -1.1942e+00, -3.4079e-01,\n",
            "        -4.1702e-01,  2.0335e-01, -4.1304e-01, -2.7517e-01, -3.5582e-01,\n",
            "         1.2506e+00,  2.2780e-01, -1.0841e+00, -5.4296e-01, -5.8250e-02,\n",
            "        -6.9426e-01, -1.8837e-01,  5.3669e-01, -2.0386e+00, -5.9547e-02,\n",
            "        -8.2888e-01, -4.8975e-02,  7.7588e-01, -1.3328e+00, -1.0489e-01,\n",
            "         5.7416e-01,  4.8898e-01,  3.8249e-01, -2.3750e-01,  6.8471e-01,\n",
            "         6.9786e-02,  2.1221e-01, -5.8867e-01,  5.0084e-01,  3.4576e-01,\n",
            "         3.9317e-01, -5.5552e-01,  5.3301e-01,  5.0514e-01,  9.0821e-01,\n",
            "        -5.6957e-01, -1.3808e-01, -6.7278e-01, -3.5637e-01,  2.7933e-01,\n",
            "         2.2289e+00, -1.1168e+00, -6.1566e-01, -3.8212e-02,  3.5542e-02,\n",
            "        -1.3656e-01, -1.2177e+00,  7.2268e-01,  3.2184e-01, -1.6260e+00,\n",
            "         2.2047e+00, -4.1029e-01,  1.5484e-01,  5.6019e-01,  2.6677e-01,\n",
            "        -3.8237e-01, -1.3961e-01, -2.2954e-01,  6.1479e-01,  2.3900e-01,\n",
            "         3.1175e-01, -1.3358e+00, -1.2963e+00,  1.3927e+00,  6.5730e-01,\n",
            "        -3.5688e-01,  3.9044e-01, -7.6101e-02,  1.0614e-01,  1.0716e+00,\n",
            "         4.8194e-01, -4.7829e-01, -8.3172e-01,  3.9102e-01, -6.0123e-01,\n",
            "        -3.4716e-01,  2.5495e-01,  9.4554e-01,  4.7483e-01, -1.7651e+00,\n",
            "         3.0147e-02,  6.6477e-01, -5.2592e-01, -1.0920e+00, -5.0789e-01,\n",
            "        -5.3747e-01, -1.7801e+00, -4.1711e-01, -1.0417e-01, -1.9206e-01,\n",
            "         2.9496e-01,  5.4229e-01,  1.0263e+00, -3.2335e-01,  5.3929e-01,\n",
            "         1.2261e+00, -4.4695e-01, -5.5222e-01, -1.2582e+00,  1.9667e-01,\n",
            "        -6.8727e-01, -8.2419e-01,  7.7651e-01, -3.4362e-01,  6.0838e-01,\n",
            "         1.6633e-01, -1.6531e+00,  9.3371e-02,  5.5540e-01, -8.2898e-01,\n",
            "         7.1447e-01, -2.9008e-01, -7.3548e-02, -7.3294e-01, -1.0018e+00,\n",
            "         1.5664e-01, -1.3451e-01,  1.1898e+00, -6.3232e-02,  5.6854e-01,\n",
            "        -1.2371e+00, -8.3744e-01, -6.7139e-01, -1.1234e+00,  2.8217e-01,\n",
            "         6.1443e-01,  3.1704e-01, -4.1022e-01, -4.1094e-01, -1.7225e+00,\n",
            "         4.7891e-01, -5.2595e-01,  8.9561e-01, -1.3816e-01,  6.2623e-01,\n",
            "        -6.6561e-01,  6.3823e-01,  8.5824e-01,  2.1833e-01,  6.1223e-01,\n",
            "         3.4710e-01, -6.5733e-01,  2.3919e-01, -1.1423e+00, -5.9575e-01,\n",
            "         4.8318e-03, -5.6199e-01,  4.0574e-01,  2.7491e-01,  7.5150e-01,\n",
            "        -9.2032e-01, -1.3501e+00, -1.2059e-01, -8.8212e-02,  7.7560e-02,\n",
            "        -1.6158e-01, -3.4891e-01, -8.1216e-01,  4.0081e-01, -3.5100e-01,\n",
            "        -3.8394e-01,  2.3397e-01,  3.5763e-01, -1.5067e+00, -5.0933e-01,\n",
            "        -1.0112e+00, -9.8702e-01,  1.1727e+00, -1.0969e+00, -7.7303e-02,\n",
            "         3.6881e-01, -2.3871e-01,  6.0574e-01, -3.1086e-01,  5.9886e-01,\n",
            "        -5.4277e-01,  4.4754e-02,  5.4046e-01, -3.1402e-01,  8.0073e-02,\n",
            "        -1.8563e-01,  3.1259e-01,  2.4064e-01,  1.4499e-01, -6.2771e-01,\n",
            "         8.0997e-01,  7.0280e-01,  3.2467e-01,  1.2122e+00,  3.7244e-01,\n",
            "        -7.9300e-01,  4.9046e-01, -5.1304e-02, -4.7649e-01,  5.0584e-01,\n",
            "        -1.2696e+00, -1.1256e+00,  8.2377e-02,  1.3242e-01, -2.8485e-01,\n",
            "         1.2419e-02,  1.2339e+00, -1.1754e+00, -1.0227e+00,  1.4494e-01,\n",
            "        -2.7391e-01,  9.4912e-02,  1.3633e+00, -1.1221e-02, -5.4145e-01,\n",
            "         1.4102e-01,  1.6611e+00,  4.1098e-01, -1.2711e+00, -1.0454e+00,\n",
            "        -5.9840e-02, -1.7384e+00,  6.3322e-01,  2.8241e-01, -8.6521e-01,\n",
            "         1.2074e+00, -7.0172e-01, -5.1283e-01,  4.1084e-01, -4.1960e-01,\n",
            "         9.9357e-01,  1.1699e+00, -4.5896e-01, -8.4578e-01, -2.1386e-01,\n",
            "        -7.3130e-01,  3.8942e-01,  3.1382e-01,  6.3610e-02,  7.1254e-01,\n",
            "         2.1725e-01,  8.2044e-01,  1.4266e-01, -4.9927e-01, -1.0632e+00,\n",
            "         1.3722e-01, -8.6938e-02,  2.2034e-03, -6.6209e-01,  2.0146e-01,\n",
            "         2.6168e-01, -1.3751e+00, -9.1973e-01,  2.5846e-01, -9.5120e-01,\n",
            "        -7.8495e-01,  5.7984e-01, -7.6785e-02, -7.4047e-01,  4.9295e-01,\n",
            "         2.9420e-01,  3.3801e-02,  6.0554e-01, -7.1891e-01,  1.1592e-01,\n",
            "        -2.3310e-01, -4.3617e-01, -3.5733e-01,  9.5482e-01, -1.1595e-01,\n",
            "         2.2047e-01, -8.5243e-01, -4.1056e-01,  9.1369e-01,  2.7131e-01,\n",
            "        -7.3503e-01, -8.3899e-01, -1.4446e+00, -6.1165e-01, -6.8451e-01,\n",
            "        -1.9697e-01, -1.8792e+00,  7.4614e-01, -1.0116e-01, -4.2522e-01,\n",
            "        -1.8225e+00, -1.0188e+00, -7.8748e-01,  1.0198e+00, -6.7891e-01,\n",
            "        -1.2893e+00, -1.5808e-01,  3.0206e-01,  3.9269e-01, -1.5280e+00,\n",
            "        -8.1819e-01, -4.4382e-01, -5.0783e-02, -1.1215e-01, -1.1083e+00,\n",
            "        -8.5601e-01,  4.9849e-01,  7.7051e-02,  7.5440e-02, -3.9283e-01,\n",
            "        -4.1233e-01,  3.4494e-01,  1.3187e-01,  1.1287e-01,  5.8660e-01,\n",
            "        -1.2530e+00,  5.1201e-02,  1.2299e-01, -1.0378e+00,  3.5555e-01,\n",
            "        -2.7162e-01,  9.6878e-02, -4.7668e-01,  3.0438e-01, -2.7134e-01,\n",
            "        -5.9614e-01, -1.3713e+00, -3.0777e-01, -1.1612e+00,  3.1459e-01,\n",
            "        -6.5029e-01,  3.3030e-01, -1.5060e-01, -5.9526e-01,  5.9407e-02,\n",
            "        -4.4708e-01, -1.6308e+00, -4.7266e-01,  1.2323e-01, -5.7037e-01,\n",
            "        -6.8581e-02,  5.7441e-01,  1.2284e+00, -4.7334e-01,  6.8766e-01,\n",
            "        -1.3046e+00,  4.0057e-01,  3.9587e-02, -8.7092e-01, -3.6595e-01,\n",
            "         1.8832e-01,  1.3181e+00,  4.6561e-02, -1.5418e-01,  3.2528e-01,\n",
            "         8.0237e-02, -3.6924e-02, -2.1904e+00,  1.1394e+00, -1.4673e-01,\n",
            "         2.3047e-01, -1.3403e-02, -4.6175e-01,  8.3620e-01, -9.7151e-02,\n",
            "        -6.2219e-01, -3.3649e-02, -8.2231e-02,  4.8802e-01,  7.4466e-01,\n",
            "         8.7059e-01,  3.3565e-01,  2.3038e-01, -6.5099e-01, -7.3456e-01,\n",
            "        -1.5868e-01, -1.5719e+00, -4.8249e-02,  5.9532e-01, -9.9994e-02,\n",
            "        -1.1662e+00,  1.1669e-01, -6.7211e-02, -2.6968e-01, -1.4500e-01,\n",
            "         1.6565e-02, -1.3446e+00, -5.8347e-02, -1.0978e+00,  4.1985e-01,\n",
            "         7.5157e-01, -6.7723e-01,  5.8976e-01,  1.5515e-01, -6.9374e-01,\n",
            "         2.4378e-01, -8.2264e-01, -2.5939e-01,  1.9054e-01,  4.0980e-01,\n",
            "         2.4189e-01, -3.0795e-01, -6.7133e-01, -3.9073e-01, -3.9424e-01,\n",
            "        -7.5727e-02,  5.7732e-02,  5.8011e-01,  5.4435e-01, -1.4835e-01,\n",
            "         3.5737e-01, -2.3680e-01,  2.6470e-01,  5.9873e-01,  2.3761e-01,\n",
            "         7.0081e-02,  7.9809e-01,  8.2733e-01,  2.8611e-01, -3.2861e-01,\n",
            "        -3.9866e-01,  1.9304e-01,  3.3430e-01,  4.2769e-01,  1.7890e-01,\n",
            "        -2.4288e-01, -6.9093e-01,  2.5839e-01, -5.2566e-01,  9.3598e-01,\n",
            "        -7.9771e-01,  4.2803e-01,  9.1057e-01,  8.5400e-02,  3.7610e-02,\n",
            "        -4.7212e-01, -7.5679e-01, -8.4184e-01, -1.5392e+00, -1.1814e-01,\n",
            "        -6.4562e-01, -4.5740e-01,  9.9492e-01,  2.1013e-01,  3.8472e-01,\n",
            "        -5.4830e-01, -1.3452e+00, -3.1317e-01,  6.4888e-01, -2.6927e-01,\n",
            "        -4.4077e-01, -6.5075e-01,  3.3662e-01,  9.0165e-02,  7.7567e-01,\n",
            "        -6.9236e-01, -5.6209e-01,  6.3573e-02,  1.1844e-01, -4.7624e-02,\n",
            "         2.7499e-01,  3.8243e-01, -6.0482e-01, -3.0917e-01,  7.8583e-02,\n",
            "        -4.8715e-01,  9.1335e-02, -4.4232e-01, -4.4155e-01, -5.0455e-01,\n",
            "         4.2123e-01, -7.6019e-01, -7.1297e-01, -9.6199e-01,  2.5683e-01,\n",
            "        -1.4922e-01, -6.8558e-02,  4.1806e-01, -8.8040e-01,  1.1221e-01,\n",
            "        -2.9045e-01,  4.6414e-01,  4.4811e-01, -6.9700e-02,  2.3801e-01,\n",
            "        -8.3506e-01, -2.5545e-02, -1.2675e+00, -2.0969e-02, -1.2893e+00,\n",
            "         1.0660e+00, -1.3577e-01, -8.7009e-01,  1.2947e-01,  5.3351e-01,\n",
            "         1.2634e-01, -9.3809e-01,  2.0959e-02,  7.7596e-02, -9.6498e-01,\n",
            "        -3.1904e-01,  4.6324e-01,  8.1016e-02,  1.0609e+00,  1.1846e+00,\n",
            "         7.4962e-01,  5.4170e-01, -8.7270e-01, -5.1127e-01,  1.2008e-01,\n",
            "         1.1117e+00, -2.2093e-01,  9.9252e-02, -2.3184e-01,  4.9650e-01,\n",
            "         4.1152e-01,  1.2790e-02,  4.2557e-01, -6.3409e-02, -1.0841e-01,\n",
            "         1.9163e-01, -1.7277e+00, -7.6773e-03,  2.0607e-01,  7.4533e-01,\n",
            "         2.4103e-01, -5.4561e-01,  2.3635e-02, -7.6042e-01, -1.1734e-01,\n",
            "        -3.4279e-01, -1.1551e+00, -1.2812e+00, -5.2763e-01, -9.8289e-01,\n",
            "        -7.6043e-01,  5.3885e-01, -9.4558e-01, -9.8213e-01,  2.7775e-01,\n",
            "        -6.3640e-01, -1.1622e+00,  1.8084e-01,  8.8509e-01, -4.8243e-01,\n",
            "        -4.0845e-01, -8.8346e-01,  7.3602e-02,  1.0998e-01, -1.6620e-01,\n",
            "         4.5026e-01, -2.6528e-01,  3.8333e-01,  4.1220e-01, -6.1793e-01,\n",
            "         5.3492e-01,  4.7545e-01,  2.8108e-01,  6.2530e-01, -2.6547e-02,\n",
            "         5.8226e-01,  6.3290e-01,  8.6417e-01, -2.9211e-01,  1.0605e-01,\n",
            "         1.4458e-02,  8.0104e-01,  9.1818e-01,  3.4820e-01, -1.1779e-01,\n",
            "        -9.2698e-01,  2.2611e-01, -6.2480e-02, -2.4291e-02,  4.9248e-01,\n",
            "         6.1261e-01,  6.4878e-01, -1.1781e+00,  2.4578e-01,  6.5397e-02,\n",
            "         8.2027e-01,  1.5219e-01, -6.3504e-01, -7.4657e-01, -1.1616e+00,\n",
            "         1.2512e-01, -4.2894e-01, -5.4766e-01,  4.8125e-01, -1.9406e+00],\n",
            "       device='cuda:0')\n",
            "tensor([2.0124e-04, 5.2911e-05, 7.5275e-05, 1.1489e-04, 1.5390e-04, 1.6769e-04,\n",
            "        4.8766e-04, 7.6960e-04, 7.7806e-04, 9.9382e-05, 8.3611e-05, 2.1918e-04,\n",
            "        1.1570e-04, 9.2575e-05, 9.0624e-05, 1.4191e-04, 3.4787e-04, 1.7156e-04,\n",
            "        4.4491e-04, 1.6924e-04, 1.0947e-04, 1.9629e-04, 3.9977e-04, 1.1540e-04,\n",
            "        7.9600e-05, 2.1839e-04, 1.1004e-04, 1.5016e-04, 1.8609e-04, 8.3435e-04,\n",
            "        1.2878e-04, 2.5464e-04, 1.8405e-04, 2.4759e-04, 1.0693e-04, 8.2364e-05,\n",
            "        1.0310e-04, 1.3842e-04, 2.5669e-04, 4.6429e-04, 1.9415e-04, 2.9077e-04,\n",
            "        9.7952e-05, 1.2457e-04, 2.1883e-04, 2.7265e-04, 1.8059e-04, 2.0472e-04,\n",
            "        1.0288e-04, 9.0297e-05, 1.7999e-04, 2.0521e-04, 5.3183e-04, 8.9156e-05,\n",
            "        2.2953e-04, 2.8910e-04, 9.2043e-05, 1.7526e-04, 1.2364e-04, 3.3759e-04,\n",
            "        5.9507e-04, 1.2203e-04, 1.1714e-04, 1.7459e-04, 1.0624e-04, 1.0879e-04,\n",
            "        1.3642e-04, 1.3225e-04, 2.0949e-04, 1.2199e-04, 2.2382e-04, 3.0448e-04,\n",
            "        1.9326e-04, 1.6367e-04, 2.0092e-04, 1.2997e-04, 5.6681e-05, 1.9013e-04,\n",
            "        2.6514e-04, 1.9078e-04, 5.5269e-05, 2.9902e-04, 1.1501e-04, 7.7039e-05,\n",
            "        5.5002e-04, 2.1918e-04, 1.1882e-04, 3.1044e-05, 1.0470e-04, 3.2192e-04,\n",
            "        3.5931e-04, 1.2897e-04, 1.6250e-04, 9.8743e-05, 1.1171e-04, 6.2398e-05,\n",
            "        1.3646e-04, 1.8992e-04, 1.4875e-04, 2.5351e-04, 2.9700e-04, 5.6366e-05,\n",
            "        7.6284e-05, 8.0398e-05, 3.8057e-03, 8.3688e-05, 1.3392e-04, 5.1689e-05,\n",
            "        1.9455e-04, 1.1579e-04, 1.2555e-04, 6.7220e-05, 3.0891e-04, 1.5070e-04,\n",
            "        1.7663e-04, 6.8485e-04, 1.0150e-04, 1.5476e-04, 1.9315e-04, 2.9955e-04,\n",
            "        1.6629e-04, 4.8878e-04, 1.6442e-04, 2.7631e-04, 2.7645e-04, 1.6280e-04,\n",
            "        8.1497e-05, 2.4232e-04, 7.4664e-05, 1.6461e-04, 1.0853e-04, 4.9843e-05,\n",
            "        2.1482e-04, 2.4205e-05, 2.3159e-04, 7.5940e-05, 4.6449e-05, 6.6647e-05,\n",
            "        2.4194e-04, 1.7582e-04, 5.1970e-05, 8.7849e-05, 1.6470e-04, 2.1893e-04,\n",
            "        4.7760e-04, 5.0927e-05, 2.1293e-04, 9.1974e-05, 1.9353e-04, 8.6655e-05,\n",
            "        1.8049e-04, 2.0178e-03, 2.5729e-03, 4.1950e-04, 5.5577e-04, 6.6522e-05,\n",
            "        1.0440e-04, 3.7727e-03, 1.1165e-04, 1.5619e-04, 5.8054e-05, 3.6973e-05,\n",
            "        4.3321e-05, 6.5927e-05, 1.8052e-04, 5.8332e-05, 1.8256e-04, 4.6049e-05,\n",
            "        1.2325e-04, 3.5067e-04, 6.9808e-04, 1.4071e-04, 8.3693e-05, 6.4675e-04,\n",
            "        3.9211e-04, 1.3811e-04, 2.3388e-05, 1.0358e-04, 6.4328e-05, 2.5186e-04,\n",
            "        2.3429e-04, 2.7368e-05, 1.5571e-04, 1.4419e-04, 1.4771e-04, 1.3160e-04,\n",
            "        5.4070e-04, 8.7263e-05, 1.9385e-04, 6.1034e-05, 9.1233e-05, 9.2944e-05,\n",
            "        4.8193e-04, 1.8014e-04, 8.7306e-05, 6.4299e-04, 7.2422e-05, 2.0727e-04,\n",
            "        2.9061e-05, 3.5034e-04, 1.6443e-04, 9.8951e-05, 9.2993e-05, 2.9084e-03,\n",
            "        7.2502e-05, 8.6420e-05, 5.8622e-05, 2.0393e-04, 1.2504e-04, 7.5547e-05,\n",
            "        2.1680e-04, 1.0934e-04, 4.9420e-04, 1.0073e-04, 8.4364e-05, 1.7399e-04,\n",
            "        6.3816e-05, 1.5890e-04, 9.9198e-05, 7.0227e-05, 5.5060e-05, 8.2292e-05,\n",
            "        1.4731e-03, 3.4188e-03, 2.7820e-04, 2.1281e-04, 6.6628e-04, 1.4487e-04,\n",
            "        4.3922e-04, 4.9384e-04, 3.2513e-03, 2.7741e-03, 2.6320e-03, 2.0293e-04,\n",
            "        8.3325e-05, 3.5579e-04, 2.6601e-04, 1.2889e-04, 2.8343e-04, 1.8330e-04,\n",
            "        1.8133e-04, 3.4073e-04, 4.4879e-04, 1.3563e-04, 9.3818e-05, 1.5123e-04,\n",
            "        4.6660e-04, 5.1141e-04, 1.7909e-02, 3.4775e-03, 7.1004e-03, 6.7310e-04,\n",
            "        1.6368e-04, 1.9544e-04, 8.1804e-05, 1.3789e-04, 5.6360e-04, 4.2466e-03,\n",
            "        6.5402e-01, 5.7683e-02, 1.7406e-03, 3.4833e-03, 2.4215e-04, 4.9320e-04,\n",
            "        3.5352e-04, 1.5291e-04, 1.3095e-04, 1.6358e-04, 5.6672e-05, 4.2494e-04,\n",
            "        1.0787e-02, 1.5488e-04, 1.1833e-04, 4.6295e-04, 2.7778e-04, 6.1552e-05,\n",
            "        2.4131e-04, 3.3264e-04, 2.5817e-04, 1.0666e-02, 3.8098e-04, 2.3990e-04,\n",
            "        1.0430e-04, 1.1524e-03, 3.2706e-04, 5.8151e-04, 4.7730e-05, 1.0003e-04,\n",
            "        1.9598e-04, 3.8652e-04, 1.6579e-04, 9.3711e-05, 1.8476e-04, 1.2963e-04,\n",
            "        8.2968e-05, 6.8410e-05, 9.6365e-05, 4.4932e-05, 4.4655e-05, 6.3879e-05,\n",
            "        1.7591e-04, 1.5943e-04, 9.5694e-05, 8.4188e-05, 3.6181e-04, 1.8314e-04,\n",
            "        2.8341e-04, 9.2092e-05, 2.4215e-04, 7.6628e-05, 2.8703e-04, 1.2312e-04,\n",
            "        3.4227e-04, 3.4865e-04, 2.3155e-04, 2.9507e-04, 2.2537e-04, 1.7869e-04,\n",
            "        1.8061e-04, 8.5713e-05, 1.3315e-04, 1.5705e-04, 7.5779e-05, 2.2628e-04,\n",
            "        5.2226e-05, 1.7190e-04, 9.5692e-05, 3.9855e-04, 2.2646e-04, 1.7908e-04,\n",
            "        1.5915e-04, 5.7110e-04, 2.3957e-03, 2.2680e-04, 1.6641e-04, 4.8196e-04,\n",
            "        1.0860e-04, 1.5212e-04, 2.0366e-04, 2.3632e-04, 1.2195e-04, 1.1053e-04,\n",
            "        1.4754e-04, 1.6348e-04, 1.2307e-04, 8.2839e-05, 1.7217e-04, 1.9104e-04,\n",
            "        4.4985e-04, 1.6706e-04, 5.3548e-05, 2.0732e-04, 5.8992e-05, 7.6176e-05,\n",
            "        8.9970e-05, 2.0181e-03, 1.9933e-04, 1.0846e-04, 1.0134e-03, 4.6970e-04,\n",
            "        2.5716e-05, 1.9705e-03, 4.1512e-04, 2.9451e-04, 7.8737e-05, 1.6472e-04,\n",
            "        5.8087e-05, 1.0813e-04, 1.6194e-04, 1.2715e-04, 7.4609e-05, 3.6272e-05,\n",
            "        7.8766e-05, 1.1508e-04, 2.6260e-04, 1.4405e-04, 1.1559e-04, 1.2151e-04,\n",
            "        2.1531e-04, 1.0081e-04, 8.6364e-05, 9.3645e-05, 1.4429e-04, 2.0510e-04,\n",
            "        5.7473e-05, 1.0122e-04, 6.3865e-05, 2.6688e-04, 2.0331e-04, 9.7391e-05,\n",
            "        1.5271e-04, 9.6452e-05, 1.1331e-04, 1.5341e-04, 9.2355e-05, 8.9114e-05,\n",
            "        2.7992e-04, 7.5948e-05, 6.2125e-05, 7.6549e-05, 1.3705e-04, 1.7116e-04,\n",
            "        8.9703e-05, 2.0164e-04, 2.9879e-04, 2.0251e-04, 1.2250e-04, 2.2931e-04,\n",
            "        8.6712e-05, 1.8867e-04, 1.6905e-04, 9.5175e-05, 2.1070e-04, 1.7085e-04,\n",
            "        1.4320e-04, 4.3469e-04, 1.4621e-04, 1.4029e-04, 5.4058e-05, 1.2691e-04,\n",
            "        1.1760e-04, 2.1869e-04, 1.1807e-04, 1.3552e-04, 1.2502e-04, 6.2324e-04,\n",
            "        2.2410e-04, 6.0351e-05, 1.0368e-04, 1.6835e-04, 8.9125e-05, 1.4781e-04,\n",
            "        3.0521e-04, 2.3236e-05, 1.6813e-04, 7.7900e-05, 1.6992e-04, 3.8768e-04,\n",
            "        4.7065e-05, 1.6068e-04, 3.1686e-04, 2.9099e-04, 2.6160e-04, 1.4073e-04,\n",
            "        3.5390e-04, 1.9135e-04, 2.2064e-04, 9.9051e-05, 2.9446e-04, 2.5216e-04,\n",
            "        2.6440e-04, 1.0239e-04, 3.0409e-04, 2.9573e-04, 4.4253e-04, 1.0096e-04,\n",
            "        1.5543e-04, 9.1061e-05, 1.2495e-04, 2.3595e-04, 1.6577e-03, 5.8409e-05,\n",
            "        9.6413e-05, 1.7176e-04, 1.8491e-04, 1.5567e-04, 5.2805e-05, 3.6760e-04,\n",
            "        2.4620e-04, 3.5102e-05, 1.6182e-03, 1.1839e-04, 2.0833e-04, 3.1247e-04,\n",
            "        2.3301e-04, 1.2175e-04, 1.5520e-04, 1.4185e-04, 3.3000e-04, 2.2663e-04,\n",
            "        2.4373e-04, 4.6922e-05, 4.8815e-05, 7.1841e-04, 3.4433e-04, 1.2489e-04,\n",
            "        2.6368e-04, 1.6537e-04, 1.9843e-04, 5.2110e-04, 2.8895e-04, 1.1061e-04,\n",
            "        7.7679e-05, 2.6384e-04, 9.7815e-05, 1.2611e-04, 2.3027e-04, 4.5937e-04,\n",
            "        2.8690e-04, 3.0546e-05, 1.8391e-04, 3.4691e-04, 1.0547e-04, 5.9879e-05,\n",
            "        1.0738e-04, 1.0425e-04, 3.0089e-05, 1.1759e-04, 1.6080e-04, 1.4727e-04,\n",
            "        2.3967e-04, 3.0692e-04, 4.9798e-04, 1.2915e-04, 3.0600e-04, 6.0812e-04,\n",
            "        1.1413e-04, 1.0273e-04, 5.0708e-05, 2.1723e-04, 8.9750e-05, 7.8266e-05,\n",
            "        3.8793e-04, 1.2656e-04, 3.2789e-04, 2.1074e-04, 3.4167e-05, 1.9591e-04,\n",
            "        3.1097e-04, 7.7892e-05, 3.6459e-04, 1.3352e-04, 1.6580e-04, 8.5744e-05,\n",
            "        6.5527e-05, 2.0871e-04, 1.5599e-04, 5.8646e-04, 1.6751e-04, 3.1509e-04,\n",
            "        5.1788e-05, 7.7236e-05, 9.1187e-05, 5.8025e-05, 2.3662e-04, 3.2988e-04,\n",
            "        2.4502e-04, 1.1840e-04, 1.1832e-04, 3.1876e-05, 2.8807e-04, 1.0546e-04,\n",
            "        4.3699e-04, 1.5542e-04, 3.3380e-04, 9.1716e-05, 3.3783e-04, 4.2096e-04,\n",
            "        2.2199e-04, 3.2916e-04, 2.5250e-04, 9.2478e-05, 2.2667e-04, 5.6939e-05,\n",
            "        9.8353e-05, 1.7931e-04, 1.0173e-04, 2.6775e-04, 2.3491e-04, 3.7835e-04,\n",
            "        7.1093e-05, 4.6258e-05, 1.5818e-04, 1.6338e-04, 1.9284e-04, 1.5182e-04,\n",
            "        1.2589e-04, 7.9213e-05, 2.6643e-04, 1.2563e-04, 1.2156e-04, 2.2549e-04,\n",
            "        2.5517e-04, 3.9550e-05, 1.0723e-04, 6.4919e-05, 6.6505e-05, 5.7650e-04,\n",
            "        5.9587e-05, 1.6517e-04, 2.5804e-04, 1.4055e-04, 3.2703e-04, 1.3077e-04,\n",
            "        3.2479e-04, 1.0370e-04, 1.8662e-04, 3.0636e-04, 1.3036e-04, 1.9333e-04,\n",
            "        1.4822e-04, 2.4393e-04, 2.2700e-04, 2.0629e-04, 9.5259e-05, 4.0112e-04,\n",
            "        3.6036e-04, 2.4690e-04, 5.9976e-04, 2.5898e-04, 8.0746e-05, 2.9142e-04,\n",
            "        1.6953e-04, 1.1081e-04, 2.9594e-04, 5.0137e-05, 5.7900e-05, 1.9377e-04,\n",
            "        2.0372e-04, 1.3422e-04, 1.8068e-04, 6.1290e-04, 5.5087e-05, 6.4174e-05,\n",
            "        2.0628e-04, 1.3569e-04, 1.9622e-04, 6.9757e-04, 1.7646e-04, 1.0384e-04,\n",
            "        2.0548e-04, 9.3956e-04, 2.6915e-04, 5.0061e-05, 6.2736e-05, 1.6808e-04,\n",
            "        3.1370e-05, 3.3614e-04, 2.3668e-04, 7.5121e-05, 5.9689e-04, 8.8463e-05,\n",
            "        1.0686e-04, 2.6912e-04, 1.1730e-04, 4.8197e-04, 5.7491e-04, 1.1277e-04,\n",
            "        7.6594e-05, 1.4409e-04, 8.5884e-05, 2.6341e-04, 2.4423e-04, 1.9017e-04,\n",
            "        3.6389e-04, 2.2175e-04, 4.0535e-04, 2.0581e-04, 1.0831e-04, 6.1627e-05,\n",
            "        2.0470e-04, 1.6359e-04, 1.7884e-04, 9.2039e-05, 2.1828e-04, 2.3183e-04,\n",
            "        4.5115e-05, 7.1135e-05, 2.3108e-04, 6.8931e-05, 8.1398e-05, 3.1867e-04,\n",
            "        1.6526e-04, 8.5101e-05, 2.9215e-04, 2.3949e-04, 1.8458e-04, 3.2696e-04,\n",
            "        8.6955e-05, 2.0038e-04, 1.4134e-04, 1.1537e-04, 1.2483e-04, 4.6365e-04,\n",
            "        1.5891e-04, 2.2247e-04, 7.6087e-05, 1.1836e-04, 4.4497e-04, 2.3407e-04,\n",
            "        8.5565e-05, 7.7116e-05, 4.2086e-05, 9.6800e-05, 8.9999e-05, 1.4655e-04,\n",
            "        2.7250e-05, 3.7632e-04, 1.6128e-04, 1.1664e-04, 2.8842e-05, 6.4426e-05,\n",
            "        8.1193e-05, 4.9480e-04, 9.0504e-05, 4.9157e-05, 1.5236e-04, 2.4138e-04,\n",
            "        2.6428e-04, 3.8719e-05, 7.8737e-05, 1.1449e-04, 1.6961e-04, 1.5952e-04,\n",
            "        5.8912e-05, 7.5815e-05, 2.9377e-04, 1.9274e-04, 1.9243e-04, 1.2048e-04,\n",
            "        1.1815e-04, 2.5195e-04, 2.0360e-04, 1.9977e-04, 3.2083e-04, 5.0973e-05,\n",
            "        1.8782e-04, 2.0180e-04, 6.3213e-05, 2.5464e-04, 1.3600e-04, 1.9660e-04,\n",
            "        1.1079e-04, 2.4194e-04, 1.3604e-04, 9.8314e-05, 4.5286e-05, 1.3117e-04,\n",
            "        5.5873e-05, 2.4442e-04, 9.3132e-05, 2.4829e-04, 1.5350e-04, 9.8400e-05,\n",
            "        1.8937e-04, 1.1412e-04, 3.4937e-05, 1.1123e-04, 2.0185e-04, 1.0088e-04,\n",
            "        1.6662e-04, 3.1694e-04, 6.0954e-04, 1.1116e-04, 3.5495e-04, 4.8409e-05,\n",
            "        2.6637e-04, 1.8566e-04, 7.4693e-05, 1.2376e-04, 2.1543e-04, 6.6677e-04,\n",
            "        1.8695e-04, 1.5295e-04, 2.4705e-04, 1.9336e-04, 1.7198e-04, 1.9963e-05,\n",
            "        5.5765e-04, 1.5410e-04, 2.2470e-04, 1.7607e-04, 1.1246e-04, 4.1179e-04,\n",
            "        1.6193e-04, 9.5786e-05, 1.7254e-04, 1.6436e-04, 2.9071e-04, 3.7577e-04,\n",
            "        4.2619e-04, 2.4962e-04, 2.2468e-04, 9.3067e-05, 8.5605e-05, 1.5227e-04,\n",
            "        3.7054e-05, 1.7004e-04, 3.2364e-04, 1.6147e-04, 5.5597e-05, 2.0054e-04,\n",
            "        1.6685e-04, 1.3627e-04, 1.5436e-04, 1.8143e-04, 4.6512e-05, 1.6834e-04,\n",
            "        5.9532e-05, 2.7155e-04, 3.7837e-04, 9.0656e-05, 3.2184e-04, 2.0840e-04,\n",
            "        8.9171e-05, 2.2771e-04, 7.8388e-05, 1.3768e-04, 2.1591e-04, 2.6884e-04,\n",
            "        2.2728e-04, 1.3115e-04, 9.1192e-05, 1.2073e-04, 1.2031e-04, 1.6543e-04,\n",
            "        1.8905e-04, 3.1875e-04, 3.0756e-04, 1.5385e-04, 2.5511e-04, 1.4082e-04,\n",
            "        2.3253e-04, 3.2474e-04, 2.2631e-04, 1.9140e-04, 3.9639e-04, 4.0815e-04,\n",
            "        2.3756e-04, 1.2847e-04, 1.1978e-04, 2.1645e-04, 2.4929e-04, 2.7369e-04,\n",
            "        2.1341e-04, 1.3997e-04, 8.9423e-05, 2.3106e-04, 1.0549e-04, 4.5500e-04,\n",
            "        8.0366e-05, 2.7378e-04, 4.4358e-04, 1.9436e-04, 1.8529e-04, 1.1130e-04,\n",
            "        8.3723e-05, 7.6897e-05, 3.8288e-05, 1.5857e-04, 9.3567e-05, 1.1295e-04,\n",
            "        4.8262e-04, 2.2018e-04, 2.6218e-04, 1.0313e-04, 4.6483e-05, 1.3047e-04,\n",
            "        3.4144e-04, 1.3632e-04, 1.1484e-04, 9.3089e-05, 2.4987e-04, 1.9529e-04,\n",
            "        3.8760e-04, 8.9295e-05, 1.0172e-04, 1.9016e-04, 2.0089e-04, 1.7015e-04,\n",
            "        2.3493e-04, 2.6158e-04, 9.7464e-05, 1.3099e-04, 1.9304e-04, 1.0963e-04,\n",
            "        1.9552e-04, 1.1466e-04, 1.1475e-04, 1.0774e-04, 2.7193e-04, 8.3439e-05,\n",
            "        8.7474e-05, 6.8191e-05, 2.3070e-04, 1.5371e-04, 1.6663e-04, 2.7107e-04,\n",
            "        7.3988e-05, 1.9964e-04, 1.3347e-04, 2.8385e-04, 2.7933e-04, 1.6643e-04,\n",
            "        2.2640e-04, 7.7420e-05, 1.7395e-04, 5.0240e-05, 1.7475e-04, 4.9156e-05,\n",
            "        5.1815e-04, 1.5579e-04, 7.4755e-05, 2.0312e-04, 3.0424e-04, 2.0248e-04,\n",
            "        6.9841e-05, 1.8223e-04, 1.9285e-04, 6.7988e-05, 1.2971e-04, 2.8359e-04,\n",
            "        1.9351e-04, 5.1556e-04, 5.8344e-04, 3.7763e-04, 3.0674e-04, 7.4560e-05,\n",
            "        1.0702e-04, 2.0122e-04, 5.4241e-04, 1.4308e-04, 1.9707e-04, 1.4152e-04,\n",
            "        2.9319e-04, 2.6930e-04, 1.8075e-04, 2.7311e-04, 1.6749e-04, 1.6012e-04,\n",
            "        2.1614e-04, 3.1710e-05, 1.7708e-04, 2.1929e-04, 3.7602e-04, 2.2709e-04,\n",
            "        1.0341e-04, 1.8272e-04, 8.3420e-05, 1.5869e-04, 1.2666e-04, 5.6216e-05,\n",
            "        4.9555e-05, 1.0529e-04, 6.6780e-05, 8.3419e-05, 3.0587e-04, 6.9319e-05,\n",
            "        6.6831e-05, 2.3558e-04, 9.4434e-05, 5.5820e-05, 2.1382e-04, 4.3242e-04,\n",
            "        1.1015e-04, 1.1861e-04, 7.3762e-05, 1.9208e-04, 1.9920e-04, 1.5112e-04,\n",
            "        2.7994e-04, 1.3687e-04, 2.6181e-04, 2.6948e-04, 9.6195e-05, 3.0467e-04,\n",
            "        2.8708e-04, 2.3637e-04, 3.3349e-04, 1.7377e-04, 3.1944e-04, 3.3603e-04,\n",
            "        4.2347e-04, 1.3325e-04, 1.9841e-04, 1.8105e-04, 3.9756e-04, 4.4697e-04,\n",
            "        2.5278e-04, 1.5862e-04, 7.0621e-05, 2.2372e-04, 1.6764e-04, 1.7417e-04,\n",
            "        2.9201e-04, 3.2928e-04, 3.4141e-04, 5.4936e-05, 2.2817e-04, 1.9051e-04,\n",
            "        4.0528e-04, 2.0778e-04, 9.4563e-05, 8.4583e-05, 5.5849e-05, 2.0223e-04,\n",
            "        1.1621e-04, 1.0320e-04, 2.8875e-04, 2.5629e-05], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bSAOL34FMYG"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "The GhostNet architecture is based on an Ghost module structure which generate more features from cheap operations. Based on a set of intrinsic feature maps, a series of cheap operations are applied to generate many ghost feature maps that could fully reveal information underlying intrinsic features. Experiments conducted on benchmarks demonstrate that the superiority of GhostNet in terms of speed and accuracy tradeoff.\n",
        "\n",
        "The corresponding accuracy on ImageNet dataset with pretrained model is listed below.\n",
        "\n",
        "| Model structure | FLOPs       | Top-1 acc   | Top-5 acc   |\n",
        "| --------------- | ----------- | ----------- | ----------- |\n",
        "|  GhostNet 1.0x  | 142M        | 73.98       | 91.46       |\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "You can read the full paper at this [link](https://arxiv.org/abs/1911.11907)."
      ]
    }
  ]
}