

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-12-29 19:30:59
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 19:31:18
 * @Description:
 * @TODO::
 * @Reference:http://www.tensorinfinity.com/paper_118.html
-->

目前CNN网络尽管取得了很成功的发展，模型准确率在很多方面都远远高于传统方法，但是其参数个数，和计算复杂度始终是制约其快速发展的一个因素，例如8层的AlexNet就有60M参数，在分类一张图需要729M的浮点操作个数，尽管训练过程可以离线进行，GPU也可以有很好的加速效果，但是对于一般的个人电脑和移动设备，仅仅是测试中的forward过程就已经十分吃力，甚至是无法负担这样巨大的计算需求，再加上移动设备上存储条件的限制，当前很多效果很好的深度学习模型无法在移动端实现。因此网络模型的加速和压缩至关重要。

对于大多的CNN网络，卷积层是最耗时的部分，全连接层则是最占存储空间的部分，由于这两者在CNN网络的复杂度和空间占用上存在本质上的不同，因此大多的工作是针对其一进行的，比如【7，13，32，31，18，17】低秩估计和矩阵分解方法用于加速卷积层，另一方面【3，7，11，30，2，12，28】则针对全连接层研究参数压缩的方法。总体看上述工作都仅仅完成了加速网络，或压缩参数中的一个工作，却没有同时做加速和压缩。

本文提出的方法Quantized CNN（Q-CNN）可以在很小的准确率损失下同时加速和压缩CNN模型。该方法的核心是对参数进行量化（用少部分的参数来表示全体参数），并通过内积运算来估计卷积层和全连接层的输出。为保证准确率，该方法在参数量化过程中最小化每层输出的估计误差。同时为了减少多层量化导致的误差累积，本文采用的策略是每层的优化目标中考虑之前的估计误差。

第二节将先叙述一些基础知识，第三节将详细叙述本文提出的方法，第四节是相关实验部分，作者在MNIST，ILSVRC-12上分别进行了实验，针对AlexNet，CaffeNet，CNN-S，VGG16网络结构进行量化，总体实验结果表明，对每一种网络，Quantized CNN都可以达到4倍的加速，和15倍的压缩，并保证Top-5准确率下降不到1%。最后作者在实际的手机上也进行了实验，证明了该方法的实际效果。
