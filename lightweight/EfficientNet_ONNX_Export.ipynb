{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EfficientNet ONNX Export",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "17c0343545124845881aac0a2827cb67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc0c17a6cf8f4ee498a6c520c89ddf42",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7f35c329964a4d7db6471cc791a068cd",
              "IPY_MODEL_3e38fc2394b545039c494f6b62345b9a"
            ]
          }
        },
        "fc0c17a6cf8f4ee498a6c520c89ddf42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f35c329964a4d7db6471cc791a068cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_1753395a2e3548f3b67a8d1736951806",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 31519111,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 31519111,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_29999962487646fbbcc15439339ebdc0"
          }
        },
        "3e38fc2394b545039c494f6b62345b9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f52614150dbe48a895bf0ded0e024f96",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30.1M/30.1M [00:04&lt;00:00, 6.85MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_48cc3d9ba50e405d9bb4abe040291da7"
          }
        },
        "1753395a2e3548f3b67a8d1736951806": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "29999962487646fbbcc15439339ebdc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f52614150dbe48a895bf0ded0e024f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "48cc3d9ba50e405d9bb4abe040291da7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQpzOrtMYsuE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1860f71-b6ee-4c48-c34c-df1fb481e885"
      },
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (1.19.4)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch) (0.16.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16032 sha256=170494d6d0af1ab32729aa5351b82b3b614da571e31557bb5a86563098ad5ec3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpaiBnC4Yxh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7314c881-6a8c-432f-b93b-2c0a97a9af5c"
      },
      "source": [
        "# Install ONNX in Colab\n",
        "!sudo apt-get install protobuf-compiler libprotoc-dev\n",
        "!pip install onnx"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "protobuf-compiler is already the newest version (3.0.0-9.1ubuntu1).\n",
            "The following additional packages will be installed:\n",
            "  libprotobuf-dev libprotobuf-lite10\n",
            "The following NEW packages will be installed:\n",
            "  libprotobuf-dev libprotobuf-lite10 libprotoc-dev\n",
            "0 upgraded, 3 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 1,738 kB of archives.\n",
            "After this operation, 13.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-lite10 amd64 3.0.0-9.1ubuntu1 [97.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotobuf-dev amd64 3.0.0-9.1ubuntu1 [959 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/main amd64 libprotoc-dev amd64 3.0.0-9.1ubuntu1 [682 kB]\n",
            "Fetched 1,738 kB in 3s (654 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libprotobuf-lite10:amd64.\n",
            "(Reading database ... 145480 files and directories currently installed.)\n",
            "Preparing to unpack .../libprotobuf-lite10_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libprotobuf-dev:amd64.\n",
            "Preparing to unpack .../libprotobuf-dev_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Selecting previously unselected package libprotoc-dev:amd64.\n",
            "Preparing to unpack .../libprotoc-dev_3.0.0-9.1ubuntu1_amd64.deb ...\n",
            "Unpacking libprotoc-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libprotobuf-lite10:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libprotobuf-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Setting up libprotoc-dev:amd64 (3.0.0-9.1ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting onnx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2a/82/e8d0fb64df623a3b716145192ed50604f444889778b37e0e9262753d5046/onnx-1.8.0-cp36-cp36m-manylinux2010_x86_64.whl (7.7MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.7MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from onnx) (1.19.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx) (51.0.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKeGU_GgYxrp"
      },
      "source": [
        "import torch\n",
        "from efficientnet_pytorch import EfficientNet"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq4PfcmFYyeH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "17c0343545124845881aac0a2827cb67",
            "fc0c17a6cf8f4ee498a6c520c89ddf42",
            "7f35c329964a4d7db6471cc791a068cd",
            "3e38fc2394b545039c494f6b62345b9a",
            "1753395a2e3548f3b67a8d1736951806",
            "29999962487646fbbcc15439339ebdc0",
            "f52614150dbe48a895bf0ded0e024f96",
            "48cc3d9ba50e405d9bb4abe040291da7"
          ]
        },
        "outputId": "8cc9e814-17dd-4aa8-81e4-de0a5260b06e"
      },
      "source": [
        "# Specify which model to use\n",
        "model_name = 'efficientnet-b1'\n",
        "image_size = EfficientNet.get_image_size(model_name)\n",
        "print('Image size: ', image_size)\n",
        "\n",
        "# Load model\n",
        "model = EfficientNet.from_pretrained(model_name)\n",
        "model.eval()\n",
        "print('Model image size: ', model._global_params.image_size)\n",
        "\n",
        "# Dummy input for ONNX\n",
        "dummy_input = torch.randn(10, 3, 224, 224)\n",
        "\n",
        "# Export with ONNX\n",
        "torch.onnx.export(model, dummy_input, \"efficientnet-b1.onnx\", verbose=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image size:  240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b1-f1951068.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17c0343545124845881aac0a2827cb67",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=31519111.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loaded pretrained weights for efficientnet-b1\n",
            "Model image size:  240\n",
            "graph(%input.1 : Float(10:150528, 3:50176, 224:224, 224:1, requires_grad=0, device=cpu),\n",
            "      %_blocks.0._se_reduce.weight : Float(8:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.0._se_reduce.bias : Float(8:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.0._se_expand.weight : Float(32:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.0._se_expand.bias : Float(32:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.1._se_reduce.weight : Float(4:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.1._se_reduce.bias : Float(4:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.1._se_expand.weight : Float(16:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.1._se_expand.bias : Float(16:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.2._se_reduce.weight : Float(4:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.2._se_reduce.bias : Float(4:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.2._se_expand.weight : Float(96:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.2._se_expand.bias : Float(96:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.3._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.3._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.3._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.3._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.4._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.4._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.4._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.4._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.5._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.5._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.5._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.5._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.6._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.6._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.6._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.6._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.7._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.7._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.7._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.7._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.8._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.8._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.8._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.8._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.9._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.9._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.9._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.9._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.10._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.10._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.10._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.10._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.11._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.11._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.11._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.11._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.12._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.12._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.12._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.12._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.13._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.13._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.13._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.13._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.14._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.14._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.14._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.14._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.15._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.15._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.15._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.15._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.16._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.16._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.16._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.16._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.17._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.17._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.17._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.17._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.18._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.18._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.18._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.18._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.19._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.19._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.19._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.19._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.20._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.20._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.20._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.20._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.21._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.21._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.21._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.21._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.22._se_reduce.weight : Float(80:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.22._se_reduce.bias : Float(80:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.22._se_expand.weight : Float(1920:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu),\n",
            "      %_blocks.22._se_expand.bias : Float(1920:1, requires_grad=1, device=cpu),\n",
            "      %_fc.weight : Float(1000:1280, 1280:1, requires_grad=1, device=cpu),\n",
            "      %_fc.bias : Float(1000:1, requires_grad=1, device=cpu),\n",
            "      %875 : Float(32:27, 3:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %876 : Float(32:1, requires_grad=0, device=cpu),\n",
            "      %878 : Float(32:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %879 : Float(32:1, requires_grad=0, device=cpu),\n",
            "      %881 : Float(16:32, 32:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %882 : Float(16:1, requires_grad=0, device=cpu),\n",
            "      %884 : Float(16:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %885 : Float(16:1, requires_grad=0, device=cpu),\n",
            "      %887 : Float(16:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %888 : Float(16:1, requires_grad=0, device=cpu),\n",
            "      %890 : Float(96:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %891 : Float(96:1, requires_grad=0, device=cpu),\n",
            "      %893 : Float(96:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %894 : Float(96:1, requires_grad=0, device=cpu),\n",
            "      %896 : Float(24:96, 96:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %897 : Float(24:1, requires_grad=0, device=cpu),\n",
            "      %899 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %900 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %902 : Float(144:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %903 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %905 : Float(24:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %906 : Float(24:1, requires_grad=0, device=cpu),\n",
            "      %908 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %909 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %911 : Float(144:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %912 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %914 : Float(24:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %915 : Float(24:1, requires_grad=0, device=cpu),\n",
            "      %917 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %918 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %920 : Float(144:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %921 : Float(144:1, requires_grad=0, device=cpu),\n",
            "      %923 : Float(40:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %924 : Float(40:1, requires_grad=0, device=cpu),\n",
            "      %926 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %927 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %929 : Float(240:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %930 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %932 : Float(40:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %933 : Float(40:1, requires_grad=0, device=cpu),\n",
            "      %935 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %936 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %938 : Float(240:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %939 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %941 : Float(40:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %942 : Float(40:1, requires_grad=0, device=cpu),\n",
            "      %944 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %945 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %947 : Float(240:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %948 : Float(240:1, requires_grad=0, device=cpu),\n",
            "      %950 : Float(80:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %951 : Float(80:1, requires_grad=0, device=cpu),\n",
            "      %953 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %954 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %956 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %957 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %959 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %960 : Float(80:1, requires_grad=0, device=cpu),\n",
            "      %962 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %963 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %965 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %966 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %968 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %969 : Float(80:1, requires_grad=0, device=cpu),\n",
            "      %971 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %972 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %974 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %975 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %977 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %978 : Float(80:1, requires_grad=0, device=cpu),\n",
            "      %980 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %981 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %983 : Float(480:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %984 : Float(480:1, requires_grad=0, device=cpu),\n",
            "      %986 : Float(112:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %987 : Float(112:1, requires_grad=0, device=cpu),\n",
            "      %989 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %990 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %992 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %993 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %995 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %996 : Float(112:1, requires_grad=0, device=cpu),\n",
            "      %998 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %999 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1001 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1002 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1004 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1005 : Float(112:1, requires_grad=0, device=cpu),\n",
            "      %1007 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1008 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1010 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1011 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1013 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1014 : Float(112:1, requires_grad=0, device=cpu),\n",
            "      %1016 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1017 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1019 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1020 : Float(672:1, requires_grad=0, device=cpu),\n",
            "      %1022 : Float(192:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1023 : Float(192:1, requires_grad=0, device=cpu),\n",
            "      %1025 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1026 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1028 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1029 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1031 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1032 : Float(192:1, requires_grad=0, device=cpu),\n",
            "      %1034 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1035 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1037 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1038 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1040 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1041 : Float(192:1, requires_grad=0, device=cpu),\n",
            "      %1043 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1044 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1046 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1047 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1049 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1050 : Float(192:1, requires_grad=0, device=cpu),\n",
            "      %1052 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1053 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1055 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n",
            "      %1056 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1058 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1059 : Float(192:1, requires_grad=0, device=cpu),\n",
            "      %1061 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1062 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1064 : Float(1152:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %1065 : Float(1152:1, requires_grad=0, device=cpu),\n",
            "      %1067 : Float(320:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1068 : Float(320:1, requires_grad=0, device=cpu),\n",
            "      %1070 : Float(1920:320, 320:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1071 : Float(1920:1, requires_grad=0, device=cpu),\n",
            "      %1073 : Float(1920:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n",
            "      %1074 : Float(1920:1, requires_grad=0, device=cpu),\n",
            "      %1076 : Float(320:1920, 1920:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1077 : Float(320:1, requires_grad=0, device=cpu),\n",
            "      %1079 : Float(1280:320, 320:1, 1:1, 1:1, requires_grad=0, device=cpu),\n",
            "      %1080 : Float(1280:1, requires_grad=0, device=cpu)):\n",
            "  %509 : Float(10:153228, 3:51076, 226:226, 226:1, requires_grad=0, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.1) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %874 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%509, %875, %876)\n",
            "  %input.3 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%874) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %513 : Float(10:415872, 32:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.3) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %877 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%513, %878, %879)\n",
            "  %x.3 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%877) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %517 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.3) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %518 : Float(10:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%517, %_blocks.0._se_reduce.weight, %_blocks.0._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %519 : Float(10:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%518) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %520 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%519, %_blocks.0._se_expand.weight, %_blocks.0._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %521 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%520) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %522 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Mul(%521, %x.3) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %880 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%522, %881, %882)\n",
            "  %525 : Float(10:207936, 16:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%880) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %883 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=16, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%525, %884, %885)\n",
            "  %x.6 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%883) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %529 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.6) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %530 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%529, %_blocks.1._se_reduce.weight, %_blocks.1._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %531 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%530) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %532 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%531, %_blocks.1._se_expand.weight, %_blocks.1._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %533 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%532) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %534 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Mul(%533, %x.6) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %886 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%534, %887, %888)\n",
            "  %537 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Add(%886, %880) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %889 : Float(10:1204224, 96:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %890, %891)\n",
            "  %input.10 : Float(10:1204224, 96:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%889) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %541 : Float(10:1247616, 96:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.10) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %892 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%541, %893, %894)\n",
            "  %x.10 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%892) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %545 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.10) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %546 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%545, %_blocks.2._se_reduce.weight, %_blocks.2._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %547 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%546) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %548 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%547, %_blocks.2._se_expand.weight, %_blocks.2._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %549 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%548) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %550 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%549, %x.10) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %895 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%550, %896, %897)\n",
            "  %898 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%895, %899, %900)\n",
            "  %input.14 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%898) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %556 : Float(10:484416, 144:3364, 58:58, 58:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.14) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %901 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%556, %902, %903)\n",
            "  %x.14 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%901) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %560 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.14) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %561 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%560, %_blocks.3._se_reduce.weight, %_blocks.3._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %562 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%561) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %563 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%562, %_blocks.3._se_expand.weight, %_blocks.3._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %564 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%563) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %565 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%564, %x.14) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %904 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%565, %905, %906)\n",
            "  %568 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%904, %895) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %907 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%568, %908, %909)\n",
            "  %input.18 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%907) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %572 : Float(10:484416, 144:3364, 58:58, 58:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.18) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %910 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%572, %911, %912)\n",
            "  %x.18 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%910) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %576 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.18) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %577 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%576, %_blocks.4._se_reduce.weight, %_blocks.4._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %578 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%577) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %579 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%578, %_blocks.4._se_expand.weight, %_blocks.4._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %580 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%579) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %581 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%580, %x.18) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %913 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%581, %914, %915)\n",
            "  %584 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%913, %568) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %916 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%584, %917, %918)\n",
            "  %input.22 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%916) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %588 : Float(10:518400, 144:3600, 60:60, 60:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.22) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %919 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%588, %920, %921)\n",
            "  %x.22 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%919) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %592 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.22) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %593 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%592, %_blocks.5._se_reduce.weight, %_blocks.5._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %594 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%593) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %595 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%594, %_blocks.5._se_expand.weight, %_blocks.5._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %596 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%595) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %597 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%596, %x.22) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %922 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%597, %923, %924)\n",
            "  %925 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%922, %926, %927)\n",
            "  %input.26 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%925) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %603 : Float(10:245760, 240:1024, 32:32, 32:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.26) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %928 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%603, %929, %930)\n",
            "  %x.26 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%928) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %607 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.26) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %608 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%607, %_blocks.6._se_reduce.weight, %_blocks.6._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %609 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%608) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %610 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%609, %_blocks.6._se_expand.weight, %_blocks.6._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %611 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%610) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %612 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%611, %x.26) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %931 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%612, %932, %933)\n",
            "  %615 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%931, %922) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %934 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%615, %935, %936)\n",
            "  %input.30 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%934) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %619 : Float(10:245760, 240:1024, 32:32, 32:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.30) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %937 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%619, %938, %939)\n",
            "  %x.30 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%937) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %623 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.30) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %624 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%623, %_blocks.7._se_reduce.weight, %_blocks.7._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %625 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%624) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %626 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%625, %_blocks.7._se_expand.weight, %_blocks.7._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %627 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%626) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %628 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%627, %x.30) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %940 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%628, %941, %942)\n",
            "  %631 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%940, %615) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %943 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%631, %944, %945)\n",
            "  %input.34 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%943) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %635 : Float(10:216000, 240:900, 30:30, 30:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.34) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %946 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%635, %947, %948)\n",
            "  %x.34 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%946) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %639 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.34) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %640 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%639, %_blocks.8._se_reduce.weight, %_blocks.8._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %641 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%640) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %642 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%641, %_blocks.8._se_expand.weight, %_blocks.8._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %643 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%642) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %644 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%643, %x.34) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %949 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%644, %950, %951)\n",
            "  %952 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%949, %953, %954)\n",
            "  %input.38 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%952) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %650 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.38) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %955 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%650, %956, %957)\n",
            "  %x.38 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%955) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %654 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.38) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %655 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%654, %_blocks.9._se_reduce.weight, %_blocks.9._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %656 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%655) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %657 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%656, %_blocks.9._se_expand.weight, %_blocks.9._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %658 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%657) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %659 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%658, %x.38) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %958 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%659, %959, %960)\n",
            "  %662 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%958, %949) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %961 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%662, %962, %963)\n",
            "  %input.42 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%961) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %666 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.42) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %964 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%666, %965, %966)\n",
            "  %x.42 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%964) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %670 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.42) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %671 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%670, %_blocks.10._se_reduce.weight, %_blocks.10._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %672 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%671) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %673 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%672, %_blocks.10._se_expand.weight, %_blocks.10._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %674 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%673) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %675 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%674, %x.42) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %967 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%675, %968, %969)\n",
            "  %678 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%967, %662) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %970 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%678, %971, %972)\n",
            "  %input.46 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%970) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %682 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.46) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %973 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%682, %974, %975)\n",
            "  %x.46 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%973) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %686 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.46) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %687 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%686, %_blocks.11._se_reduce.weight, %_blocks.11._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %688 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%687) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %689 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%688, %_blocks.11._se_expand.weight, %_blocks.11._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %690 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%689) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %691 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%690, %x.46) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %976 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%691, %977, %978)\n",
            "  %694 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%976, %678) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %979 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%694, %980, %981)\n",
            "  %input.50 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%979) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %698 : Float(10:155520, 480:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.50) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %982 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%698, %983, %984)\n",
            "  %x.50 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%982) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %702 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.50) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %703 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%702, %_blocks.12._se_reduce.weight, %_blocks.12._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %704 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%703) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %705 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%704, %_blocks.12._se_expand.weight, %_blocks.12._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %706 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%705) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %707 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%706, %x.50) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %985 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%707, %986, %987)\n",
            "  %988 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%985, %989, %990)\n",
            "  %input.54 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%988) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %713 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.54) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %991 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%713, %992, %993)\n",
            "  %x.54 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%991) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %717 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.54) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %718 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%717, %_blocks.13._se_reduce.weight, %_blocks.13._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %719 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%718) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %720 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%719, %_blocks.13._se_expand.weight, %_blocks.13._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %721 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%720) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %722 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%721, %x.54) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %994 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%722, %995, %996)\n",
            "  %725 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%994, %985) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %997 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%725, %998, %999)\n",
            "  %input.58 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%997) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %729 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.58) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1000 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%729, %1001, %1002)\n",
            "  %x.58 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1000) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %733 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.58) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %734 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%733, %_blocks.14._se_reduce.weight, %_blocks.14._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %735 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%734) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %736 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%735, %_blocks.14._se_expand.weight, %_blocks.14._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %737 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%736) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %738 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%737, %x.58) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1003 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%738, %1004, %1005)\n",
            "  %741 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%1003, %725) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1006 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%741, %1007, %1008)\n",
            "  %input.62 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1006) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %745 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.62) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1009 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%745, %1010, %1011)\n",
            "  %x.62 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1009) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %749 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.62) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %750 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%749, %_blocks.15._se_reduce.weight, %_blocks.15._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %751 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%750) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %752 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%751, %_blocks.15._se_expand.weight, %_blocks.15._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %753 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%752) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %754 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%753, %x.62) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1012 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%754, %1013, %1014)\n",
            "  %757 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%1012, %741) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1015 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%757, %1016, %1017)\n",
            "  %input.66 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1015) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %761 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.66) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1018 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%761, %1019, %1020)\n",
            "  %x.66 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1018) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %765 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.66) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %766 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%765, %_blocks.16._se_reduce.weight, %_blocks.16._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %767 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%766) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %768 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%767, %_blocks.16._se_expand.weight, %_blocks.16._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %769 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%768) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %770 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%769, %x.66) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1021 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%770, %1022, %1023)\n",
            "  %1024 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%1021, %1025, %1026)\n",
            "  %input.70 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1024) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %776 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.70) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1027 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%776, %1028, %1029)\n",
            "  %x.70 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1027) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %780 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.70) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %781 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%780, %_blocks.17._se_reduce.weight, %_blocks.17._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %782 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%781) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %783 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%782, %_blocks.17._se_expand.weight, %_blocks.17._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %784 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%783) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %785 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%784, %x.70) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1030 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%785, %1031, %1032)\n",
            "  %788 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1030, %1021) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1033 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%788, %1034, %1035)\n",
            "  %input.74 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1033) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %792 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.74) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1036 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%792, %1037, %1038)\n",
            "  %x.74 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1036) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %796 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.74) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %797 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%796, %_blocks.18._se_reduce.weight, %_blocks.18._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %798 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%797) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %799 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%798, %_blocks.18._se_expand.weight, %_blocks.18._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %800 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%799) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %801 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%800, %x.74) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1039 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%801, %1040, %1041)\n",
            "  %804 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1039, %788) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1042 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%804, %1043, %1044)\n",
            "  %input.78 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1042) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %808 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.78) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1045 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%808, %1046, %1047)\n",
            "  %x.78 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1045) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %812 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.78) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %813 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%812, %_blocks.19._se_reduce.weight, %_blocks.19._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %814 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%813) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %815 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%814, %_blocks.19._se_expand.weight, %_blocks.19._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %816 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%815) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %817 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%816, %x.78) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1048 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%817, %1049, %1050)\n",
            "  %820 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1048, %804) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1051 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%820, %1052, %1053)\n",
            "  %input.82 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1051) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %824 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.82) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1054 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%824, %1055, %1056)\n",
            "  %x.82 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1054) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %828 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.82) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %829 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%828, %_blocks.20._se_reduce.weight, %_blocks.20._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %830 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%829) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %831 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%830, %_blocks.20._se_expand.weight, %_blocks.20._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %832 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%831) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %833 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%832, %x.82) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1057 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%833, %1058, %1059)\n",
            "  %836 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1057, %820) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1060 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%836, %1061, %1062)\n",
            "  %input.86 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1060) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %840 : Float(10:93312, 1152:81, 9:9, 9:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.86) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1063 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%840, %1064, %1065)\n",
            "  %x.86 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1063) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %844 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.86) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %845 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%844, %_blocks.21._se_reduce.weight, %_blocks.21._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %846 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%845) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %847 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%846, %_blocks.21._se_expand.weight, %_blocks.21._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %848 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%847) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %849 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%848, %x.86) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1066 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%849, %1067, %1068)\n",
            "  %1069 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%1066, %1070, %1071)\n",
            "  %input.90 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1069) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %855 : Float(10:155520, 1920:81, 9:9, 9:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.90) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n",
            "  %1072 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1920, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%855, %1073, %1074)\n",
            "  %x.90 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1072) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %859 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.90) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %860 : Float(10:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%859, %_blocks.22._se_reduce.weight, %_blocks.22._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %861 : Float(10:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%860) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %862 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%861, %_blocks.22._se_expand.weight, %_blocks.22._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n",
            "  %863 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%862) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %864 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%863, %x.90) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n",
            "  %1075 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%864, %1076, %1077)\n",
            "  %867 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1075, %1066) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n",
            "  %1078 : Float(10:62720, 1280:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%867, %1079, %1080)\n",
            "  %input.94 : Float(10:62720, 1280:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1078) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n",
            "  %871 : Float(10:1280, 1280:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%input.94) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n",
            "  %872 : Float(10:1280, 1280:1, requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%871) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:983:0\n",
            "  %873 : Float(10:1000, 1000:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%872, %_fc.weight, %_fc.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1690:0\n",
            "  return (%873)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-64a7fa550fc1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Export with ONNX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"efficientnet-b1.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m    228\u001b[0m                         \u001b[0mdo_constant_folding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                         custom_opsets, enable_onnx_checker, use_external_data_format)\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, aten, export_raw_ir, operator_export_type, opset_version, _retain_param_name, do_constant_folding, example_outputs, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, custom_opsets, enable_onnx_checker, use_external_data_format)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_initializers_as_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_onnx_checker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             use_external_data_format=use_external_data_format)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, example_outputs, opset_version, _retain_param_name, do_constant_folding, strip_doc_string, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, enable_onnx_checker, use_external_data_format, onnx_shape_inference, use_new_jit_passes)\u001b[0m\n\u001b[1;32m    648\u001b[0m                     \u001b[0mparams_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopset_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdynamic_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefer_weight_export\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m                     \u001b[0moperator_export_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_doc_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_keep_init_as_ip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_opsets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                     val_add_node_names, val_use_external_data_format, model_file_location)\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 proto, export_map = graph._export_onnx(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: ONNX export failed: Couldn't export Python operator SwishImplementation\n\nDefined at:\n/usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py(76): forward\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(709): _slow_forward\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(725): _call_impl\n/usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py(286): extract_features\n/usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py(311): forward\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(709): _slow_forward\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(725): _call_impl\n/usr/local/lib/python3.6/dist-packages/torch/jit/_trace.py(116): wrapper\n/usr/local/lib/python3.6/dist-packages/torch/jit/_trace.py(130): forward\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py(727): _call_impl\n/usr/local/lib/python3.6/dist-packages/torch/jit/_trace.py(1148): _get_trace_graph\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py(342): _trace_and_get_graph_from_model\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py(379): _create_jit_graph\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py(411): _model_to_graph\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py(639): _export\n/usr/local/lib/python3.6/dist-packages/torch/onnx/utils.py(91): export\n/usr/local/lib/python3.6/dist-packages/torch/onnx/__init__.py(230): export\n<ipython-input-4-64a7fa550fc1>(15): <module>\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2882): run_code\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2828): run_ast_nodes\n/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py(2718): run_cell\n/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py(537): run_cell\n/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py(208): do_execute\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py(399): execute_request\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py(233): dispatch_shell\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py(283): dispatcher\n/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py(300): null_wrapper\n/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py(444): _run_callback\n/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py(492): _handle_recv\n/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py(462): _handle_events\n/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py(548): <lambda>\n/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py(300): null_wrapper\n/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py(758): _run_callback\n/usr/lib/python3.6/asyncio/events.py(145): _run\n/usr/lib/python3.6/asyncio/base_events.py(1451): _run_once\n/usr/lib/python3.6/asyncio/base_events.py(438): run_forever\n/usr/local/lib/python3.6/dist-packages/tornado/platform/asyncio.py(132): start\n/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py(499): start\n/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py(664): launch_instance\n/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py(16): <module>\n/usr/lib/python3.6/runpy.py(85): _run_code\n/usr/lib/python3.6/runpy.py(193): _run_module_as_main\n\n\nGraph we tried to export:\ngraph(%input.1 : Float(10:150528, 3:50176, 224:224, 224:1, requires_grad=0, device=cpu),\n      %_blocks.0._se_reduce.weight : Float(8:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.0._se_reduce.bias : Float(8:1, requires_grad=1, device=cpu),\n      %_blocks.0._se_expand.weight : Float(32:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.0._se_expand.bias : Float(32:1, requires_grad=1, device=cpu),\n      %_blocks.1._se_reduce.weight : Float(4:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.1._se_reduce.bias : Float(4:1, requires_grad=1, device=cpu),\n      %_blocks.1._se_expand.weight : Float(16:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.1._se_expand.bias : Float(16:1, requires_grad=1, device=cpu),\n      %_blocks.2._se_reduce.weight : Float(4:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.2._se_reduce.bias : Float(4:1, requires_grad=1, device=cpu),\n      %_blocks.2._se_expand.weight : Float(96:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.2._se_expand.bias : Float(96:1, requires_grad=1, device=cpu),\n      %_blocks.3._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.3._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n      %_blocks.3._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.3._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n      %_blocks.4._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.4._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n      %_blocks.4._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.4._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n      %_blocks.5._se_reduce.weight : Float(6:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.5._se_reduce.bias : Float(6:1, requires_grad=1, device=cpu),\n      %_blocks.5._se_expand.weight : Float(144:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.5._se_expand.bias : Float(144:1, requires_grad=1, device=cpu),\n      %_blocks.6._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.6._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n      %_blocks.6._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.6._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n      %_blocks.7._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.7._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n      %_blocks.7._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.7._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n      %_blocks.8._se_reduce.weight : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.8._se_reduce.bias : Float(10:1, requires_grad=1, device=cpu),\n      %_blocks.8._se_expand.weight : Float(240:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.8._se_expand.bias : Float(240:1, requires_grad=1, device=cpu),\n      %_blocks.9._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.9._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n      %_blocks.9._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.9._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %_blocks.10._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.10._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n      %_blocks.10._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.10._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %_blocks.11._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.11._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n      %_blocks.11._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.11._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %_blocks.12._se_reduce.weight : Float(20:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.12._se_reduce.bias : Float(20:1, requires_grad=1, device=cpu),\n      %_blocks.12._se_expand.weight : Float(480:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.12._se_expand.bias : Float(480:1, requires_grad=1, device=cpu),\n      %_blocks.13._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.13._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n      %_blocks.13._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.13._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %_blocks.14._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.14._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n      %_blocks.14._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.14._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %_blocks.15._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.15._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n      %_blocks.15._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.15._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %_blocks.16._se_reduce.weight : Float(28:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.16._se_reduce.bias : Float(28:1, requires_grad=1, device=cpu),\n      %_blocks.16._se_expand.weight : Float(672:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.16._se_expand.bias : Float(672:1, requires_grad=1, device=cpu),\n      %_blocks.17._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.17._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n      %_blocks.17._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.17._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n      %_blocks.18._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.18._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n      %_blocks.18._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.18._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n      %_blocks.19._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.19._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n      %_blocks.19._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.19._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n      %_blocks.20._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.20._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n      %_blocks.20._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.20._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n      %_blocks.21._se_reduce.weight : Float(48:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.21._se_reduce.bias : Float(48:1, requires_grad=1, device=cpu),\n      %_blocks.21._se_expand.weight : Float(1152:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.21._se_expand.bias : Float(1152:1, requires_grad=1, device=cpu),\n      %_blocks.22._se_reduce.weight : Float(80:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.22._se_reduce.bias : Float(80:1, requires_grad=1, device=cpu),\n      %_blocks.22._se_expand.weight : Float(1920:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu),\n      %_blocks.22._se_expand.bias : Float(1920:1, requires_grad=1, device=cpu),\n      %_fc.weight : Float(1000:1280, 1280:1, requires_grad=1, device=cpu),\n      %_fc.bias : Float(1000:1, requires_grad=1, device=cpu),\n      %875 : Float(32:27, 3:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %876 : Float(32:1, requires_grad=0, device=cpu),\n      %878 : Float(32:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %879 : Float(32:1, requires_grad=0, device=cpu),\n      %881 : Float(16:32, 32:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %882 : Float(16:1, requires_grad=0, device=cpu),\n      %884 : Float(16:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %885 : Float(16:1, requires_grad=0, device=cpu),\n      %887 : Float(16:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %888 : Float(16:1, requires_grad=0, device=cpu),\n      %890 : Float(96:16, 16:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %891 : Float(96:1, requires_grad=0, device=cpu),\n      %893 : Float(96:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %894 : Float(96:1, requires_grad=0, device=cpu),\n      %896 : Float(24:96, 96:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %897 : Float(24:1, requires_grad=0, device=cpu),\n      %899 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %900 : Float(144:1, requires_grad=0, device=cpu),\n      %902 : Float(144:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %903 : Float(144:1, requires_grad=0, device=cpu),\n      %905 : Float(24:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %906 : Float(24:1, requires_grad=0, device=cpu),\n      %908 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %909 : Float(144:1, requires_grad=0, device=cpu),\n      %911 : Float(144:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %912 : Float(144:1, requires_grad=0, device=cpu),\n      %914 : Float(24:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %915 : Float(24:1, requires_grad=0, device=cpu),\n      %917 : Float(144:24, 24:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %918 : Float(144:1, requires_grad=0, device=cpu),\n      %920 : Float(144:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %921 : Float(144:1, requires_grad=0, device=cpu),\n      %923 : Float(40:144, 144:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %924 : Float(40:1, requires_grad=0, device=cpu),\n      %926 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %927 : Float(240:1, requires_grad=0, device=cpu),\n      %929 : Float(240:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %930 : Float(240:1, requires_grad=0, device=cpu),\n      %932 : Float(40:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %933 : Float(40:1, requires_grad=0, device=cpu),\n      %935 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %936 : Float(240:1, requires_grad=0, device=cpu),\n      %938 : Float(240:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %939 : Float(240:1, requires_grad=0, device=cpu),\n      %941 : Float(40:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %942 : Float(40:1, requires_grad=0, device=cpu),\n      %944 : Float(240:40, 40:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %945 : Float(240:1, requires_grad=0, device=cpu),\n      %947 : Float(240:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %948 : Float(240:1, requires_grad=0, device=cpu),\n      %950 : Float(80:240, 240:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %951 : Float(80:1, requires_grad=0, device=cpu),\n      %953 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %954 : Float(480:1, requires_grad=0, device=cpu),\n      %956 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %957 : Float(480:1, requires_grad=0, device=cpu),\n      %959 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %960 : Float(80:1, requires_grad=0, device=cpu),\n      %962 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %963 : Float(480:1, requires_grad=0, device=cpu),\n      %965 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %966 : Float(480:1, requires_grad=0, device=cpu),\n      %968 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %969 : Float(80:1, requires_grad=0, device=cpu),\n      %971 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %972 : Float(480:1, requires_grad=0, device=cpu),\n      %974 : Float(480:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %975 : Float(480:1, requires_grad=0, device=cpu),\n      %977 : Float(80:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %978 : Float(80:1, requires_grad=0, device=cpu),\n      %980 : Float(480:80, 80:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %981 : Float(480:1, requires_grad=0, device=cpu),\n      %983 : Float(480:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %984 : Float(480:1, requires_grad=0, device=cpu),\n      %986 : Float(112:480, 480:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %987 : Float(112:1, requires_grad=0, device=cpu),\n      %989 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %990 : Float(672:1, requires_grad=0, device=cpu),\n      %992 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %993 : Float(672:1, requires_grad=0, device=cpu),\n      %995 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %996 : Float(112:1, requires_grad=0, device=cpu),\n      %998 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %999 : Float(672:1, requires_grad=0, device=cpu),\n      %1001 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1002 : Float(672:1, requires_grad=0, device=cpu),\n      %1004 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1005 : Float(112:1, requires_grad=0, device=cpu),\n      %1007 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1008 : Float(672:1, requires_grad=0, device=cpu),\n      %1010 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1011 : Float(672:1, requires_grad=0, device=cpu),\n      %1013 : Float(112:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1014 : Float(112:1, requires_grad=0, device=cpu),\n      %1016 : Float(672:112, 112:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1017 : Float(672:1, requires_grad=0, device=cpu),\n      %1019 : Float(672:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1020 : Float(672:1, requires_grad=0, device=cpu),\n      %1022 : Float(192:672, 672:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1023 : Float(192:1, requires_grad=0, device=cpu),\n      %1025 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1026 : Float(1152:1, requires_grad=0, device=cpu),\n      %1028 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1029 : Float(1152:1, requires_grad=0, device=cpu),\n      %1031 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1032 : Float(192:1, requires_grad=0, device=cpu),\n      %1034 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1035 : Float(1152:1, requires_grad=0, device=cpu),\n      %1037 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1038 : Float(1152:1, requires_grad=0, device=cpu),\n      %1040 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1041 : Float(192:1, requires_grad=0, device=cpu),\n      %1043 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1044 : Float(1152:1, requires_grad=0, device=cpu),\n      %1046 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1047 : Float(1152:1, requires_grad=0, device=cpu),\n      %1049 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1050 : Float(192:1, requires_grad=0, device=cpu),\n      %1052 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1053 : Float(1152:1, requires_grad=0, device=cpu),\n      %1055 : Float(1152:25, 1:25, 5:5, 5:1, requires_grad=0, device=cpu),\n      %1056 : Float(1152:1, requires_grad=0, device=cpu),\n      %1058 : Float(192:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1059 : Float(192:1, requires_grad=0, device=cpu),\n      %1061 : Float(1152:192, 192:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1062 : Float(1152:1, requires_grad=0, device=cpu),\n      %1064 : Float(1152:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %1065 : Float(1152:1, requires_grad=0, device=cpu),\n      %1067 : Float(320:1152, 1152:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1068 : Float(320:1, requires_grad=0, device=cpu),\n      %1070 : Float(1920:320, 320:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1071 : Float(1920:1, requires_grad=0, device=cpu),\n      %1073 : Float(1920:9, 1:9, 3:3, 3:1, requires_grad=0, device=cpu),\n      %1074 : Float(1920:1, requires_grad=0, device=cpu),\n      %1076 : Float(320:1920, 1920:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1077 : Float(320:1, requires_grad=0, device=cpu),\n      %1079 : Float(1280:320, 320:1, 1:1, 1:1, requires_grad=0, device=cpu),\n      %1080 : Float(1280:1, requires_grad=0, device=cpu)):\n  %509 : Float(10:153228, 3:51076, 226:226, 226:1, requires_grad=0, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.1) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %874 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%509, %875, %876)\n  %input.3 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%874) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %513 : Float(10:415872, 32:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.3) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %877 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=32, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%513, %878, %879)\n  %x.3 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%877) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %517 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.3) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %518 : Float(10:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%517, %_blocks.0._se_reduce.weight, %_blocks.0._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %519 : Float(10:8, 8:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%518) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %520 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%519, %_blocks.0._se_expand.weight, %_blocks.0._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %521 : Float(10:32, 32:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%520) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %522 : Float(10:401408, 32:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Mul(%521, %x.3) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %880 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%522, %881, %882)\n  %525 : Float(10:207936, 16:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%880) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %883 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=16, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%525, %884, %885)\n  %x.6 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%883) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %529 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.6) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %530 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%529, %_blocks.1._se_reduce.weight, %_blocks.1._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %531 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%530) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %532 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%531, %_blocks.1._se_expand.weight, %_blocks.1._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %533 : Float(10:16, 16:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%532) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %534 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Mul(%533, %x.6) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %886 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%534, %887, %888)\n  %537 : Float(10:200704, 16:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Add(%886, %880) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %889 : Float(10:1204224, 96:12544, 112:112, 112:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%537, %890, %891)\n  %input.10 : Float(10:1204224, 96:12544, 112:112, 112:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%889) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %541 : Float(10:1247616, 96:12996, 114:114, 114:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.10) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %892 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=96, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%541, %893, %894)\n  %x.10 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%892) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %545 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.10) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %546 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%545, %_blocks.2._se_reduce.weight, %_blocks.2._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %547 : Float(10:4, 4:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%546) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %548 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%547, %_blocks.2._se_expand.weight, %_blocks.2._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %549 : Float(10:96, 96:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%548) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %550 : Float(10:301056, 96:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%549, %x.10) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %895 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%550, %896, %897)\n  %898 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%895, %899, %900)\n  %input.14 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%898) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %556 : Float(10:484416, 144:3364, 58:58, 58:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.14) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %901 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%556, %902, %903)\n  %x.14 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%901) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %560 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.14) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %561 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%560, %_blocks.3._se_reduce.weight, %_blocks.3._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %562 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%561) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %563 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%562, %_blocks.3._se_expand.weight, %_blocks.3._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %564 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%563) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %565 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%564, %x.14) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %904 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%565, %905, %906)\n  %568 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%904, %895) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %907 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%568, %908, %909)\n  %input.18 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%907) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %572 : Float(10:484416, 144:3364, 58:58, 58:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.18) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %910 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%572, %911, %912)\n  %x.18 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%910) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %576 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.18) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %577 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%576, %_blocks.4._se_reduce.weight, %_blocks.4._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %578 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%577) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %579 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%578, %_blocks.4._se_expand.weight, %_blocks.4._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %580 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%579) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %581 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Mul(%580, %x.18) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %913 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%581, %914, %915)\n  %584 : Float(10:75264, 24:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Add(%913, %568) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %916 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%584, %917, %918)\n  %input.22 : Float(10:451584, 144:3136, 56:56, 56:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%916) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %588 : Float(10:518400, 144:3600, 60:60, 60:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.22) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %919 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=144, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%588, %920, %921)\n  %x.22 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%919) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %592 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.22) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %593 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%592, %_blocks.5._se_reduce.weight, %_blocks.5._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %594 : Float(10:6, 6:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%593) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %595 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%594, %_blocks.5._se_expand.weight, %_blocks.5._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %596 : Float(10:144, 144:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%595) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %597 : Float(10:112896, 144:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%596, %x.22) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %922 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%597, %923, %924)\n  %925 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%922, %926, %927)\n  %input.26 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%925) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %603 : Float(10:245760, 240:1024, 32:32, 32:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.26) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %928 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%603, %929, %930)\n  %x.26 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%928) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %607 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.26) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %608 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%607, %_blocks.6._se_reduce.weight, %_blocks.6._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %609 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%608) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %610 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%609, %_blocks.6._se_expand.weight, %_blocks.6._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %611 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%610) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %612 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%611, %x.26) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %931 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%612, %932, %933)\n  %615 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%931, %922) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %934 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%615, %935, %936)\n  %input.30 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%934) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %619 : Float(10:245760, 240:1024, 32:32, 32:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.30) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %937 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%619, %938, %939)\n  %x.30 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%937) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %623 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.30) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %624 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%623, %_blocks.7._se_reduce.weight, %_blocks.7._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %625 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%624) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %626 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%625, %_blocks.7._se_expand.weight, %_blocks.7._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %627 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%626) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %628 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Mul(%627, %x.30) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %940 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%628, %941, %942)\n  %631 : Float(10:31360, 40:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Add(%940, %615) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %943 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%631, %944, %945)\n  %input.34 : Float(10:188160, 240:784, 28:28, 28:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%943) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %635 : Float(10:216000, 240:900, 30:30, 30:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.34) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %946 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=240, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%635, %947, %948)\n  %x.34 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%946) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %639 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.34) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %640 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%639, %_blocks.8._se_reduce.weight, %_blocks.8._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %641 : Float(10:10, 10:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%640) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %642 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%641, %_blocks.8._se_expand.weight, %_blocks.8._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %643 : Float(10:240, 240:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%642) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %644 : Float(10:47040, 240:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%643, %x.34) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %949 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%644, %950, %951)\n  %952 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%949, %953, %954)\n  %input.38 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%952) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %650 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.38) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %955 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%650, %956, %957)\n  %x.38 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%955) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %654 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.38) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %655 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%654, %_blocks.9._se_reduce.weight, %_blocks.9._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %656 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%655) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %657 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%656, %_blocks.9._se_expand.weight, %_blocks.9._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %658 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%657) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %659 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%658, %x.38) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %958 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%659, %959, %960)\n  %662 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%958, %949) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %961 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%662, %962, %963)\n  %input.42 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%961) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %666 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.42) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %964 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%666, %965, %966)\n  %x.42 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%964) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %670 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.42) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %671 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%670, %_blocks.10._se_reduce.weight, %_blocks.10._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %672 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%671) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %673 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%672, %_blocks.10._se_expand.weight, %_blocks.10._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %674 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%673) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %675 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%674, %x.42) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %967 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%675, %968, %969)\n  %678 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%967, %662) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %970 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%678, %971, %972)\n  %input.46 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%970) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %682 : Float(10:122880, 480:256, 16:16, 16:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.46) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %973 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%682, %974, %975)\n  %x.46 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%973) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %686 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.46) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %687 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%686, %_blocks.11._se_reduce.weight, %_blocks.11._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %688 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%687) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %689 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%688, %_blocks.11._se_expand.weight, %_blocks.11._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %690 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%689) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %691 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%690, %x.46) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %976 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%691, %977, %978)\n  %694 : Float(10:15680, 80:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%976, %678) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %979 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%694, %980, %981)\n  %input.50 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%979) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %698 : Float(10:155520, 480:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.50) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %982 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=480, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%698, %983, %984)\n  %x.50 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%982) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %702 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.50) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %703 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%702, %_blocks.12._se_reduce.weight, %_blocks.12._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %704 : Float(10:20, 20:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%703) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %705 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%704, %_blocks.12._se_expand.weight, %_blocks.12._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %706 : Float(10:480, 480:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%705) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %707 : Float(10:94080, 480:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%706, %x.50) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %985 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%707, %986, %987)\n  %988 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%985, %989, %990)\n  %input.54 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%988) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %713 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.54) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %991 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%713, %992, %993)\n  %x.54 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%991) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %717 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.54) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %718 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%717, %_blocks.13._se_reduce.weight, %_blocks.13._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %719 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%718) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %720 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%719, %_blocks.13._se_expand.weight, %_blocks.13._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %721 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%720) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %722 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%721, %x.54) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %994 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%722, %995, %996)\n  %725 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%994, %985) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %997 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%725, %998, %999)\n  %input.58 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%997) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %729 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.58) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1000 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%729, %1001, %1002)\n  %x.58 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1000) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %733 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.58) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %734 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%733, %_blocks.14._se_reduce.weight, %_blocks.14._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %735 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%734) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %736 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%735, %_blocks.14._se_expand.weight, %_blocks.14._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %737 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%736) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %738 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%737, %x.58) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1003 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%738, %1004, %1005)\n  %741 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%1003, %725) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1006 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%741, %1007, %1008)\n  %input.62 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1006) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %745 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.62) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1009 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%745, %1010, %1011)\n  %x.62 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1009) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %749 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.62) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %750 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%749, %_blocks.15._se_reduce.weight, %_blocks.15._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %751 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%750) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %752 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%751, %_blocks.15._se_expand.weight, %_blocks.15._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %753 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%752) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %754 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Mul(%753, %x.62) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1012 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%754, %1013, %1014)\n  %757 : Float(10:21952, 112:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Add(%1012, %741) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1015 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%757, %1016, %1017)\n  %input.66 : Float(10:131712, 672:196, 14:14, 14:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1015) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %761 : Float(10:217728, 672:324, 18:18, 18:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.66) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1018 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=672, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[2, 2]](%761, %1019, %1020)\n  %x.66 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1018) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %765 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.66) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %766 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%765, %_blocks.16._se_reduce.weight, %_blocks.16._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %767 : Float(10:28, 28:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%766) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %768 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%767, %_blocks.16._se_expand.weight, %_blocks.16._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %769 : Float(10:672, 672:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%768) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %770 : Float(10:32928, 672:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%769, %x.66) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1021 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%770, %1022, %1023)\n  %1024 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%1021, %1025, %1026)\n  %input.70 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1024) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %776 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.70) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1027 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%776, %1028, %1029)\n  %x.70 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1027) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %780 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.70) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %781 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%780, %_blocks.17._se_reduce.weight, %_blocks.17._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %782 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%781) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %783 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%782, %_blocks.17._se_expand.weight, %_blocks.17._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %784 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%783) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %785 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%784, %x.70) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1030 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%785, %1031, %1032)\n  %788 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1030, %1021) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1033 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%788, %1034, %1035)\n  %input.74 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1033) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %792 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.74) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1036 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%792, %1037, %1038)\n  %x.74 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1036) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %796 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.74) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %797 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%796, %_blocks.18._se_reduce.weight, %_blocks.18._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %798 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%797) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %799 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%798, %_blocks.18._se_expand.weight, %_blocks.18._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %800 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%799) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %801 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%800, %x.74) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1039 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%801, %1040, %1041)\n  %804 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1039, %788) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1042 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%804, %1043, %1044)\n  %input.78 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1042) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %808 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.78) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1045 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%808, %1046, %1047)\n  %x.78 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1045) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %812 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.78) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %813 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%812, %_blocks.19._se_reduce.weight, %_blocks.19._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %814 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%813) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %815 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%814, %_blocks.19._se_expand.weight, %_blocks.19._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %816 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%815) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %817 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%816, %x.78) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1048 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%817, %1049, %1050)\n  %820 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1048, %804) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1051 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%820, %1052, %1053)\n  %input.82 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1051) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %824 : Float(10:139392, 1152:121, 11:11, 11:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 2, 2, 0, 0, 2, 2], value=0.](%input.82) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1054 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[5, 5], pads=[0, 0, 0, 0], strides=[1, 1]](%824, %1055, %1056)\n  %x.82 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1054) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %828 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.82) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %829 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%828, %_blocks.20._se_reduce.weight, %_blocks.20._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %830 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%829) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %831 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%830, %_blocks.20._se_expand.weight, %_blocks.20._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %832 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%831) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %833 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%832, %x.82) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1057 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%833, %1058, %1059)\n  %836 : Float(10:9408, 192:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1057, %820) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1060 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%836, %1061, %1062)\n  %input.86 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1060) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %840 : Float(10:93312, 1152:81, 9:9, 9:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.86) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1063 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1152, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%840, %1064, %1065)\n  %x.86 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1063) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %844 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.86) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %845 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%844, %_blocks.21._se_reduce.weight, %_blocks.21._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %846 : Float(10:48, 48:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%845) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %847 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%846, %_blocks.21._se_expand.weight, %_blocks.21._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %848 : Float(10:1152, 1152:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%847) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %849 : Float(10:56448, 1152:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%848, %x.86) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1066 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%849, %1067, %1068)\n  %1069 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%1066, %1070, %1071)\n  %input.90 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1069) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %855 : Float(10:155520, 1920:81, 9:9, 9:1, requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", pads=[0, 0, 1, 1, 0, 0, 1, 1], value=0.](%input.90) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:3553:0\n  %1072 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1920, kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[1, 1]](%855, %1073, %1074)\n  %x.90 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1072) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %859 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%x.90) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %860 : Float(10:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%859, %_blocks.22._se_reduce.weight, %_blocks.22._se_reduce.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %861 : Float(10:80, 80:1, 1:1, 1:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%860) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %862 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%861, %_blocks.22._se_expand.weight, %_blocks.22._se_expand.bias) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:271:0\n  %863 : Float(10:1920, 1920:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::Sigmoid(%862) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %864 : Float(10:94080, 1920:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Mul(%863, %x.90) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:119:0\n  %1075 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%864, %1076, %1077)\n  %867 : Float(10:15680, 320:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Add(%1075, %1066) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/model.py:131:0\n  %1078 : Float(10:62720, 1280:49, 7:7, 7:1, requires_grad=1, device=cpu) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[1, 1], pads=[0, 0, 0, 0], strides=[1, 1]](%867, %1079, %1080)\n  %input.94 : Float(10:62720, 1280:49, 7:7, 7:1, requires_grad=1, device=cpu) = ^SwishImplementation()(%1078) # /usr/local/lib/python3.6/dist-packages/efficientnet_pytorch/utils.py:76:0\n  %871 : Float(10:1280, 1280:1, 1:1, 1:1, requires_grad=1, device=cpu) = onnx::GlobalAveragePool(%input.94) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:936:0\n  %872 : Float(10:1280, 1280:1, requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%871) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:983:0\n  %873 : Float(10:1000, 1000:1, requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1](%872, %_fc.weight, %_fc.bias) # /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1690:0\n  return (%873)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-oB_5qJusw4"
      },
      "source": [
        "https://github.com/lukemelas/EfficientNet-PyTorch/issues/91"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUXyT4c4uk2H"
      },
      "source": [
        "model = EfficientNet.from_name(model_name='efficientnet-b0')\r\n",
        "model.set_swish(memory_efficient=False)\r\n",
        "torch.onnx.export(model, torch.rand(10,3,240,240), \"EfficientNet-B0.onnx\")"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uG0jiOIdYybb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "b1d5f1fc-7ce6-4cf7-d077-c716d7216095"
      },
      "source": [
        "# Test export \n",
        "import onnx\n",
        "\n",
        "model = onnx.load(\"efficientnet-b1.onnx\")\n",
        "\n",
        "# Check that the IR is well formed\n",
        "onnx.checker.check_model(model)\n",
        "\n",
        "# Print a human readable representation of the graph\n",
        "onnx.helper.printable_graph(model.graph)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c3042f1a3549>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"efficientnet-b1.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check that the IR is well formed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mLoaded\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     '''\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'efficientnet-b1.onnx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "SVLBu4ZiuqWT",
        "outputId": "4a8c9d38-f256-42f2-c41c-288a700a2c35"
      },
      "source": [
        "# Test export \r\n",
        "import onnx\r\n",
        "\r\n",
        "model = onnx.load(\"efficientnet-B0.onnx\")\r\n",
        "\r\n",
        "# Check that the IR is well formed\r\n",
        "onnx.checker.check_model(model)\r\n",
        "\r\n",
        "# Print a human readable representation of the graph\r\n",
        "onnx.helper.printable_graph(model.graph)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d856dac12c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"efficientnet-B0.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check that the IR is well formed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mLoaded\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     '''\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/onnx/__init__.py\u001b[0m in \u001b[0;36m_load_bytes\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'efficientnet-B0.onnx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mVGFxctJu1ka",
        "outputId": "2b83a131-c700-4262-b93a-435bdf10f8b7"
      },
      "source": [
        "# Test export \r\n",
        "import onnx\r\n",
        "\r\n",
        "model = onnx.load(\"EfficientNet-B0.onnx\")\r\n",
        "\r\n",
        "# Check that the IR is well formed\r\n",
        "onnx.checker.check_model(model)\r\n",
        "\r\n",
        "# Print a human readable representation of the graph\r\n",
        "onnx.helper.printable_graph(model.graph)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"graph torch-jit-export (\\n  %input.1[FLOAT, 10x3x240x240]\\n) initializers (\\n  %667[FLOAT, 32x3x3x3]\\n  %668[FLOAT, 32]\\n  %670[FLOAT, 32x1x3x3]\\n  %671[FLOAT, 32]\\n  %673[FLOAT, 16x32x1x1]\\n  %674[FLOAT, 16]\\n  %676[FLOAT, 96x16x1x1]\\n  %677[FLOAT, 96]\\n  %679[FLOAT, 96x1x3x3]\\n  %680[FLOAT, 96]\\n  %682[FLOAT, 24x96x1x1]\\n  %683[FLOAT, 24]\\n  %685[FLOAT, 144x24x1x1]\\n  %686[FLOAT, 144]\\n  %688[FLOAT, 144x1x3x3]\\n  %689[FLOAT, 144]\\n  %691[FLOAT, 24x144x1x1]\\n  %692[FLOAT, 24]\\n  %694[FLOAT, 144x24x1x1]\\n  %695[FLOAT, 144]\\n  %697[FLOAT, 144x1x5x5]\\n  %698[FLOAT, 144]\\n  %700[FLOAT, 40x144x1x1]\\n  %701[FLOAT, 40]\\n  %703[FLOAT, 240x40x1x1]\\n  %704[FLOAT, 240]\\n  %706[FLOAT, 240x1x5x5]\\n  %707[FLOAT, 240]\\n  %709[FLOAT, 40x240x1x1]\\n  %710[FLOAT, 40]\\n  %712[FLOAT, 240x40x1x1]\\n  %713[FLOAT, 240]\\n  %715[FLOAT, 240x1x3x3]\\n  %716[FLOAT, 240]\\n  %718[FLOAT, 80x240x1x1]\\n  %719[FLOAT, 80]\\n  %721[FLOAT, 480x80x1x1]\\n  %722[FLOAT, 480]\\n  %724[FLOAT, 480x1x3x3]\\n  %725[FLOAT, 480]\\n  %727[FLOAT, 80x480x1x1]\\n  %728[FLOAT, 80]\\n  %730[FLOAT, 480x80x1x1]\\n  %731[FLOAT, 480]\\n  %733[FLOAT, 480x1x3x3]\\n  %734[FLOAT, 480]\\n  %736[FLOAT, 80x480x1x1]\\n  %737[FLOAT, 80]\\n  %739[FLOAT, 480x80x1x1]\\n  %740[FLOAT, 480]\\n  %742[FLOAT, 480x1x5x5]\\n  %743[FLOAT, 480]\\n  %745[FLOAT, 112x480x1x1]\\n  %746[FLOAT, 112]\\n  %748[FLOAT, 672x112x1x1]\\n  %749[FLOAT, 672]\\n  %751[FLOAT, 672x1x5x5]\\n  %752[FLOAT, 672]\\n  %754[FLOAT, 112x672x1x1]\\n  %755[FLOAT, 112]\\n  %757[FLOAT, 672x112x1x1]\\n  %758[FLOAT, 672]\\n  %760[FLOAT, 672x1x5x5]\\n  %761[FLOAT, 672]\\n  %763[FLOAT, 112x672x1x1]\\n  %764[FLOAT, 112]\\n  %766[FLOAT, 672x112x1x1]\\n  %767[FLOAT, 672]\\n  %769[FLOAT, 672x1x5x5]\\n  %770[FLOAT, 672]\\n  %772[FLOAT, 192x672x1x1]\\n  %773[FLOAT, 192]\\n  %775[FLOAT, 1152x192x1x1]\\n  %776[FLOAT, 1152]\\n  %778[FLOAT, 1152x1x5x5]\\n  %779[FLOAT, 1152]\\n  %781[FLOAT, 192x1152x1x1]\\n  %782[FLOAT, 192]\\n  %784[FLOAT, 1152x192x1x1]\\n  %785[FLOAT, 1152]\\n  %787[FLOAT, 1152x1x5x5]\\n  %788[FLOAT, 1152]\\n  %790[FLOAT, 192x1152x1x1]\\n  %791[FLOAT, 192]\\n  %793[FLOAT, 1152x192x1x1]\\n  %794[FLOAT, 1152]\\n  %796[FLOAT, 1152x1x5x5]\\n  %797[FLOAT, 1152]\\n  %799[FLOAT, 192x1152x1x1]\\n  %800[FLOAT, 192]\\n  %802[FLOAT, 1152x192x1x1]\\n  %803[FLOAT, 1152]\\n  %805[FLOAT, 1152x1x3x3]\\n  %806[FLOAT, 1152]\\n  %808[FLOAT, 320x1152x1x1]\\n  %809[FLOAT, 320]\\n  %811[FLOAT, 1280x320x1x1]\\n  %812[FLOAT, 1280]\\n  %_blocks.0._se_expand.bias[FLOAT, 32]\\n  %_blocks.0._se_expand.weight[FLOAT, 32x8x1x1]\\n  %_blocks.0._se_reduce.bias[FLOAT, 8]\\n  %_blocks.0._se_reduce.weight[FLOAT, 8x32x1x1]\\n  %_blocks.1._se_expand.bias[FLOAT, 96]\\n  %_blocks.1._se_expand.weight[FLOAT, 96x4x1x1]\\n  %_blocks.1._se_reduce.bias[FLOAT, 4]\\n  %_blocks.1._se_reduce.weight[FLOAT, 4x96x1x1]\\n  %_blocks.10._se_expand.bias[FLOAT, 672]\\n  %_blocks.10._se_expand.weight[FLOAT, 672x28x1x1]\\n  %_blocks.10._se_reduce.bias[FLOAT, 28]\\n  %_blocks.10._se_reduce.weight[FLOAT, 28x672x1x1]\\n  %_blocks.11._se_expand.bias[FLOAT, 672]\\n  %_blocks.11._se_expand.weight[FLOAT, 672x28x1x1]\\n  %_blocks.11._se_reduce.bias[FLOAT, 28]\\n  %_blocks.11._se_reduce.weight[FLOAT, 28x672x1x1]\\n  %_blocks.12._se_expand.bias[FLOAT, 1152]\\n  %_blocks.12._se_expand.weight[FLOAT, 1152x48x1x1]\\n  %_blocks.12._se_reduce.bias[FLOAT, 48]\\n  %_blocks.12._se_reduce.weight[FLOAT, 48x1152x1x1]\\n  %_blocks.13._se_expand.bias[FLOAT, 1152]\\n  %_blocks.13._se_expand.weight[FLOAT, 1152x48x1x1]\\n  %_blocks.13._se_reduce.bias[FLOAT, 48]\\n  %_blocks.13._se_reduce.weight[FLOAT, 48x1152x1x1]\\n  %_blocks.14._se_expand.bias[FLOAT, 1152]\\n  %_blocks.14._se_expand.weight[FLOAT, 1152x48x1x1]\\n  %_blocks.14._se_reduce.bias[FLOAT, 48]\\n  %_blocks.14._se_reduce.weight[FLOAT, 48x1152x1x1]\\n  %_blocks.15._se_expand.bias[FLOAT, 1152]\\n  %_blocks.15._se_expand.weight[FLOAT, 1152x48x1x1]\\n  %_blocks.15._se_reduce.bias[FLOAT, 48]\\n  %_blocks.15._se_reduce.weight[FLOAT, 48x1152x1x1]\\n  %_blocks.2._se_expand.bias[FLOAT, 144]\\n  %_blocks.2._se_expand.weight[FLOAT, 144x6x1x1]\\n  %_blocks.2._se_reduce.bias[FLOAT, 6]\\n  %_blocks.2._se_reduce.weight[FLOAT, 6x144x1x1]\\n  %_blocks.3._se_expand.bias[FLOAT, 144]\\n  %_blocks.3._se_expand.weight[FLOAT, 144x6x1x1]\\n  %_blocks.3._se_reduce.bias[FLOAT, 6]\\n  %_blocks.3._se_reduce.weight[FLOAT, 6x144x1x1]\\n  %_blocks.4._se_expand.bias[FLOAT, 240]\\n  %_blocks.4._se_expand.weight[FLOAT, 240x10x1x1]\\n  %_blocks.4._se_reduce.bias[FLOAT, 10]\\n  %_blocks.4._se_reduce.weight[FLOAT, 10x240x1x1]\\n  %_blocks.5._se_expand.bias[FLOAT, 240]\\n  %_blocks.5._se_expand.weight[FLOAT, 240x10x1x1]\\n  %_blocks.5._se_reduce.bias[FLOAT, 10]\\n  %_blocks.5._se_reduce.weight[FLOAT, 10x240x1x1]\\n  %_blocks.6._se_expand.bias[FLOAT, 480]\\n  %_blocks.6._se_expand.weight[FLOAT, 480x20x1x1]\\n  %_blocks.6._se_reduce.bias[FLOAT, 20]\\n  %_blocks.6._se_reduce.weight[FLOAT, 20x480x1x1]\\n  %_blocks.7._se_expand.bias[FLOAT, 480]\\n  %_blocks.7._se_expand.weight[FLOAT, 480x20x1x1]\\n  %_blocks.7._se_reduce.bias[FLOAT, 20]\\n  %_blocks.7._se_reduce.weight[FLOAT, 20x480x1x1]\\n  %_blocks.8._se_expand.bias[FLOAT, 480]\\n  %_blocks.8._se_expand.weight[FLOAT, 480x20x1x1]\\n  %_blocks.8._se_reduce.bias[FLOAT, 20]\\n  %_blocks.8._se_reduce.weight[FLOAT, 20x480x1x1]\\n  %_blocks.9._se_expand.bias[FLOAT, 672]\\n  %_blocks.9._se_expand.weight[FLOAT, 672x28x1x1]\\n  %_blocks.9._se_reduce.bias[FLOAT, 28]\\n  %_blocks.9._se_reduce.weight[FLOAT, 28x672x1x1]\\n  %_fc.bias[FLOAT, 1000]\\n  %_fc.weight[FLOAT, 1000x1280]\\n) {\\n  %361 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%input.1)\\n  %666 = Conv[dilations = [1, 1], group = 1, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%361, %667, %668)\\n  %364 = Sigmoid(%666)\\n  %365 = Mul(%666, %364)\\n  %366 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%365)\\n  %669 = Conv[dilations = [1, 1], group = 32, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%366, %670, %671)\\n  %369 = Sigmoid(%669)\\n  %370 = Mul(%669, %369)\\n  %371 = GlobalAveragePool(%370)\\n  %372 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%371, %_blocks.0._se_reduce.weight, %_blocks.0._se_reduce.bias)\\n  %373 = Sigmoid(%372)\\n  %374 = Mul(%372, %373)\\n  %375 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%374, %_blocks.0._se_expand.weight, %_blocks.0._se_expand.bias)\\n  %376 = Sigmoid(%375)\\n  %377 = Mul(%376, %370)\\n  %672 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%377, %673, %674)\\n  %675 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%672, %676, %677)\\n  %382 = Sigmoid(%675)\\n  %383 = Mul(%675, %382)\\n  %384 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%383)\\n  %678 = Conv[dilations = [1, 1], group = 96, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%384, %679, %680)\\n  %387 = Sigmoid(%678)\\n  %388 = Mul(%678, %387)\\n  %389 = GlobalAveragePool(%388)\\n  %390 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%389, %_blocks.1._se_reduce.weight, %_blocks.1._se_reduce.bias)\\n  %391 = Sigmoid(%390)\\n  %392 = Mul(%390, %391)\\n  %393 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%392, %_blocks.1._se_expand.weight, %_blocks.1._se_expand.bias)\\n  %394 = Sigmoid(%393)\\n  %395 = Mul(%394, %388)\\n  %681 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%395, %682, %683)\\n  %684 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%681, %685, %686)\\n  %400 = Sigmoid(%684)\\n  %401 = Mul(%684, %400)\\n  %402 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%401)\\n  %687 = Conv[dilations = [1, 1], group = 144, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%402, %688, %689)\\n  %405 = Sigmoid(%687)\\n  %406 = Mul(%687, %405)\\n  %407 = GlobalAveragePool(%406)\\n  %408 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%407, %_blocks.2._se_reduce.weight, %_blocks.2._se_reduce.bias)\\n  %409 = Sigmoid(%408)\\n  %410 = Mul(%408, %409)\\n  %411 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%410, %_blocks.2._se_expand.weight, %_blocks.2._se_expand.bias)\\n  %412 = Sigmoid(%411)\\n  %413 = Mul(%412, %406)\\n  %690 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%413, %691, %692)\\n  %416 = Add(%690, %681)\\n  %693 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%416, %694, %695)\\n  %419 = Sigmoid(%693)\\n  %420 = Mul(%693, %419)\\n  %421 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%420)\\n  %696 = Conv[dilations = [1, 1], group = 144, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [2, 2]](%421, %697, %698)\\n  %424 = Sigmoid(%696)\\n  %425 = Mul(%696, %424)\\n  %426 = GlobalAveragePool(%425)\\n  %427 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%426, %_blocks.3._se_reduce.weight, %_blocks.3._se_reduce.bias)\\n  %428 = Sigmoid(%427)\\n  %429 = Mul(%427, %428)\\n  %430 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%429, %_blocks.3._se_expand.weight, %_blocks.3._se_expand.bias)\\n  %431 = Sigmoid(%430)\\n  %432 = Mul(%431, %425)\\n  %699 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%432, %700, %701)\\n  %702 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%699, %703, %704)\\n  %437 = Sigmoid(%702)\\n  %438 = Mul(%702, %437)\\n  %439 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%438)\\n  %705 = Conv[dilations = [1, 1], group = 240, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%439, %706, %707)\\n  %442 = Sigmoid(%705)\\n  %443 = Mul(%705, %442)\\n  %444 = GlobalAveragePool(%443)\\n  %445 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%444, %_blocks.4._se_reduce.weight, %_blocks.4._se_reduce.bias)\\n  %446 = Sigmoid(%445)\\n  %447 = Mul(%445, %446)\\n  %448 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%447, %_blocks.4._se_expand.weight, %_blocks.4._se_expand.bias)\\n  %449 = Sigmoid(%448)\\n  %450 = Mul(%449, %443)\\n  %708 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%450, %709, %710)\\n  %453 = Add(%708, %699)\\n  %711 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%453, %712, %713)\\n  %456 = Sigmoid(%711)\\n  %457 = Mul(%711, %456)\\n  %458 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%457)\\n  %714 = Conv[dilations = [1, 1], group = 240, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [2, 2]](%458, %715, %716)\\n  %461 = Sigmoid(%714)\\n  %462 = Mul(%714, %461)\\n  %463 = GlobalAveragePool(%462)\\n  %464 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%463, %_blocks.5._se_reduce.weight, %_blocks.5._se_reduce.bias)\\n  %465 = Sigmoid(%464)\\n  %466 = Mul(%464, %465)\\n  %467 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%466, %_blocks.5._se_expand.weight, %_blocks.5._se_expand.bias)\\n  %468 = Sigmoid(%467)\\n  %469 = Mul(%468, %462)\\n  %717 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%469, %718, %719)\\n  %720 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%717, %721, %722)\\n  %474 = Sigmoid(%720)\\n  %475 = Mul(%720, %474)\\n  %476 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%475)\\n  %723 = Conv[dilations = [1, 1], group = 480, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%476, %724, %725)\\n  %479 = Sigmoid(%723)\\n  %480 = Mul(%723, %479)\\n  %481 = GlobalAveragePool(%480)\\n  %482 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%481, %_blocks.6._se_reduce.weight, %_blocks.6._se_reduce.bias)\\n  %483 = Sigmoid(%482)\\n  %484 = Mul(%482, %483)\\n  %485 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%484, %_blocks.6._se_expand.weight, %_blocks.6._se_expand.bias)\\n  %486 = Sigmoid(%485)\\n  %487 = Mul(%486, %480)\\n  %726 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%487, %727, %728)\\n  %490 = Add(%726, %717)\\n  %729 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%490, %730, %731)\\n  %493 = Sigmoid(%729)\\n  %494 = Mul(%729, %493)\\n  %495 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%494)\\n  %732 = Conv[dilations = [1, 1], group = 480, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%495, %733, %734)\\n  %498 = Sigmoid(%732)\\n  %499 = Mul(%732, %498)\\n  %500 = GlobalAveragePool(%499)\\n  %501 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%500, %_blocks.7._se_reduce.weight, %_blocks.7._se_reduce.bias)\\n  %502 = Sigmoid(%501)\\n  %503 = Mul(%501, %502)\\n  %504 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%503, %_blocks.7._se_expand.weight, %_blocks.7._se_expand.bias)\\n  %505 = Sigmoid(%504)\\n  %506 = Mul(%505, %499)\\n  %735 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%506, %736, %737)\\n  %509 = Add(%735, %490)\\n  %738 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%509, %739, %740)\\n  %512 = Sigmoid(%738)\\n  %513 = Mul(%738, %512)\\n  %514 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%513)\\n  %741 = Conv[dilations = [1, 1], group = 480, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%514, %742, %743)\\n  %517 = Sigmoid(%741)\\n  %518 = Mul(%741, %517)\\n  %519 = GlobalAveragePool(%518)\\n  %520 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%519, %_blocks.8._se_reduce.weight, %_blocks.8._se_reduce.bias)\\n  %521 = Sigmoid(%520)\\n  %522 = Mul(%520, %521)\\n  %523 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%522, %_blocks.8._se_expand.weight, %_blocks.8._se_expand.bias)\\n  %524 = Sigmoid(%523)\\n  %525 = Mul(%524, %518)\\n  %744 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%525, %745, %746)\\n  %747 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%744, %748, %749)\\n  %530 = Sigmoid(%747)\\n  %531 = Mul(%747, %530)\\n  %532 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%531)\\n  %750 = Conv[dilations = [1, 1], group = 672, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%532, %751, %752)\\n  %535 = Sigmoid(%750)\\n  %536 = Mul(%750, %535)\\n  %537 = GlobalAveragePool(%536)\\n  %538 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%537, %_blocks.9._se_reduce.weight, %_blocks.9._se_reduce.bias)\\n  %539 = Sigmoid(%538)\\n  %540 = Mul(%538, %539)\\n  %541 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%540, %_blocks.9._se_expand.weight, %_blocks.9._se_expand.bias)\\n  %542 = Sigmoid(%541)\\n  %543 = Mul(%542, %536)\\n  %753 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%543, %754, %755)\\n  %546 = Add(%753, %744)\\n  %756 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%546, %757, %758)\\n  %549 = Sigmoid(%756)\\n  %550 = Mul(%756, %549)\\n  %551 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%550)\\n  %759 = Conv[dilations = [1, 1], group = 672, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%551, %760, %761)\\n  %554 = Sigmoid(%759)\\n  %555 = Mul(%759, %554)\\n  %556 = GlobalAveragePool(%555)\\n  %557 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%556, %_blocks.10._se_reduce.weight, %_blocks.10._se_reduce.bias)\\n  %558 = Sigmoid(%557)\\n  %559 = Mul(%557, %558)\\n  %560 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%559, %_blocks.10._se_expand.weight, %_blocks.10._se_expand.bias)\\n  %561 = Sigmoid(%560)\\n  %562 = Mul(%561, %555)\\n  %762 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%562, %763, %764)\\n  %565 = Add(%762, %546)\\n  %765 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%565, %766, %767)\\n  %568 = Sigmoid(%765)\\n  %569 = Mul(%765, %568)\\n  %570 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%569)\\n  %768 = Conv[dilations = [1, 1], group = 672, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [2, 2]](%570, %769, %770)\\n  %573 = Sigmoid(%768)\\n  %574 = Mul(%768, %573)\\n  %575 = GlobalAveragePool(%574)\\n  %576 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%575, %_blocks.11._se_reduce.weight, %_blocks.11._se_reduce.bias)\\n  %577 = Sigmoid(%576)\\n  %578 = Mul(%576, %577)\\n  %579 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%578, %_blocks.11._se_expand.weight, %_blocks.11._se_expand.bias)\\n  %580 = Sigmoid(%579)\\n  %581 = Mul(%580, %574)\\n  %771 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%581, %772, %773)\\n  %774 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%771, %775, %776)\\n  %586 = Sigmoid(%774)\\n  %587 = Mul(%774, %586)\\n  %588 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%587)\\n  %777 = Conv[dilations = [1, 1], group = 1152, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%588, %778, %779)\\n  %591 = Sigmoid(%777)\\n  %592 = Mul(%777, %591)\\n  %593 = GlobalAveragePool(%592)\\n  %594 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%593, %_blocks.12._se_reduce.weight, %_blocks.12._se_reduce.bias)\\n  %595 = Sigmoid(%594)\\n  %596 = Mul(%594, %595)\\n  %597 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%596, %_blocks.12._se_expand.weight, %_blocks.12._se_expand.bias)\\n  %598 = Sigmoid(%597)\\n  %599 = Mul(%598, %592)\\n  %780 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%599, %781, %782)\\n  %602 = Add(%780, %771)\\n  %783 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%602, %784, %785)\\n  %605 = Sigmoid(%783)\\n  %606 = Mul(%783, %605)\\n  %607 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%606)\\n  %786 = Conv[dilations = [1, 1], group = 1152, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%607, %787, %788)\\n  %610 = Sigmoid(%786)\\n  %611 = Mul(%786, %610)\\n  %612 = GlobalAveragePool(%611)\\n  %613 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%612, %_blocks.13._se_reduce.weight, %_blocks.13._se_reduce.bias)\\n  %614 = Sigmoid(%613)\\n  %615 = Mul(%613, %614)\\n  %616 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%615, %_blocks.13._se_expand.weight, %_blocks.13._se_expand.bias)\\n  %617 = Sigmoid(%616)\\n  %618 = Mul(%617, %611)\\n  %789 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%618, %790, %791)\\n  %621 = Add(%789, %602)\\n  %792 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%621, %793, %794)\\n  %624 = Sigmoid(%792)\\n  %625 = Mul(%792, %624)\\n  %626 = Pad[mode = 'constant', pads = [0, 0, 2, 2, 0, 0, 2, 2], value = 0](%625)\\n  %795 = Conv[dilations = [1, 1], group = 1152, kernel_shape = [5, 5], pads = [0, 0, 0, 0], strides = [1, 1]](%626, %796, %797)\\n  %629 = Sigmoid(%795)\\n  %630 = Mul(%795, %629)\\n  %631 = GlobalAveragePool(%630)\\n  %632 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%631, %_blocks.14._se_reduce.weight, %_blocks.14._se_reduce.bias)\\n  %633 = Sigmoid(%632)\\n  %634 = Mul(%632, %633)\\n  %635 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%634, %_blocks.14._se_expand.weight, %_blocks.14._se_expand.bias)\\n  %636 = Sigmoid(%635)\\n  %637 = Mul(%636, %630)\\n  %798 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%637, %799, %800)\\n  %640 = Add(%798, %621)\\n  %801 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%640, %802, %803)\\n  %643 = Sigmoid(%801)\\n  %644 = Mul(%801, %643)\\n  %645 = Pad[mode = 'constant', pads = [0, 0, 1, 1, 0, 0, 1, 1], value = 0](%644)\\n  %804 = Conv[dilations = [1, 1], group = 1152, kernel_shape = [3, 3], pads = [0, 0, 0, 0], strides = [1, 1]](%645, %805, %806)\\n  %648 = Sigmoid(%804)\\n  %649 = Mul(%804, %648)\\n  %650 = GlobalAveragePool(%649)\\n  %651 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%650, %_blocks.15._se_reduce.weight, %_blocks.15._se_reduce.bias)\\n  %652 = Sigmoid(%651)\\n  %653 = Mul(%651, %652)\\n  %654 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%653, %_blocks.15._se_expand.weight, %_blocks.15._se_expand.bias)\\n  %655 = Sigmoid(%654)\\n  %656 = Mul(%655, %649)\\n  %807 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%656, %808, %809)\\n  %810 = Conv[dilations = [1, 1], group = 1, kernel_shape = [1, 1], pads = [0, 0, 0, 0], strides = [1, 1]](%807, %811, %812)\\n  %661 = Sigmoid(%810)\\n  %662 = Mul(%810, %661)\\n  %663 = GlobalAveragePool(%662)\\n  %664 = Flatten[axis = 1](%663)\\n  %665 = Gemm[alpha = 1, beta = 1, transB = 1](%664, %_fc.weight, %_fc.bias)\\n  return %665\\n}\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}