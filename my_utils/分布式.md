

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-07 22:21:53
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-11-27 20:01:43
 * @Description:
 * @TODO::
 * @Reference:
-->

为了解决这些问题，从业者越来越多地转向分布式训练。 分布式训练是使用多个GPU和/或多个机器训练深度学习模型的技术。 分布式训练作业使您能够克服单GPU内存瓶颈，通过同时利用多个GPU来开发更大，功能更强大的模型。[1]

涵盖torch.dist和DistributedDataParallel的相关功能，并举例说明如何使用它们




1. 分布式训练策略
模型并行：用于模型过大的情况，需要把模型的不同层放在不同节点orGPU上，计算效率不高，不常用。
数据并行：把数据分成多份，每份数据单独进行前向计算和梯度更新，效率高，较常用。
2. 分布式并行模式
同步训练：所有进程前向完成后统一计算梯度，统一反向更新。
异步训练：每个进程计算自己的梯度，并拷贝主节点的参数进行更新，容易造成错乱，陷入次优解。
3. 分布式训练架构
Parameter Server：集群中有一个parameter server和多个worker，server需要等待所有节点计算完毕统一计算梯度，在server上更新参数，之后把新的参数广播给worker。
Ring AllReduce：只有worker，所有worker形成一个闭环，接受上家的梯度，再把累加好的梯度传给下家，最终计算完成后更新整个环上worker的梯度（这样所有worker上的梯度就相等了），然后求梯度反向传播。比PS架构要高效。
4. Tensorflow分布式框架
Horovod
TF>=1.11中官方提供了AllReduce策略，支持Estimator API：CollectiveAllReduceStrategy
https://github.com/logicalclocks/hops-examples/tree/master/tensorflow/notebooks/Distributed_Training/collective_allreduce_strategy
5. Pytorch分布式框架
torch.nn.DataParallel：数据并行，PS架构，不建议
torch.distributed.DistributedDataParallel：数据并行，优于DataParallel，好像仍是PS架构，建议
NVIDIA/apex：封装了DistributedDataParallel，AllReduce架构，
https://github.com/nvidia/apex

建议
参考资料
机器之心：GPU捉襟见肘还想训练大批量模型？谁说不可以
https://zhuanlan.zhihu.com/p/46972713
pytorch 1.0 分布式
https://zhuanlan.zhihu.com/p/52110617
pytorch分布式训练
https://zhuanlan.zhihu.com/p/58620622
一文说清楚Tensorflow分布式训练必备知识
https://zhuanlan.zhihu.com/p/56991108




[1]: https://github.com/zergtant/pytorch-handbook/tree/master/chapter4/distributeddataparallel


