

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-07 22:21:53
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-26 19:00:20
 * @Description:
 * @TODO::
 * @Reference:
-->

为了解决这些问题，从业者越来越多地转向分布式训练。 分布式训练是使用多个GPU和/或多个机器训练深度学习模型的技术。 分布式训练作业使您能够克服单GPU内存瓶颈，通过同时利用多个GPU来开发更大，功能更强大的模型。[1]

涵盖torch.dist和DistributedDataParallel的相关功能，并举例说明如何使用它们




1. 分布式训练策略
模型并行：用于模型过大的情况，需要把模型的不同层放在不同节点orGPU上，计算效率不高，不常用。
数据并行：把数据分成多份，每份数据单独进行前向计算和梯度更新，效率高，较常用。
2. 分布式并行模式
同步训练：所有进程前向完成后统一计算梯度，统一反向更新。
异步训练：每个进程计算自己的梯度，并拷贝主节点的参数进行更新，容易造成错乱，陷入次优解。
3. 分布式训练架构
Parameter Server：集群中有一个parameter server和多个worker，server需要等待所有节点计算完毕统一计算梯度，在server上更新参数，之后把新的参数广播给worker。
Ring AllReduce：只有worker，所有worker形成一个闭环，接受上家的梯度，再把累加好的梯度传给下家，最终计算完成后更新整个环上worker的梯度（这样所有worker上的梯度就相等了），然后求梯度反向传播。比PS架构要高效。
4. Tensorflow分布式框架
Horovod
TF>=1.11中官方提供了AllReduce策略，支持Estimator API：CollectiveAllReduceStrategy
https://github.com/logicalclocks/hops-examples/tree/master/tensorflow/notebooks/Distributed_Training/collective_allreduce_strategy
5. Pytorch分布式框架
torch.nn.DataParallel：数据并行，PS架构，不建议
torch.distributed.DistributedDataParallel：数据并行，优于DataParallel，好像仍是PS架构，建议
NVIDIA/apex：封装了DistributedDataParallel，AllReduce架构，
https://github.com/nvidia/apex

建议
参考资料
机器之心：GPU捉襟见肘还想训练大批量模型？谁说不可以
https://zhuanlan.zhihu.com/p/46972713
pytorch 1.0 分布式
https://zhuanlan.zhihu.com/p/52110617
pytorch分布式训练
https://zhuanlan.zhihu.com/p/58620622
一文说清楚Tensorflow分布式训练必备知识
https://zhuanlan.zhihu.com/p/56991108

[2]
    if args.multi_gpu == 'ddp' and torch.distributed.is_initialized():
        para_model = DistributedDataParallel(model,
                                             device_ids=[args.local_rank],
                                             output_device=args.local_rank,
                                             broadcast_buffers=False,
                                             find_unused_parameters=True,
                                             )
    elif args.multi_gpu == 'dp':
        if args.gpu0_bsz >= 0:
            para_model = BalancedDataParallel(args.gpu0_bsz // args.batch_chunk,
                                              model, dim=1).to(device)
        else:
            para_model = nn.DataParallel(model, dim=1).to(device)
    else:
        para_model = model


[1]: https://github.com/zergtant/pytorch-handbook/tree/master/chapter4/distributeddataparallel
[2]: https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/LanguageModeling/Transformer-XL/pytorch/train.py

---
https://github.com/leaderj1001/MobileNetV3-Pytorch/blob/master/preprocess.py

        if args.distributed:
            train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)
        else:
            train_sampler = None

---

https://gitee.com/chenhanxuan/pytorch-handbook/blob/master/chapter4/4.5-multiply-gpu-parallel-training.ipynb

torch.distributed
torch.distributed相对于torch.nn.DataParalle 是一个底层的API，所以我们要修改我们的代码，使其能够独立的在机器（节点）中运行。我们想要完全实现分布式，并且在每个结点的每个GPU上独立运行进程，这一共需要N个进程。N是我们的GPU总数，这里我们以4来计算。

首先 初始化分布式后端，封装模型以及准备数据，这些数据用于在独立的数据子集中训练进程。修改后的代码如下

```py
# 以下脚本在jupyter notebook执行肯定会不成功，请保存成py文件后测试
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import DataLoader

# 这里的node_rank是本地GPU的标识
parser = argparse.ArgumentParser()
parser.add_argument("--node_rank", type=int)
args = parser.parse_args()

# 使用Nvdea的nccl来初始化节点
torch.distributed.init_process_group(backend='nccl')

# 封装分配给当前进程的GPU上的模型
device = torch.device('cuda', arg.local_rank)
model = model.to(device)
distrib_model = torch.nn.parallel.DistributedDataParallel(model,
                                                          device_ids=[args.node_rank],
                                                          output_device=args.node_rank)

# 将数据加载限制为数据集的子集（不包括当前进程）
sampler = DistributedSampler(dataset)

dataloader = DataLoader(dataset, sampler=sampler)
for inputs, labels in dataloader:
    predictions = distrib_model(inputs.to(device))         # 正向传播
    loss = loss_function(predictions, labels.to(device))   # 计算损失
    loss.backward()                                        # 反向传播
    optimizer.step()                                       # 优化
```
在运行时我们也不能简单的使用python 文件名来执行了，我们这里需要使用PyTorch中为我们准备好的torch.distributed.launch运行脚本。它能自动进行环境变量的设置，并使用正确的node_rank参数调用脚本。

这里我们要准备一台机器作为master，所有的机器都要求能对它进行访问。因此，它需要拥有一个可以访问的IP地址（示例中为：196.168.100.100）以及一个开放的端口（示例中为：6666）。我们将使用torch.distributed.launch在第一台机器上运行脚本：

python -m torch.distributed.launch --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr="192.168.100.100" --master_port=6666 文件名 (--arg1 --arg2 等其他参数)
第二台主机上只需要更改 --node_rank=0即可

很有可能你在运行的时候报错，那是因为我们没有设置NCCL socket网络接口
我们以网卡名为ens3为例，输入

export NCCL_SOCKET_IFNAME=ens3
ens3这个名称 可以使用ifconfig命令查看确认

参数说明：

--nproc_per_node ： 主机中包含的GPU总数

--nnodes ： 总计的主机数

--node_rank ：主机中的GPU标识

其他一些参数可以查看官方的文档

torch.distributed 不仅支持nccl还支持其他的两个后端 gloo和mpi，具体的对比这里就不细说了，请查看官方的文档
