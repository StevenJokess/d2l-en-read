

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 20:47:47
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-05 22:58:51
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/adam.html#yogi
 * https://zh.d2l.ai/chapter_optimization/adam.html
-->

# Adam

在本节之前的讨论中，我们遇到了一些有效优化的技术。让我们在这里详细回顾一下:

* 我们看到11.4节在解决优化问题时比梯度下降法更有效，例如，由于其固有的对冗余数据的弹性。
* 我们看到，11.5节提供了显着的额外效率产生的向量化，使用一个小批量更大的观测集。这是高效多机、多 gpu和整体并行处理的关键。
* 第11.6节增加了一个机制，用于汇总过去的梯度历史，以加速收敛。
* 第11.7节使用每个坐标缩放，以允许一个计算有效的前置条件。
* 第11.8节从学习率调整中解耦了每个坐标的缩放。

Adam[Kingma & Ba, 2014] 将所有这些技术组合成一个有效的学习算法。正如预期的那样，这是一个相当流行的算法，作为一个更健壮和有效的优化算法在深度学习中使用。不过，这并非没有问题。特别是，[ Reddi 等人，2019]表明，在有些情况下，亚当可能会由于差的方差控制而分歧。在后续的工作中[ Zaheer 等人，2018]提出了一个修复亚当，称为瑜伽，解决这些问题。稍后将详细介绍。现在让我们回顾一下 Adam 算法。

## 算法

Adam 的一个关键组成部分是，它使用指数加权移动平均值(也称为泄漏平均值)来获得动量和梯度的第二阶矩的估计。也就是说，它使用状态变量

TODO:MATH

这里11和22是非负加权参数。他们通常的选择是1 = 0.91 = 0.9和2 = 0.9992 = 0.999。也就是说，方差估计的变化比动量项慢得多。请注意，如果我们初始化 v0 = s0 = 0 v0 = s0 = 0，我们最初会对较小的值有很大的偏见。这可以通过使用∑ ti = 0i = 1-t1-∑ i = 0ti = 1-t1-来重新规范化项来解决。给出了相应的归一化状态变量

TODO:MATH

有了适当的估计，我们现在就可以写出更新方程式了。首先，我们以一种非常类似于 RMSProp 的方式对梯度进行调整以获得

TODO:MATH

与 RMSProp 不同，我们的更新使用了动量 v ^ t v ^ t 而不是梯度本身。此外，还有一个轻微的表面上的区别，因为重新标记发生时使用1 s ^ t √ + 1 s ^ t + 而不是1 s ^ t + √1 s ^ t + 。前者在实践中的效果可以说稍微好一些，因此偏离了 RMSProp。我们通常会在数值稳定性和保真度之间选择 = 10-6 = 10-6。

现在我们已经有了计算更新所需的所有部件。这有点虎头蛇尾，我们对表单进行了简单的更新

TODO:MATH

回顾亚当的设计，其灵感是显而易见的。动量和尺度在状态变量中清晰可见。它们相当特殊的定义迫使我们去除项(这可以通过稍微不同的初始化和更新条件来修正)。其次，考虑到 RMSProp，这两个术语的组合非常简单。最后，显式学习速率允许我们控制步长来解决收敛问题。

## 实施

从零开始实现亚当并不令人畏惧。为了方便起见，我们将 timestep 计数器 t 存储在 hyperparams 字典中。除此之外，一切都很简单。

TODO:CODE

我们准备使用亚当来训练模型。我们使用η=0.01η= 0.01的学习率。

TODO:CODE

由于adam是作为Gluon训练器优化库的一部分提供的算法之一，因此更简洁的实现是简单明了的。因此，我们只需要为Gluon中的实现传递配置参数。

TODO:CODE

## Yogi

亚当的问题之一是，即使在凸面设置中，当stst中的第二矩估计值爆炸时，它也可能无法收敛。作为修复[Zaheer等，2018]提出了stst的改进的更新（和初始化）。要了解发生了什么，让我们重写Adam更新，如下所示：

TODO:MATH

每当g2tgt2有高方差或更新是稀疏的，stst可能会很快忘记过去的值。可能的补救方法就是取代g2t圣圣−−1 gt2−−1×g2t⊙胡志明市(g2t圣−−1)gt2⊙胡志明市⁡(gt2圣−−1)。现在更新的大小不再取决于偏差的大小。这会产生瑜伽修行者的更新

TODO:MATH

作者还建议初始化动量在一个更大的初始批而不是仅仅初始点态估计。我们省略了细节，因为它们不是讨论的实质，因为即使没有这种聚合仍然很好。

TODO:CODE

## 小结

* 亚当将许多优化算法的功能组合到一个相当健壮的更新规则中。
* 在RMSProp的基础上创建的Adam在最小批量随机梯度上也使用了EWMA
* 当估算动量和第二力矩时，Adam使用偏差校正来调整启动速度。
* 对于具有明显方差的渐变，我们可能会遇到收敛问题。可以通过使用较大的迷你批处理或切换到改进的stst估算值来对其进行修改。瑜伽士提供了这样的选择。

## 练习

1. 调整学习率，观察并分析实验结果。
1. 您可以重写动量和第二时刻更新，使其不需要偏差校正吗？
1. 为什么我们收敛时需要降低学习率ηη？
1. 尝试建立亚当背离而瑜伽士趋同的情况吗？
