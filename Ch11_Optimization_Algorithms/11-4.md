

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-06 20:29:13
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-06 20:48:03
 * @Description:MT half
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/sgd.html
 * https://zh.d2l.ai/chapter_optimization/gd-sgd.html
-->

# 随机梯度下降（stochastic gradient descent）

在这一节中，我们将介绍随机梯度下降（stochastic gradient descent）的基本原理。

## 随机梯度更新

在深度学习中，目标函数通常是训练数据集中每个示例的损失函数的平均值。 我们假设fi（x）fi（x）是训练数据集的损失函数，具有nn个示例，索引为ii，参数向量为xx，则我们有目标函数

TODO:MATH




如果使用梯度下降，则每个自变量迭代的计算成本为O（n）O（n），它随nn线性增长。 因此，当模型训练数据集很大时，每次迭代的梯度下降成本将非常高。

随机梯度下降（SGD）降低了每次迭代的计算成本。 在随机梯度下降的每次迭代中，我们随机地为数据示例统一采样索引i∈{1，…，n}i∈{1，…，n}，并计算梯度∇fi（x）∇fi（x ）更新xx：

（11.4.3）

x←x-η∇fi（x）。

x←x-η∇fi（x）。

在此，ηη是学习率。 我们可以看到，每次迭代的计算成本都从梯度下降的O（n）O（n）降至常数O（1）O（1）。 我们应该提到，随机梯度∇fi（x）∇fi（x）是梯度∇f（x）∇f（x）的无偏估计。

（11.4.4）

Ei∇fi（x）= 1n∑i =1n∇fi（x）=∇f（x）。

Ei∇fi（x）= 1n∑i =1n∇fi（x）=∇f（x）。

这意味着，平均而言，随机梯度是对梯度的良好估计。

现在，我们将通过向梯度中添加平均值为0，方差为1的随机噪声来模拟SGD，将其与梯度下降进行比较。

TODO:CODE

正如我们所看到的，SGD中变量的轨迹比我们在上一节中观察到的梯度下降的轨迹噪声更大。这是由于梯度的随机性。也就是说，即使在接近最小值的时候，我们仍然受到了由er∇fi(x)和er∇fi(x)注入的瞬时梯度的不确定性的影响。即使经过50步，质量仍然不是很好。更糟糕的是,它不会提高额外的步骤之后(我们鼓励读者与大量的实验步骤自己证实了这一点)。这给我们留下了唯一的选择——改变学习速度。然而，如果我们选择的太小，我们将不会取得任何有意义的进展。另一方面，如果我们选择太大，我们将不会得到一个好的解决方案，如上所示。解决这些冲突目标的唯一方法是随着优化的进展动态地降低学习率。

这也是增加的原因学习速率函数lr sgd阶跃函数。在上面的例子中，任何用于学习速率调度的功能都处于休眠状态，因为我们将相关的lr函数设置为常量，即lr = (lambda: 1)。

## 动态学习速率

用一个依赖时间的学习率的优化算法来代替最优优化，增加了优化算法控制收敛的复杂性。特别是，需要弄清楚它的腐烂速度。如果它太快，我们就会过早地停止优化。如果我们降低速度太慢，就会在优化上浪费太多时间。随着时间的推移，有一些基本的策略用于调整资产管理(我们将在后面的章节讨论更高级的策略):

(11.4.5)

η(t)η(t)η(t) =η我如果ti≤t≤ti + 1 =η0⋅e−t =λη0⋅(t + 1)β−α分段constantexponentialpolynomial

η(t) =η我如果ti≤t≤ti + 1分段常数η(t) =η0⋅e−λtexponentialη(t) =η0⋅(t + 1)β−α多项式

在第一种情况下，我们降低了学习率，例如，每当优化的进展停滞时。这是训练深度网络的常见策略。或者，我们可以通过指数衰减的方式更积极地降低它。不幸的是，这导致在算法收敛之前过早停止。一个普遍的选择是多项式衰减与次幂=0.5次幂=0.5。在凸优化的情况下有很多的证据表明,这种速度的表现。让我们看看实际情况是怎样的。

TODO:CODE

正如预期的那样，参数中的差异显著减少。但是，这样做的代价是不能收敛到最优解x=(0,0)x=(0,0)。即使在1000步之后，我们仍然离最佳解决方案很远。实际上，该算法根本无法收敛。另一方面，如果我们使用一个多项式衰减当学习速率随着阶数的平方根的倒数而衰减收敛是好的。

TODO:CODE

对于如何设置学习率有更多的选择。例如，我们可以从一个小速率开始，然后迅速上升，然后再次下降，尽管速度更慢。我们甚至可以在更小和更大的学习率之间进行交替。存在着各种各样的这样的时间表。现在，让我们集中在学习率计划，这是一个全面的理论分析是可能的，即，在凸设置的学习率。对于一般的非凸问题，由于一般情况下最小化非线性非凸问题是NP困难的，因此很难得到有意义的收敛保证。关于调查，请参见2015年Tibshirani的优秀[演讲笔记](https://www.stat.cmu.edu/~ryantibs/convexopt-F15/lectures/26-nonconvex.pdf)。

## 凸目标的收敛分析

下面的选项是可选的，主要是为了传达更多关于这个问题的直觉。我们限制自己在一个最简单的证明，如描述的[Nesterov & Vial, 2000]。更先进的证明技术存在,例如,当目标函数表现得尤其好。[领唱者et al ., 2008)表明,强凸函数,也就是说,从下面可以有界函数,用x⊤Qxx⊤Qx,可以减少他们在一个小数量的步骤,同时减少了学习速率像η(t) =η0 / t + 1(β)η(t) =η0 / t + 1(β)。不幸的是，这种情况在深度学习中从未真正发生过，我们在实践中所面临的下降速度要慢得多。

TODO:MATH

考虑这种情况

TODO:MATH

已知的时间范围。当r、Lr、L和TT已知时，我们可以选择:猥亵=r/LT - - -√猥亵=r/LT。得到的上界为rL(1+1/T)/2T−−√<rL/T−−√rL(1+1/T)/2T<rL/T。即以O(1/T - -√)O(1/T)的速率收敛到最优解。

未知的时间范围。无论何时我们想要对任何时间的TT有一个好的解决方案，我们可以选择:an =O(1/T−−√)这个费用我们额外的对数的因素,它会导致一个上界形式O (logT / T−−√)O (log⁡T / T)。


注意,强烈凸损失l (x, w)≥l (x, w) +⟨w”−w,∂王(x, w)⟩+λ2∥w−w '∥2 l (x, w)≥l (x, w) +⟨w”−w,∂王(x, w)⟩+λ2为w−w '为2我们可以设计更快速融合为一体的优化调度。事实上,一个指数衰减ηη导致绑定表单的O (logT / T) O (log⁡T / T)。


## 随机梯度和有限样本

到目前为止，讲到随机梯度下降时，我们讲得有点快也有点松。我们假设我们绘制实例xixi，通常从一些分布p(x,y)p(x,y)标记一一，并且我们使用它以某种方式更新权重ww。特别地，对于有限的样本容量，我们认为离散分布p(x,y)=1n∑ni=1 (x)∑yi(y)p(x,y)=1n∑i=1n (x)∑i=1n (x)∑yi(y)允许我们对其执行SGD。

然而，这并不是我们真正所做的。在本节的玩具示例中，我们只是在非随机梯度中添加了噪声，即，我们假设有对(xi,yi)(xi,yi)。事实证明，这在这里是合理的(详细讨论请参阅练习)。更令人不安的是，在以前的所有讨论中，我们显然没有这样做。相反，我们对所有实例只迭代一次。为了了解为什么这是可取的，考虑相反的情况，也就是说，我们从离散分布中采样nn的观察值。随机选择元素ii的概率是N−1N−1。因此选择它至少一次是可行的

(11.4.17)

P(选择i) = 1−P(忽略我)= 1−1−−1)N≈1 e−−1≈0.63。



类似的推理表明，精确抽取一次样本的概率为(N1)N−1(1−N−1)N−1=N−1N(1−N−1)N≈e−1≈0.37(N1)N−1(1−N−1)N−1=N−1N(1−N−1)N−1=N−1N(1−N−1)N≈e−1≈0.37。这导致了与不进行替换的抽样相比，方差的增加和数据效率的降低。因此，在实践中我们执行后者(这是贯穿全书的默认选择)。最后注意，重复通过数据集遍历它在一个不同的随机顺序。

## 总结

* 对于凸问题，我们可以证明在学习速率选择范围较广的情况下，随机梯度下降将收敛于最优解。
* 对于深度学习来说，通常不是这样的。然而，凸问题的分析给我们提供了如何接近优化的有用的见解，即逐步降低学习率，尽管不是太快。
* 当学习率太小或太大时，就会出现问题。在实践中，通常只有经过多次实验才能找到合适的学习率。
* 当训练数据集中的例子越多，计算梯度下降的每次迭代的代价就越大，因此在这些情况下，SGD是首选。
* 最优保障SGD一般没有凸情况下由于局部最小值的数量,需要检查可能是指数。

## 练习

采用不同的学习速率和不同的迭代次数对SGD进行实验。特别地，将距离最优解(0,0)(0,0)的距离绘制成迭代次数的函数。

证明对于函数f(x1,x2)=x21+2x22f(x1,x2)=x12+2x22，在梯度上添加正常噪声等价于最小化一个损失函数l(x,w)=(x1−w1)2+2(x2−w2)2l(x,w)=(x1−w1)2+2(x2−w2)2，其中xx是从正态分布中提取的。

推导xx分布的均值和方差。

显示这个属性拥有一般目标函数f (x) = 12 (x−μ)⊤Q (x−μ)f (x) = 12 (x−μ)⊤Q (x−μ)问⪰0问⪰0。

比较在{(x1,y1)，…，(xm,ym)}{(x1,y1)，…，(xm,ym)}中有替换采样和无替换采样时SGD的收敛性。

如果某个梯度(或与之相关的某个坐标)始终大于所有其他梯度，你将如何改变SGD求解器?

假设f (x) = x2 f (x) = (1 + sinx) x2 (1 + sin⁡x)。ff有多少局部最小值?你能改变ff使其最小化需要计算所有的局部最小值吗?
