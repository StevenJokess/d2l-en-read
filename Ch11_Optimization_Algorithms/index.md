

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-08-05 22:14:57
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-05 22:15:58
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/index.html
-->

# 优化算法

如果您按顺序阅读此书，那么您已经使用了许多先进的优化算法来训练深度学习模型。它们是工具，使我们能够继续更新模型参数和最小化损失函数的值，正如在训练集上评估的那样。事实上，任何人如果满足于把优化当作在简单环境中最小化目标函数的黑匣子装置，就很可能满足于知道存在这种程序的一系列咒语(如“ Adam”、“ NAG”或“ SGD”)。

然而，要做好这项工作，还需要一些更深入的知识。优化算法对深度学习很重要。一方面，训练一个复杂的深度学习模型可能需要几个小时，几天，甚至几个星期。优化算法的性能直接影响模型的训练效率。另一方面，理解不同优化算法的原理及其参数的作用将使我们能够有针对性地调整超参数，以提高深度学习模型的性能。

在本章中，我们深入探讨了常用的深度学习优化算法。几乎所有深度学习中的优化问题都是非凸的。尽管如此，凸问题背景下的算法设计和分析已被证明是非常有指导意义的。正是由于这个原因，这一部分包括了关于凸优化的入门，以及关于凸目标函数的一个非常简单的随机梯度下降算法的证明。
