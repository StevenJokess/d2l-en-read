

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 20:42:45
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-14 20:47:34
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/adadelta.html
 * https://zh.d2l.ai/chapter_optimization/adadelta.html
-->

# Adadelta

Adadelta 是 AdaGrad 的另一个变体。主要的区别在于它减少了学习速率适应坐标的数量。此外，传统上它被称为没有学习率，因为它使用变化量本身作为未来变化的校准。该算法是在[ Zeiler，2012]中提出的。考虑到迄今为止对先前算法的讨论，这是相当简单的。

## 算法

简而言之，Adadelta 使用两个状态变量，st 来存储梯度的第二阶矩的泄漏平均值，xt 来存储模型本身参数变化的第二阶矩的泄漏平均值。请注意，我们使用作者的原始符号和命名是为了与其他出版物和实现兼容(没有其他真正的理由为什么我们应该使用不同的希腊变量来表示一个参数，用于同样的目的，如 momentum，Adagrad，RMSProp 和 Adadelta)。参数 du jour 是。我们得到以下泄漏更新:

TODO:MATH

与以前不同的是，我们使用重新标度的梯度 g ′ t gt ′进行更新，这是用平均平方变化率和平均第二阶矩的比值计算出来的。G ′ t gt ′的使用纯粹是为了记号的方便。在实际应用中，我们可以实现该算法而不需要为 g ′ t ′ t ′使用额外的临时空间。和前面一样，这是一个确保非平凡数值结果的参数，即避免零步长或无限方差。通常我们将其设置为 = 10-5 = 10-5。

## 实施

Adadelta 需要为每个变量维护两个状态变量 stst 和 xtxt。

TODO:CODE

选择 = 0.9 = 0.9意味着每次参数更新的半衰期为10。这种方法往往效果很好。我们得到以下行为。

TODO:CODE

对于一个简洁的实现，我们只需要使用 Trainer 类中的 adelta 算法。对于更紧凑的调用，这将产生以下一行程序。

TODO:CODE

## 小结

* Adadelta没有学习率参数。相反，它使用参数本身的变化率来调整学习率。
* Adadelta需要两个状态变量来存储梯度的第二矩和参数的变化。
* Adadelta使用泄漏平均值来对适当的统计信息进行连续估算。

## 练习

1. 调整ρρ的值。怎么了？
1. 展示如何在不使用g'tgt'的情况下实现该算法。为什么这可能是个好主意？
1. Adadelta真的学习率免费吗？ 您能否找到破坏Adadelta的优化问题？
1. 将Adadelta与Adagrad和RMS prop进行比较，以讨论它们的收敛行为。
