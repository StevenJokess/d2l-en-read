

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 20:35:47
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-14 20:42:19
 * @Description:
 * @TODO::
 * @Reference:https://zh.d2l.ai/chapter_optimization/rmsprop.html
-->

# RMSProp

第11.7节中的一个关键问题是，学习率在有效的 o (t-12) o (t-12)的预定时间表下降。虽然这通常适用于凸问题，但对于非凸问题，比如深度学习中遇到的问题，这可能并不理想。然而，Adagrad 的协调适应性作为前置条件极为可取。

[ Tieleman & Hinton，2012]提出了 RMSProp 算法，作为解耦速率调度和协调自适应学习速率的简单方法。问题是 Adagrad 将梯度 gtgt 的平方累积成状态向量 stt = stt-1 + g2t st = st-1 + gt2。结果表明，由于缺乏标准化，算法的收敛速度呈线性增长，没有界限。

解决这个问题的方法之一是使用 stt/t。对于合理的 g t t 分布，这将收敛。不幸的是，这可能需要很长的时间，直到限制行为开始起作用，因为过程记住了值的全部轨迹。另一种方法是使用与动量方法相同的泄漏平均，即对某些参数 > 0 > 0使用 s ← s t-1 + (1 -) g2t st ← st-1 + (1 -) gt2。保持所有其他部分不变产生 RMSProp。

## 算法

让我们详细地写出这些方程式。

TODO:MATH

常数 > 0 > 0通常设置为10-610-6，以确保我们不会遭受零除法或步长过大的问题。考虑到这种扩展，我们现在可以自由地控制学习速率，而不受每个坐标应用的缩放的影响。对于泄漏的平均值，我们可以采用先前在动量法中使用的同样的推理。扩展科学技术产量的定义

TODO:MATH

和前面11.6节一样，我们使用1 + + 2 + ... ，= 11-1 + + 2 + ... ，= 11-。因此权重之和被归一化为11，其半衰期为观测值 -1-1。让我们想象一下过去40个时间/步骤的权重。

TODO:CODE

## 从零开始实现

如前所述，我们使用二次函数 f (x) = 0.1 x21 + 2 x2 f (x) = 0.1 x12 + 2 x22来观察 RMSProp 的轨迹。回想一下在11.7节中，当我们使用学习率为0.4的 Adagrad 时，由于学习率下降得太快，变量在算法的后期移动非常缓慢。由于是单独控制的，这不会发生在 RMSProp。

TODO:CODE

接下来，我们实现RMSProp以在深度网络中使用。 这同样简单。

TODO:CODE

我们将初始学习率设置为0.01，将加权项γγ设置为0.9。 即，在过去的1 /（1-γ）= 10个平方梯度观测值上，ss平均聚集。

TODO:CODE

## 简洁实现

由于RMSProp是一种非常流行的算法，因此在Trainer实例中也可用。 我们要做的就是使用一种名为rmsprop的算法实例化它，将γγ分配给参数gamma1。

TODO:CODE

## 小结

* RMSProp与Adagrad非常相似，因为两者均使用梯度的平方来缩放系数。
* RMSProp积极分享泄漏平均值。 但是，RMSProp使用该技术来调整系数方式的预处理器。
* 在实践中，学习者需要安排学习率。
* 系数γγ决定调整每个坐标比例时的历史长度。

## 练习

1. 在实验中，如果我们设=1，会发生什么呢?为什么?
1. 旋转优化问题以最小化f(x)=0.1(x1+x2)2+2(x1−x2)2f(x)=0.1(x1+x2)2+2(x1−x2)2。收敛会发生什么?
1. 在真正的机器学习问题上试试RMSProp会发生什么，比如Fashion-MNIST培训。尝试不同的选择来调整学习速率。
1. 您想要调整的优化过程中。RMSProp对此有多敏感?
