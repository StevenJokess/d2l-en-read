

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 11:47:05
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-14 12:18:38
 * @Description:translate
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1153/chapter_optimization/momentum.html
 * https://zh.d2l.ai/chapter_optimization/momentum.html
-->

# 动量

在11.4节中，我们回顾了在执行随机梯度下降时会发生什么，也就是说，当执行优化时，只有一个噪声变量的梯度是可用的。特别是，我们注意到，对于噪音渐变，在面对噪音时，我们需要格外小心地选择学习速率。如果我们减少得太快，收敛就会停滞。如果我们过于宽容，我们不能收敛到一个足够好的解决方案，因为噪音不断驱使我们远离优化。

## 基本知识

在这一节中，我们将探索更有效的优化算法，特别是在实践中常见的某些类型的优化问题。

### Leaky的平均值

上一节我们讨论了小批量 SGD 作为一种加速计算的方法。它还有一个很好的副作用，即平均梯度减少了方差的数量。小批量新增开支指数的计算方法如下:

TODO:MATH

为了保持简单的表示法，这里我们使用时间 t-1 t-1更新的权重作为样本 i 的 SGD，使用 h i，t-1 = av (xi，wt-1) hi，t-1 = x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x x。这将是很好的，如果我们可以受益于减少方差的效果，甚至超过平均梯度在一个小批量。完成这项任务的一个选择是用“泄漏平均值”取代梯度计算:

对于某些∈(0,1)∈(0,1)。这有效地替换了瞬时梯度，取而代之的是过去多个梯度的平均值。V v 叫做动量。它积累过去的梯度类似于一个重球滚动下来的目标函数景观集成过去的力量。为了更详细地了解正在发生的事情，让我们递归地扩展到

较大的数量为长期平均数，而较小的数量仅相对于梯度法略有修正。新的渐变替换不再指向特定实例上的梯度下降法方向，而是指向过去渐变的加权平均数方向。这使我们能够实现大部分的好处平均一批没有成本实际计算梯度上。我们将在稍后更详细地重新讨论这个平均化过程。

上述推理构成了现在所知的加速梯度方法的基础，例如带动量的梯度。他们享受的额外好处是在最佳化问题条件不好的情况下更有效率(例如，有些方向的进展比其他方向慢得多，就像一个狭窄的峡谷)。此外，他们允许我们平均后续梯度，以获得更稳定的下降方向。事实上，加速的方面，即使是无噪声的凸面问题，是一个关键的原因，为什么动量工作，为什么它工作得这么好。

正如人们所期望的那样，由于它的有效性，它成为深度学习和更深层次优化中一个被深入研究的课题。参见[ Goh，2017]的漂亮的说明文章，以获得深入的分析和互动的动画。它是由[波里亚克，1964]提出的。[ Nesterov，2018]在凸优化的背景下进行了详细的理论讨论。深度学习的动力在很长一段时间内都被认为是有益的。详情请参阅[ sutskiver et al. 2013]的讨论。

### 一个病态的问题

为了更好地理解动量方法的几何性质，我们重新回顾了梯度下降法方法，尽管它的目标函数明显不那么令人愉快。回想一下，在11.3节中，我们使用了 f (x) = x21 + 2 x2 f (x) = x12 + 2 x22，即一个中度扭曲的椭球物体。我们进一步扭曲这个函数，通过在 x 1 x 1方向上通过

如前所述，f 的最小值为(0,0)(0,0)。这个函数在 x 1 x 1方向上是非常平坦的。让我们看看当我们像以前一样在这个新函数上执行梯度下降法时会发生什么。我们选择0.40.4的学习率。

TODO:CODE

通过构造，x2x2方向上的坡度比水平x1x1方向上的坡度高得多，变化速度也快得多。 因此，我们陷入了两个不希望的选择之间：如果我们选择一个小的学习率，那么我们将确保解决方案在x2x2方向上不会发散，但在x1x1方向上收敛缓慢。 相反，在学习率较高的情况下，我们在x1x1方向上快速进步，但在x2x2上发散。 下面的示例说明了即使将学习率从0.40.4略微提高到0.60.6也会发生什么。 x1x1方向上的收敛性有所改善，但整体解决方案质量却差得多。

TODO:CODE

### 动量法

动量法使我们能够解决上述梯度下降问题。 查看上面的优化轨迹，我们可能会认为过去的平均梯度效果很好。 毕竟，在x1x1方向上，这将聚集对齐良好的渐变，从而增加了我们每一步覆盖的距离。 相反，在x2x2方向上，梯度会振荡，由于相互抵消的振荡，总梯度会减小步长。 使用vtvt代替梯度gtgt会产生以下更新方程式：

TODO:MATH

注意，对于β=0β= 0，我们恢复了规则的梯度下降。 在深入研究数学属性之前，让我们快速看一下该算法在实践中的行为。

TODO:CODE

我们可以看到，即使使用与以前相同的学习率，动量仍然收敛良好。 让我们看看当减小动量参数时会发生什么。 将其减半至β=0.25β= 0.25会导致轨迹几乎不会收敛。 但是，这比没有动力时要好得多（当解决方案分歧时）。

TODO:CODE

请注意，我们可以将动量与SGD结合使用，尤其是minibatch-SGD。 唯一的变化是，在这种情况下，我们将梯度gt，t-1gt，t-1替换为gtgt。 最后，为方便起见，我们在时间t = 0t = 0初始化v0 = 0v0 = 0。 让我们看一下泄漏平均对更新的实际作用。

### 有效样本权重

回想vt = ∑t-1τ =0βτgt-τ，t-τ-1vt= ∑τ =0t-1βτgt-τ，t-τ-1。 在极限中，这些项加起来等于∑∞τ =0βτ= 11-β∑τ =0∞βτ=11-β。 换句话说，不是在GD或SGD中采取ηη大小的步长，而是采取η1-βη1-β大小的步长，同时处理可能表现得更好的下降方向。 这些是合二为一的好处。 为了说明加权对于ββ的不同选择如何表现，请考虑下图。

TODO:CODE

## 实践经验

与（最小批量）SGD相比，动量法需要保持一组辅助变量，即速度。 它具有与梯度（以及优化问题的变量）相同的形状。 在下面的实现中，我们将这些变量称为状态。

TODO:CODE

让我们看看实践时是怎么工作的：

TODO:CODE

当我们将动量超参数动量增加到0.9时，它的有效样本大小明显更大，为11-0.9 = 1011-0.9 = 10。 我们将学习率略微降低至0.010.01，以控制情况。

TODO:CODE

降低学习率可进一步解决任何非平滑优化问题。 将其设置为0.0050.005会产生良好的会聚特性。

TODO:CODE

###

由于标准的sgd求解器已经内置了动量，因此在Gluon中几乎不需要做任何事情。设置匹配参数会产生非常相似的轨迹。

TODO:CODE


### 标量函数

给定以上结果，让我们看看将函数f（x）=λ2x2f（x）=λ2x2最小化时会发生什么。 对于梯度下降，我们有

TODO:MATH

无论何时|1−至至的生活方式|<1|1−至至的生活方式|<1，该优化以指数速度收敛，因为在tt步骤后，我们有xt=(1−至至的生活方式)tx0xt=(1−至至的生活方式)tx0。这表明，当我们将学习率不断提高，收敛速度在一开始是如何提高的。除此之外，还有一些问题是不同的，对于一些复杂的问题>2，复杂的问题>2，优化问题也是不同的。

TODO:CODE


为了分析动量情况下的收敛性，我们首先以两个标量来重写更新方程：一个为xx，一个为动量vv。 这样产生：

TODO:MATH

我们用RR表示2×22×2控制收敛行为。 在tt步之后，初始选择[v0，x0] [v0，x0]变为R（β，η，λ）t [v0，x0] R（β，η，λ）t [v0，x0]。 因此，取决于RR的特征值来确定收敛速度。 有关出色的动画，请参见[Goh，2017]的Distill帖子，有关详细分析，请参见[Flammarion＆Bach，2015]。 可以证明0 <ηλ<2 +2β0<ηλ<2 +2β动量收敛。 当与0 <ηλ<20 <ηλ<2进行梯度下降相比时，这是更大范围的可行参数。 这也表明，一般而言，ββ的值较大是合乎需要的。 进一步的细节需要大量的技术细节，我们建议感兴趣的读者咨询原始出版物。

## 小结

* 让我们看看动量在实践中是如何工作的，即在适当的优化程序中使用动量时。 为此，我们需要更具可扩展性
实现。
* 动量用过去的梯度的泄漏平均值代替梯度。 这大大加快了收敛速度。
* 对于无噪声梯度下降和（噪声）随机梯度下降都是理想的。
* 动量可防止优化过程的停顿，这种停顿在随机梯度下降中更容易发生。
* 由于过去数据的指数权重下降，有效梯度数由11-β11-β给出。
* 在凸二次问题的情况下，可以对此进行详细的详细分析。
* 实现非常简单，但是需要我们存储一个额外的状态向量（momentum vv）

## 练习

1. 使用动量超参数和学习率的其他组合，观察和分析不同的实验结果。
2. 对于具有多个特征值（即f（x）= 12∑iλix2if（x）= 12∑iλixi2）（例如λi=2-iλi= 2-i）的二次问题，请尝试GD和动量。 绘制初始化xi = 1xi = 1时xx的值如何减小。
3. 推导h（x）=12x⊤Qx+x⊤c+ bh（x）=12x⊤Qx+x⊤c+ b的最小值和极小值。
4. 当我们有动力执行SGD时会有什么变化？ 当我们使用带有动量的小批量SGD时会发生什么？ 试验参数？
