

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-05 23:21:08
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-06 12:11:12
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/minibatch-sgd.html
-->

# 小批量随机梯度下降

到目前为止，我们在基于梯度的学习方法中遇到了两个极端:第11.3节使用完整的数据集来计算梯度和更新参数，一次一遍。相反，第11.4节一次处理一个观察结果以取得进展。每一种都有其缺点。当数据非常相似时，梯度下降不是特别有效的数据。随机梯度下降不是特别的计算效率，因为cpu和gpu不能利用矢量化的全部力量。这意味着可能会有一个中庸之道，事实上，这就是我们到目前为止在讨论的例子中所使用的。

## 向量化和缓存

决定使用小批的核心是计算效率。在考虑对多个gpu和多个服务器进行并行时，这是最容易理解的。在这种情况下，我们需要发送至少一个图像到每个GPU。每台服务器有8个gpu和16台服务器，我们已经达到了128个小批量。

当涉及到单个gpu甚至cpu时，事情就有点微妙了。这些设备有多种类型的存储器，通常是多种类型的计算单元和它们之间不同的带宽限制。例如，一个CPU有少量的寄存器，然后是L1、L2，在某些情况下甚至还有L3缓存(在不同的处理器内核之间共享)。这些缓存的大小和延迟都在增加(同时它们的带宽在减少)。可以这样说，处理器能够执行比主存储器接口能够提供的更多的操作。

2 ghz CPU 16芯和avx - 512向量化可以处理多达109⋅⋅16⋅32 = 10122⋅109⋅16⋅32 = 1012字节每秒。gpu的能力很容易超过这个数字的100倍。另一方面,中端服务器处理器可能没有比100 GB / s带宽,也就是,不到十分之一的被要求保持处理器美联储。更糟的是,并不是所有的内存访问都是平等的:首先,内存接口通常是64位宽或更广泛的(例如,在gpu上384位),因此阅读单个字节会带来更大的成本的访问。

第一次访问的开销很大，而顺序访问相对便宜(通常称为突发读取)。还有很多事情需要记住，比如当我们有多个套接字、chiplets和其他结构时进行缓存。对此的详细讨论超出了本节的范围。请参阅维基百科上的这篇文章以获得更深入的讨论。

缓解这些限制的方法是使用一个CPU缓存的层次结构，这些缓存的速度实际上足以为处理器提供数据。这就是深度学习批量化背后的驱动力。为了简单起见，考虑矩阵-矩阵乘法，比如A=BCA=BC。我们有许多选择来计算AA。例如，我们可以尝试以下方法:

我们可以计算Aij = Bi: C⊤:jAij = Bi: C: j⊤,例如,我们可以计算它element-wise通过点产品。

我们可以计算:j = BC⊤:,j =公元前:jA: j⊤,例如,我们可以计算这一列。同样地，我们可以一次计算一行Ai, Ai，:。

我们可以简单地计算A=BCA=BC。

我们可以把BB和CC分解成更小的块矩阵，然后一次计算一个块。

如果我们遵循第一个选项，那么每次要计算元素AijAij时，我们都需要将一行和一个列向量复制到CPU中。更糟糕的是，由于矩阵元素是按顺序排列的，因此当我们从内存中读取这两个向量时，我们需要访问其中一个向量的许多不相交的位置。第二种选择更有利。在其中，当我们继续遍历BB时，我们可以将列向量C:，jC:，j保存在CPU缓存中。这将使内存带宽需求减半，并相应地使访问速度更快。当然，选项3是最理想的。不幸的是，大多数矩阵可能不完全适合缓存(这是我们要讨论的毕竟)。然而，选项4提供了一个实际有用的替代方案:我们可以将矩阵块移动到缓存中，并在本地乘它们。优化的库为我们解决了这个问题。让我们看看这些操作在实践中有多有效。

除了计算效率之外，Python和深度学习框架本身带来的开销也是相当大的。回想一下，每次我们执行一个命令时，Python解释器都会向MXNet引擎发送一个命令，该引擎需要将命令插入计算图并在调度过程中处理它。这样的管理费用可能相当有害。简而言之，尽可能使用向量化(和矩阵)是非常明智的。

TODO:CODE

Element-wise作业简单地遍历所有行和列分别为BB和CC分配至AA的价值。

TODO:CODE

一个更快的策略是执行按列分配。

TODO:CODE

最后，最有效的方式是在一个块中执行整个操作。让我们看看各自的运算速度是多少。

TODO:CODE

## 小批次

在过去，我们想当然地认为我们会读取小批数据而不是单个观测数据来更新参数。现在我们给它一个简单的理由。处理单个观察需要我们执行许多单个矩阵-向量(甚至向量-向量)乘法，这是相当昂贵的，并且对底层的深度学习框架造成了很大的开销。这既适用于评估网络时应用于数据(通常称为推断)，也适用于计算梯度来更新参数。也就是说，这适用于我们执行w←w−通知单tgtw←w−通知单tgt的地方

TODO:CODE

我们可以通过一次将其应用于一小批观测数据来提高此操作的计算效率。也就是说，我们将单个观测的梯度gtgt替换为小批观测的梯度gtgt

TODO:CODE

让我们看看这对gtgt的统计属性有什么影响:因为xtxt和minibatch BtBt的所有元素都是从训练集统一随机抽取的，所以梯度的期望保持不变。另一方面，方差显著减少。由于小批量梯度由b:=|Bt|b:=|Bt|的独立梯度进行平均，其标准差被降低一个因子b−12b−12。这本身是一件好事，因为它意味着更新更可靠地与全梯度对齐。

天真地这将表明,选择一个大minibatch BtBt是普遍的。唉，在某个点之后，与计算成本的线性增加相比，标准偏差的额外减少是最小的。在实践中，我们选择一个足够大的小批，以提供良好的计算效率，同时仍然适合GPU的内存。为了说明储蓄让我们看一看一些代码。在其中，我们执行相同的矩阵-矩阵乘法，但这次分解为每次64列的“小批量”。

TODO:CODE

正如我们所看到的，在minibatch上的计算基本上和在全矩阵上一样高效。需要提醒的是。在第7.5节中，我们使用了一种严重依赖于小批量中方差量的正则化。当我们增大后者时，方差会减小，而批量归一化带来的噪声注入的好处也会随之减小。例如，[Ioffe, 2017]了解如何重新分配和计算适当的条款的细节。

## 读取数据集

让我们看看如何有效地从数据生成小批量。下面我们使用美国宇航局开发的数据集来测试不同飞机的机翼噪音，以比较这些优化算法。为了方便起见，我们只使用前1500个示例。数据被白化以进行预处理，也就是说，我们移除平均值并将每个坐标的方差缩放到11。

## 从头开始实现

回顾3.2节中的小批量SGD实现。在下面，我们将提供一个更通用的实现。为了方便调用签名相同的其他优化算法在本章后面介绍。具体来说，我们添加状态输入`states`，并将超参数放入字典` hyperparams`中。此外，在训练函数中我们将平均每个minibatch示例的损失，所以优化算法中的梯度不需要除以batch size。

TODO:CODE

接下来，我们实现了一个通用的训练函数，以方便使用本章后面介绍的其他优化算法。该模型初始化一个线性回归模型，并可通过随后引入的小批量SGD和其他算法对模型进行训练。

TODO:CODE

让我们看看如何优化批量梯度下降。这可以通过将minibatch大小设置为1500(即示例总数)来实现。因此，模型参数每历元只更新一次。几乎没有进展。事实上，六步后进展停滞。

TODO:CODE

当批大小为1时，我们使用SGD进行优化。为了简化实现，我们选择了一个常数(尽管很小)学习率。在SGD中，每当处理一个示例时，模型参数都会更新。在我们的例子中这相当于1500每个时代更新。我们可以看到，目标函数值的下降速度在一个历期之后减缓。虽然这两种方法在一个epoch内处理1500个样本，但在我们的实验中，SGD比梯度下降消耗更多的时间。这是因为SGD更新参数的频率更高，同时处理单个观测数据的效率更低。

TODO:CODE

最后，当批量大小为100时，我们使用小批量SGD进行优化。每个历元所需的时间比SGD和批量梯度下降所需的时间短。

TODO:CODE

将批处理大小减少到10，每个epoch的时间都会增加，因为每个批处理的工作负载执行效率较低。

TODO:CODE

最后，我们比较了四个实验的时间和损失。可以看出，尽管SGD在处理的例子数量上比GD收敛得更快，但它比GD使用更多的时间来达到相同的损失，因为逐例计算梯度的效率不高。小批量SGD能够兼顾收敛速度和计算效率。小批量10比SGD效率更高;在运行时方面，100的小批处理甚至优于GD。

TODO:CODE

## 简洁的实现

在胶子中，我们可以使用Trainer类来调用优化算法。它用于实现一个通用的训练功能。我们将在本章中使用它。

TODO:CODE

用Gluon重复最后一个实验，表现出相同的行为。

TODO:CODE

## 总结

* 向量化使代码更高效，这是由于降低了深度学习框架带来的开销，也由于cpu和gpu上更好的内存局部性和缓存。
* 在SGD产生的统计效率和一次处理大量数据产生的计算效率之间存在权衡。
* 小批量随机梯度下降提供了最好的两个世界:计算和统计效率。
* 在minibatch SGD中，我们通过训练数据的随机排列来处理成批的数据(即每个历元只处理一次观察，尽管顺序是随机的)。
* 建议在训练过程中降低学习率。
* 一般来说，当以时钟时间来衡量时，迷你批SGD比SGD更快，梯度下降收敛的风险更小。

## 练习

1. 修改批量大小和学习率，观察目标函数值的递减率和每个历元消耗的时间。
1. 阅读MXNet文档并使用培训器类set_learning_rate函数在每次历元后将迷你批SGD的学习率降低到它之前值的1/10。
1. 将小批量SGD与实际上从训练集进行替换采样的变体进行比较，会发生什么?
1. 一个邪恶的精灵在没有告诉你的情况下复制了你的数据集(也就是说，每次观察都会发生两次，你的数据集会增长到原来的两倍，但是没有人告诉你)。SGD、小批量SGD和梯度下降的行为有何变化?
