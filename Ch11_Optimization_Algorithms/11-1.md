

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-03 15:24:55
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-03 16:22:56
 * @Description:translate
 * @TODO::MATH
 * @Reference:https://zh.d2l.ai/chapter_optimization/optimization-intro.html
 * http://preview.d2l.ai/d2l-en/PR-1102/chapter_optimization/optimization-intro.html
-->

# 优化与深度学习

在本节中，我们将讨论优化和深度学习之间的关系，以及在深度学习中使用优化所面临的挑战。对于一个深度学习问题，我们通常首先定义一个损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尽量减少损失。在优化中，损失函数通常被称为优化问题的目标函数。按照传统和惯例，大多数优化算法都与最小化有关。如果我们需要最大化一个目标，有一个简单的解决办法:翻转这个函数的正负号。

TODO:CODE

下图更详细地说明了这个问题。由于我们只有有限的数据量，所以训练误差的最小值可能与期望误差(或测试误差)的最小值在不同的位置。

TODO:CODE

# 深度学习中的优化挑战

在本章中，我们将特别关注优化算法在最小化目标函数方面的性能，而不是模型s泛化误差。在[3.1节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_linear-networks/linear-regression.html#sec-linear-regression)中，我们区分了优化问题的解析解和数值解。在深度学习中，大多数目标函数是复杂的，没有解析解。相反，我们必须使用数值优化算法。下面的优化算法都属于这一类。

深度学习优化存在许多挑战。一些最麻烦的是局部极小值，鞍点和消失梯度。让我们来看看其中的一些。

## 局部最小值

对于目标函数 f(x) ，如果 f(x) 在 x 上的值比在 x 邻近的其他点的值更小，那么 f(x) 可能是一个局部最小值（local minimum）。如果 f(x) 在 x 上的值是目标函数在整个定义域上的最小值，那么 f(x) 是全局最小值（global minimum）。

举个例子，给定函数

f(x)=x⋅cos(πx),−1.0≤x≤2.0,

我们可以大致找出该函数的局部最小值和全局最小值的位置。需要注意的是，图中箭头所指示的只是大致位置。

深度学习模型的目标函数通常有很多局部最优解。当一个优化问题的数值解在局部最优解附近时，由于目标函数有关解的梯度接近或变成0，最终迭代求得的数值解可能只令目标函数局部最小化而非全局最小化。只有某种程度的噪声才能使参数偏离局部最小值。事实上，这是随机梯度下降的一个有利特性，其中梯度在小批量上的自然变化能够从局部极小值中去除参数。

## 优化和评估

虽然优化为深度学习提供了一种最小化损失函数的方法，但本质上，优化和深度学习的目标是完全不同的。前者主要是最小化一个目标，而后者则是在有限的数据量下找到一个合适的模型。在[4.4节](http://preview.d2l.ai/d2l-en/PR-1102/chapter_multilayer-perceptrons/underfit-overfit.html#sec-model-selection)中，我们详细讨论了这两个目标之间的区别。例如，训练误差和泛化误差通常是不同的:由于优化算法的目标函数通常是一个基于训练数据集的损失函数然而，统计推断(以及深度学习)的目标是减少泛化误差。要实现后者，除了使用优化算法减少训练误差外，还需要注意过拟合。我们首先导入几个带有函数的库，以便在图中进行注释。

TODO:CODE

下图更详细地说明了这个问题。由于我们只有有限的数据量，所以训练误差的最小值可能与期望误差(或测试误差)的最小值在不同的位置。

对于目标函数 f(x) ，如果 f(x) 在xx处的值小于在xx附近任何其他点的值，则 f(x) 可以是局部最小值。如果f(x)在xx处的值是整个域内目标函数的最小值，则 f(x) 是全局最小值。

我们可以逼近该函数的局部最小值和全局最小值。

TODO:CODE

### 鞍点

除了局部极小值，鞍点是梯度消失的另一个原因。[鞍点（saddle point）](https://zh.wikipedia.org/wiki/Saddle_point)是函数的所有梯度消失但既不是全局最小值也不是局部最小值的任何位置。考虑函数 f(x)=x^3 。它的一阶导数和二阶导数在 x=0 时消失了。优化可能会在这一点上停止，即使它不是最小值。

TODO:MATH

我们可以逼近该函数的局部最小值和全局最小值。

在更高维度中的鞍点甚至更加隐蔽，如下面的例子所示。考虑函数 $f(x,y)=x^2-y^2$。鞍点在 (0,0) 这是对y的最大值，对x的最小值。而且，它看起来像一个马鞍，这就是这个数学性质得名的原因。

TODO:CODE

假设一个函数的输入为 k 维向量，输出为标量，那么它的海森矩阵（Hessian matrix）有 k 个特征值（参见附录中“[数学基础](http://preview.d2l.ai/d2l-en/PR-1102/chapter_appendix-mathematics-for-deep-learning/geometry-linear-algebraic-ops.html#sec-geometry-linear-algebraic-ops)”一节）。该函数在梯度为0的位置上可能是局部最小值、局部最大值或者鞍点：

- 当函数的海森矩阵在梯度为0的位置上的特征值全为正时，该函数得到局部最小值。
- 当函数的海森矩阵在梯度为0的位置上的特征值全为负时，该函数得到局部最大值。
- 当函数的海森矩阵在梯度为0的位置上的特征值有正有负时，该函数得到鞍点。

对于高维问题，至少某些特征值是负的可能性相当高。这使得鞍点比局部极小点更有可能出现。我们将在下一节介绍凸性时讨论这种情况的一些例外情况。简而言之，凸函数就是那些Hessian的特征值不为负的函数。遗憾的是，大多数深度学习问题并不属于这一类。尽管如此，它是一个伟大的工具来研究优化算法。

## 梯度消失

可能最隐晦的问题是梯度的消失。例如，假设我们想最小化函数f(x)=tanh(x)我们从x=4开始。我们可以看到，f的梯度接近于nil。具体来说，f'(x)=1-tanh^2(x)，因此f^'(4)=0.0013。因此，在我们取得进展之前，优化将停滞很长一段时间。这就是为什么在引入ReLU激活函数之前，训练深度学习模型是相当棘手的原因之一。

TODO:CODE

正如我们所见，深度学习的优化充满了挑战。幸运的是，有一个健壮的算法范围，表现良好，易于使用，即使是初学者。此外，真的没有必要去寻找最好的解决方案。局部最优解甚至近似解仍然是非常有用的。

## 小结

- 最小化训练误差并不能保证我们找到最小化期望误差的最佳参数集。
- 优化问题可能有许多局部极小值。
- 这个问题可能有更多的鞍点，因为通常这些问题不是凸的。消失梯度会导致优化失速。通常问题的重新参数化会有所帮助。
- 良好的参数初始化也是有益的。

## 练习

1. 考虑一个简单的多层感知器，它只有一个隐含层，比如隐含层中有 $d$ 维，并且只有一个输出。证明对于任何局部最小值都至少有$d!$ 行为相同的等价解。
1. 假设我们有一个对称随机矩阵MM，其中条目$M_ij=M_ji$都是从某个概率分布$p_ij$中抽取的。进一步假设pij(x)=pij(x)，即分布是对称的(例如，[Wigner, 1958])。证明特征值的分布也是对称的。即，对于任意特征向量vv，关联特征值的特征元个数的特征元个数的特征元个数的特征元个数的个数的特征元个数的个数的个数的个数的个数的个数的个数的个数的个数的个数的个数。为什么上面的不意味着P(生长率>)=0.5P(生长率>0)=0.5
1. 对于深度学习中的优化问题，你还能想到哪些其他的挑战？
1. 假设你想在（真的）马鞍上平衡一个（真实的）球。为什么这么难?你能利用这种效应来优化算法吗


