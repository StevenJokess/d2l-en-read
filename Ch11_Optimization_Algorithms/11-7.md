

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-14 20:18:06
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-27 19:41:27
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_optimization/adagrad.html
 * https://zh.d2l.ai/chapter_optimization/adagrad.html
-->

# Adagrad[1]

即 adaptive gradient ，自适应梯度法。

它通过记录每次迭代过程中的前进方向和距离，从而使得针对不同问题，有一套自适应调整学习率的方法

$$
\alpha=\frac{1}{\sqrt{\sum_{i=1}^{n-1} g_{i}+\epsilon}} \alpha_{0}
$$

优点：解决了 SGD 中学习率不能自适应调整的问题 缺点：学习率单调递减，在迭代后期可能导致学习率变得特别小而导致收敛及其缓慢。同样的，我们还需要手动设置初始 α。

首先，让我们考虑一下那些不常出现的特征的学习问题。

## 稀疏特性和学习速度

假设我们正在训练一个语言模型。为了获得良好的准确性，我们通常希望降低学习率，因为我们继续训练，通常在一个 o (t-12) o (t-12)或更慢的速度。现在考虑一个关于稀疏特性的模型训练，即只是偶尔发生的特性。这在自然语言中很常见，例如，我们看到预处理这个词的可能性比看到学习这个词的可能性小得多。然而，它在其他领域也很常见，比如计算机广告和个性化协同过滤。毕竟，有很多东西只有少数人感兴趣。

只有当这些特性出现时，与不常见特性关联的参数才会接收有意义的更新。如果学习速度降低，我们可能会陷入这样的情况，即公共特征的参数很快收敛到它们的最佳值，而对于不常见的特征，我们仍然缺乏足够频繁的观察，才能确定它们的最佳值。换句话说，对于频繁的特性来说，学习速度降低得太快，对于不频繁的特性来说，学习速度降低得太慢。

解决这个问题的一个可能的方法是计算我们看到某个特定功能的次数，并用它作为调整学习速度的时钟。也就是说，我们可以使用 i = 0 s (i，t) + c √ i = 0 s (i，t) + c (i，t) + c。这里 s (i，t) s (i，t)计算到 t 为止我们观察到的特征 i 的非零个数。这实际上很容易实现，而且没有任何有意义的开销。然而，当我们没有稀疏而只是数据时，它就失败了，因为梯度通常很小，很少很大。毕竟，我们还不清楚界定某物是否属于观察到的特征。

Adagrad 由[ Duchi 等人，2011]解决了这个问题，用先前观察到的梯度的平方的聚合代替了相当粗糙的计数器 s (i，t) s (i，t)。特别地，它使用 s (i，t + 1) = s (i，t) + (s-i-f (x))2 s (i，t + 1) = s (i，t) + (s-x-if (x))2作为调整学习率的方法。这有两个好处: 第一，我们不再需要决定什么时候梯度足够大。其次，它会根据梯度的大小自动扩展。经常对应于大梯度的坐标被显著缩小，而其他具有小梯度的坐标则得到了更温和的处理。在实践中，这导致了一个非常有效的计算广告和相关问题的优化程序。但这掩盖了 Adagrad 固有的一些额外优势，而这些优势最好在预条件的背景下理解。

## 预处理

凸优化问题有助于分析算法的特点。毕竟，对于大多数非凸问题来说，很难得到有意义的理论保证，但是直觉和洞察力往往会继续存在。我们研究极小化问题 f (x) = 12 x Qx + c something x + b f (x) = 12x something Qx + c something x + b。

正如我们在11.6节中看到的，可以根据其特征分解 q = u something u q = u something u 重写这个问题，从而得到一个非常简单的问题，其中每个坐标都可以单独求解:

TODO:MATH

这里我们使用 x = Ux x = Ux，因此 c = Uc = Uc。修正后的问题有极小值 x =-1 c x =-1 c 和极小值 -12 c something-1 c + b-12 c something-1 c + b。这是非常容易计算，因为是一个对角矩阵包含 q 的特征值。

如果我们稍微扰动 c，我们将希望在 f 的极小值中找到只有微小的变化。不幸的是，事实并非如此。虽然 c c 中的细微变化同样会导致 c c 中的细微变化，但 f f (分别是 f 的极小值)的情况并非如此。当本征值 i 很大的时候，我们只能看到 x i 和 f 的最小值有很小的变化。相反，对于小的 i i，x i x i 的变化可能是戏剧性的。最大特征值和最小特征值之间的比值称为最佳化问题的条件数。

如果条件数很大，就很难准确地求解最佳化问题。我们需要确保在获得正确的大动态范围值时非常小心。我们的分析引出了一个显而易见的，虽然有点幼稚的问题: 难道我们不能通过扭曲空间使所有的特征值都是11来“解决”问题吗。理论上这很简单: 我们只需要 q 的特征矢量就可以将问题从 x 变为 z 中的1: = 12 Ux z: = 12 Ux x。在新的坐标系中，x something Qx something Qx 可以简化为2‖ z ‖2。唉，这是一个相当不切实际的建议。一般来说，计算特征矢量比解决实际问题要昂贵得多。

虽然计算特征值可能很昂贵，但是猜测特征值并且近似地计算它们可能已经比什么都不做要好得多。特别是，我们可以使用 q q 的对角线条目并相应地重新赋值。这比计算特征值要便宜得多。

TODO:MATH

在这种情况下，我们有 q ~ ij = q ij/q ii q jj-----√ q ~ ij = qij/qiqjj，特别是 q ~ ii = 1 q ~ ii = 1。在大多数情况下，这大大简化了条件数。例如，我们前面讨论过的情况，这将完全消除手头的问题，因为问题是轴对齐的。

不幸的是，我们面临另一个问题：在深度学习中，我们通常甚至无法访问目标函数的二阶导数：对于x∈Rdx∈Rd，即使在小批量上，二阶导数也可能需要O（d2）O（d2）空间 并努力进行计算，因此实际上不可行。阿达格勒（Adagrad）的巧妙想法是为黑森州的那条难以捉摸的对角线使用代理，该对角线计算起来相对便宜且有效-梯度本身的大小。

为了了解为什么这样做，让我们看看f¯(x¯)f¯(x¯)。我们有

(11.7.4)

∂x¯f¯(x¯) =Λx¯+ c¯=Λ(x¯−x¯0),

其中x¯0x¯0是f¯f的极小值。因此梯度的大小都取决于ΛΛ和距离最优。如果x¯−x¯0x¯−x¯0不变，这就是所需的全部。毕竟，在这种情况下，梯度的大小∂x f¯(x¯)∂x¯f¯(x¯)就足够了。由于AdaGrad是一种随机梯度下降算法，即使在最优状态下，我们也会看到非零方差的梯度。因此，我们可以安全地使用梯度的方差作为黑森规模的廉价代理。全面的分析超出了本节的范围(可能需要几页)。详情请参阅[Duchi et al.， 2011]。

## 算法

让我们从上面把讨论正式化。我们使用变量stst来累积过去的梯度方差，如下所示。

TODO:MATH

这里的操作是在坐标上应用的。也就是说，v2v2有项v2ivi2。同样1 v√1 v条目1 vi√1 vi和u⋅vu⋅v uiviuivi条目。ηη之前学习速率和ϵϵ是一种确保我们不加常数除以00。最后，初始化s0=0s0=0。

就像在动量的情况下我们需要跟踪一个辅助变量，在这种情况下允许每个坐标有一个单独的学习速率。这并不会显著增加Adagrad相对于SGD的成本，因为主要成本通常是计算l(yt,f(xt,w))l(yt,f(xt,w))及其衍生物。

注意，在stst中累积平方梯度意味着stst基本上以线性速率增长（实际上比线性增长慢一些，因为梯度最初会减小）。这导致O（t-12）O（t-12）学习速率，尽管是在每个坐标的基础上进行调整的。对于凸问题，这是完全足够的。但是，在深度学习中，我们可能希望更缓慢地降低学习速度。这导致了许多Adagrad变体，我们将在后续章节中进行讨论。现在让我们看看它在二次凸问题中的表现。我们使用与以前相同的问题：

TODO:MATH

我们将使用以前相同的学习率（即η=0.4η= 0.4）实现Adagrad。如我们所见，自变量的迭代轨迹更平滑。但是，由于stst的累积作用，学习率不断下降，因此自变量在迭代的后期不会移动太多。

TODO:CODE

随着我们将学习率提高到22，我们看到了更好的行为。这已经表明，即使在无噪声的情况下，学习率的降低也可能是相当大的，我们需要确保参数正确收敛。

TODO:CODE

## 从零开始实现

同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。我们根据AdaGrad算法中的公式实现该算法。

TODO:CODE

与第11.5节中的实验相比，我们使用更高的学习率来训练模型。

TODO:CODE

## 简洁实现

通过名称为“adagrad”的Trainer实例，我们便可使用Gluon提供的AdaGrad算法来训练模型。

TODO:CODE

## 小结

* Adagrad在每个坐标的基础上动态地降低学习率。
* 它使用梯度的大小作为一种手段来调整如何快速取得进展-坐标与大梯度是补偿较小的学习率。
* 由于记忆和计算的限制，计算精确的二阶导数在深度学习问题中通常是不可行的。梯度可以是一个有用的代理。
* 如果优化问题具有相当不均匀的结构，Adagrad可以帮助减轻失真。
* Adagrad对于稀疏特性特别有效，因为对于不经常出现的术语，学习率需要降低得更慢。
* 在深度学习问题上，Adagrad有时在降低学习率方面过于激进。我们将在第11.10节中讨论减轻这种情况的策略。

## 练习

1. 证明对于正交矩阵UU和向量cc成立：∥c-δ∥2= ∥Uc-Uδ∥2′c-δ′2 =′Uc-Uδ′2。为什么这意味着在变量正交变化后摄动幅度不会改变？
1. 对于f（x）= 0.1x21 + 2x22f（x）= 0.1x12 + 2x22尝试Adagrad，并且将目标函数旋转45度，即f（x）= 0.1（x1 + x2）2 + 2（ x1-x2）2f（x）= 0.1（x1 + x2）2 + 2（x1-x2）2。它的行为是否不同？
1. 证明Gerschgorin的圆定理，该定理指出矩阵MM的特征值λiλi满足|λi-Mjj| ≤∑k≠j | Mjk ||λi-Mjj| ≤∑k≠j | Mjk | 对于jj的至少一种选择。
1. Gerschgorin定理告诉我们关于对角预处理矩阵diag−12（M）Mdiag−12（M）diag−12（M）Mdiag−12（M）的特征值？
1. 试用Adagrad以获得适当的深度网络，例如将其应用于Fashion MNIST时，请参见第6.6节。
1. 您将如何修改Adagrad以降低学习速度的下降？

[1]:  [《Adaptive subgradient methods for online learning and stochastic optimization》 2011](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
More:

