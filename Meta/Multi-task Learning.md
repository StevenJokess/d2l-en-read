

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-14 22:51:45
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 17:45:19
 * @Description:
 * @TODO::
 * @Reference:
-->

# å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-task Learningï¼‰

## What is a task? (more formally this time)

A task: $\mathscr{T}_{i} \triangleq\left\{p_{i}(\mathbf{x}), p_{i}(\mathbf{y} \mid \mathbf{x}), \mathscr{L}_{i}\right\}$
data generating distributions
Corresponding datasets: $\quad \mathscr{D}_{i}^{t r} \quad \mathscr{D}_{i}^{\text {est}}$
will use $\mathscr{D}_{i}$ as shorthand for $\mathscr{D}_{i}^{t r}$ :[2]

å¦‚æœæœ‰ä¸¤ä¸ªä»»åŠ¡æ¯”è¾ƒç›¸å…³ï¼Œå®ƒä»¬ä¹‹é—´ä¼šå­˜åœ¨ä¸€å®šçš„å…±äº«çŸ¥è¯†ï¼Œè¿™äº›çŸ¥è¯†å¯¹ä¸¤ä¸ªä»»åŠ¡éƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼è¿™äº›å…±äº«çš„çŸ¥è¯†å¯ä»¥æ˜¯è¡¨ç¤ºï¼ˆç‰¹å¾ï¼‰ã€æ¨¡å‹å‚æ•°æˆ–å­¦ä¹ ç®—æ³•ç­‰ï¼ç›®å‰ï¼Œä¸»æµçš„å¤šä»»åŠ¡å­¦ä¹ æ–¹æ³•ä¸»è¦å…³æ³¨è¡¨ç¤ºå±‚é¢çš„å…±äº«ï¼




å¤šä»»åŠ¡å­¦ä¹ ï¼ˆMulti-task Learningï¼‰æ˜¯æŒ‡åŒæ—¶å­¦ä¹ å¤šä¸ªç›¸å…³ä»»åŠ¡ï¼Œè®©è¿™äº›ä»»åŠ¡åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å…±äº«çŸ¥è¯†ï¼Œåˆ©ç”¨å¤šä¸ªä»»åŠ¡ä¹‹é—´çš„ç›¸å…³æ€§æ¥æ”¹è¿›æ¨¡å‹åœ¨æ¯ä¸ªä»»åŠ¡ä¸Šçš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ï¼å¤šä»»åŠ¡å­¦ä¹ å¯ä»¥çœ‹ä½œä¸€ç§å½’çº³è¿ç§»å­¦ä¹ ï¼ˆInductiveTransfer Learningï¼‰ï¼Œå³é€šè¿‡åˆ©ç”¨åŒ…å«åœ¨ç›¸å…³ä»»åŠ¡ä¸­çš„ä¿¡æ¯ä½œä¸ºå½’çº³åç½®ï¼ˆIn-ductive Biasï¼‰æ¥æé«˜æ³›åŒ–èƒ½åŠ›[Caruana,1997]

\text { Vanilla MTL Objective: } \min _{\theta} \sum_{i=1}^{T} \mathscr{L}_{i}\left(\theta, \mathscr{D}_{i}\right)[2]

Basic Version:
1. Sample mini-batch of tasks $\mathscr{B} \sim\left\{\mathscr{T}_{i}\right\}$
2. Sample mini-batch datapoints for each task $\mathscr{D}_{i}^{b} \sim \mathscr{D}_{i}$
3. Compute loss on the mini-batch: $\hat{\mathscr{L}}(\theta, \mathscr{B})=\sum_{\mathscr{T}_{k} \in \mathscr{B}} \mathscr{L}_{k}\left(\theta, \mathscr{D}_{k}^{b}\right)$
4. Backpropagate loss to compute gradient $\nabla_{\theta} \hat{\mathscr{L}}$
5. Apply gradient with your favorite neural net optimizer (e.g. Adam)
Note: This ensures that tasks are sampled uniformly, regardless of data quantities.
Tip: For regression problems, make sure your task labels are on the same scale!


Multitask Learning (MTL) is an inductive transfer mechanism whose principle goal is to improve generalization performance. MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks. It does this by training tasks in parallel while using a shared representation. In effect, the training signals for the extra tasks serve as an inductive bias.[2]

å…±äº«æœºåˆ¶å¤šä»»åŠ¡å­¦ä¹ çš„ä¸»è¦æŒ‘æˆ˜åœ¨äºå¦‚ä½•è®¾è®¡å¤šä»»åŠ¡ä¹‹é—´çš„å…±äº«æœºåˆ¶ï¼åœ¨ä¼ ç»Ÿçš„æœºå™¨å­¦ä¹ ç®—æ³•ä¸­ï¼Œå¼•å…¥å…±äº«çš„ä¿¡æ¯æ˜¯æ¯”è¾ƒå›°éš¾çš„ï¼Œé€šå¸¸ä¼šå¯¼è‡´æ¨¡å‹å˜å¾—å¤æ‚ï¼ä½†æ˜¯åœ¨ç¥ç»ç½‘ç»œæ¨¡å‹ä¸­ï¼Œæ¨¡å‹å…±äº«å˜å¾—ç›¸å¯¹æ¯”è¾ƒå®¹æ˜“ï¼æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹æä¾›äº†ä¸€ç§å¾ˆæ–¹ä¾¿çš„ä¿¡æ¯å…±äº«æ–¹å¼ï¼Œå¯ä»¥å¾ˆå®¹æ˜“åœ°è¿›è¡Œå¤šä»»åŠ¡å­¦ä¹ ï¼å¤šä»»åŠ¡å­¦ä¹ çš„å…±äº«æœºåˆ¶æ¯”è¾ƒçµæ´»ï¼Œæœ‰å¾ˆå¤šç§å…±äº«æ¨¡å¼ï¼å›¾10.1ç»™å‡ºäº†å¤šä»»åŠ¡å­¦ä¹ ä¸­å››ç§å¸¸è§çš„å…±äº«æ¨¡å¼ï¼Œå…¶ä¸­ğ´ã€ğµå’Œğ¶è¡¨ç¤ºä¸‰ä¸ªä¸åŒçš„ä»»åŠ¡ï¼Œçº¢è‰²æ¡†è¡¨ç¤ºå…±äº«æ¨¡å—ï¼Œè“è‰²æ¡†è¡¨ç¤ºä»»åŠ¡ç‰¹å®šæ¨¡å—ï¼è¿™å››ç§å¸¸è§çš„å…±äº«æ¨¡å¼åˆ†åˆ«ä¸ºï¼š
ï¼ˆ1ï¼‰ç¡¬å…±äº«æ¨¡å¼ï¼šè®©ä¸åŒä»»åŠ¡çš„ç¥ç»ç½‘ç»œæ¨¡å‹å…±åŒä½¿ç”¨ä¸€äº›å…±äº«æ¨¡å—ï¼ˆä¸€èˆ¬æ˜¯ä½å±‚ï¼‰æ¥æå–ä¸€äº›é€šç”¨ç‰¹å¾ï¼Œç„¶åå†é’ˆå¯¹æ¯ä¸ªä¸åŒçš„ä»»åŠ¡è®¾ç½®ä¸€äº›ç§æœ‰æ¨¡å—ï¼ˆä¸€èˆ¬æ˜¯é«˜å±‚ï¼‰æ¥æå–ä¸€äº›ä»»åŠ¡ç‰¹å®šçš„ç‰¹å¾ï¼
ï¼ˆ2ï¼‰è½¯å…±äº«æ¨¡å¼ï¼šä¸æ˜¾å¼åœ°è®¾ç½®å…±äº«æ¨¡å—ï¼Œä½†æ¯ä¸ªä»»åŠ¡éƒ½å¯ä»¥ä»å…¶ä»–ä»»åŠ¡ä¸­â€œçªƒå–â€ä¸€äº›ä¿¡æ¯æ¥æé«˜è‡ªå·±çš„èƒ½åŠ›ï¼çªƒå–çš„æ–¹å¼åŒ…æ‹¬ç›´æ¥å¤åˆ¶ä½¿ç”¨å…¶ä»–ä»»åŠ¡çš„éšçŠ¶æ€ï¼Œæˆ–ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥ä¸»åŠ¨é€‰å–æœ‰ç”¨çš„ä¿¡æ¯ï¼
ï¼ˆ3ï¼‰å±‚æ¬¡å…±äº«æ¨¡å¼ï¼šä¸€èˆ¬ç¥ç»ç½‘ç»œä¸­ä¸åŒå±‚æŠ½å–çš„ç‰¹å¾ç±»å‹ä¸åŒï¼Œä½å±‚ä¸€èˆ¬æŠ½å–ä¸€äº›ä½çº§çš„å±€éƒ¨ç‰¹å¾ï¼Œé«˜å±‚æŠ½å–ä¸€äº›é«˜çº§çš„æŠ½è±¡è¯­ä¹‰ç‰¹å¾ï¼å› æ­¤å¦‚æœå¤šä»»åŠ¡å­¦ä¹ ä¸­ä¸åŒä»»åŠ¡ä¹Ÿæœ‰çº§åˆ«é«˜ä½ä¹‹åˆ†ï¼Œé‚£ä¹ˆä¸€ä¸ªåˆç†çš„å…±äº«æ¨¡å¼æ˜¯è®©ä½çº§ä»»åŠ¡åœ¨ä½å±‚è¾“å‡ºï¼Œé«˜çº§ä»»åŠ¡åœ¨é«˜å±‚è¾“å‡ºï¼
ï¼ˆ4ï¼‰å…±äº«-ç§æœ‰æ¨¡å¼ï¼šä¸€ä¸ªæ›´åŠ åˆ†å·¥æ˜ç¡®çš„æ–¹å¼æ˜¯å°†å…±äº«æ¨¡å—å’Œä»»åŠ¡ç‰¹å®šï¼ˆç§æœ‰ï¼‰æ¨¡å—çš„è´£ä»»åˆ†å¼€ï¼å…±äº«æ¨¡å—æ•æ‰ä¸€äº›è·¨ä»»åŠ¡çš„å…±äº«ç‰¹å¾ï¼Œè€Œç§æœ‰æ¨¡å—åªæ•æ‰å’Œç‰¹å®šä»»åŠ¡ç›¸å…³çš„ç‰¹å¾ï¼æœ€ç»ˆçš„è¡¨ç¤ºç”±å…±äº«ç‰¹å¾å’Œç§æœ‰ç‰¹å¾å…±åŒæ„æˆï¼


An Alternative View on the Multi-Task Architecture
Split $\theta$ into share parameters $s^{s h}$ and task-specifÃ­c parameters $\theta^{i}$
$$
\text { Then, our objective is: } \min _{\theta^{\prime \prime}, g^{\prime}, \ldots, \theta^{\top}} \sum_{i=1}^{T} \mathscr{L}_{i}\left(\left\{\theta^{s h}, \theta^{i}\right\}, \mathscr{D}_{i}\right)
$$
Choosing how to condition on $\boldsymbol{z}_{i}$ equivalent to share parameters

å¤šä»»åŠ¡å­¦ä¹ é€šå¸¸å¯ä»¥è·å¾—æ¯”å•ä»»åŠ¡å­¦ä¹ æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªåŸå› ï¼š
ï¼ˆ1ï¼‰å¤šä»»åŠ¡å­¦ä¹ åœ¨å¤šä¸ªä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæ¯”å•ä»»åŠ¡å­¦ä¹ çš„è®­ç»ƒé›†æ›´å¤§ï¼ç”±äºå¤šä¸ªä»»åŠ¡ä¹‹é—´æœ‰ä¸€å®šçš„ç›¸å…³æ€§ï¼Œå› æ­¤å¤šä»»åŠ¡å­¦ä¹ ç›¸å½“äºæ˜¯ä¸€ç§éšå¼çš„æ•°æ®å¢å¼ºï¼Œå¯ä»¥æé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼
ï¼ˆ2ï¼‰å¤šä»»åŠ¡å­¦ä¹ ä¸­çš„å…±äº«æ¨¡å—éœ€è¦å…¼é¡¾æ‰€æœ‰ä»»åŠ¡ï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šé¿å…äº†æ¨¡å‹è¿‡æ‹Ÿåˆåˆ°å•ä¸ªä»»åŠ¡çš„è®­ç»ƒé›†ï¼Œå¯ä»¥çœ‹ä½œä¸€ç§æ­£åˆ™åŒ–ï¼
ï¼ˆ3ï¼‰æ—¢ç„¶ä¸€ä¸ªå¥½çš„è¡¨ç¤ºé€šå¸¸éœ€è¦é€‚ç”¨äºå¤šä¸ªä¸åŒä»»åŠ¡ï¼Œå‚è§ç¬¬1.3èŠ‚ï¼å¤šä»»åŠ¡å­¦ä¹ çš„æœºåˆ¶ä½¿å¾—å®ƒä¼šæ¯”å•ä»»åŠ¡å­¦ä¹ è·å¾—æ›´å¥½çš„è¡¨ç¤ºï¼
ï¼ˆ4ï¼‰åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½å¯ä»¥â€œé€‰æ‹©æ€§â€åˆ©ç”¨å…¶ä»–ä»»åŠ¡ä¸­å­¦ä¹ åˆ°çš„éšè—ç‰¹å¾ï¼Œä»è€Œæé«˜è‡ªèº«çš„èƒ½åŠ›

```python
# https://gist.github.com/dominique003/679e641210b9aade06b513b7b6750746/raw/976c370a9c789bfa8927560157b57b813411c86e/MTCNNExtractFaces.py
# extract and plot each detected face in a photograph
from matplotlib import pyplot
from mtcnn.mtcnn import MTCNN

# draw each face separately
def draw_faces(filename, result_list):
	# load the image
	data = pyplot.imread(filename)
	# plot each face as a subplot
	for i in range(len(result_list)):
		# get coordinates
		x1, y1, width, height = result_list[i]['box']
		x2, y2 = x1 + width, y1 + height
		# define subplot
		pyplot.subplot(1, len(result_list), i+1)
		pyplot.axis('off')
		# plot face
		pyplot.imshow(data[y1:y2, x1:x2])
	# show the plot
	pyplot.show()

filename = 'poupees.jpg'
# load image from file
pixels = pyplot.imread(filename)
# create the detector, using default weights
detector = MTCNN()
# detect faces in the image
faces = detector.detect_faces(pixels)
# display faces on the original image
draw_faces(filename, faces)
```

[1]: Caruana R, 1997. Multi-task learning[J]. Machine Learning, 28(1):41-75.
TODO:http://questioneurope.blogspot.com/2020/07/running-mtcnn-on-my-own-photos.html
[2]: https://cs330.stanford.edu/slides/cs330_intro.pdf
