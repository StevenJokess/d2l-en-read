

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-08-09 20:32:22
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-09 21:54:28
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_introduction/index.html#tab-intro-decade
-->

# 引言

直到最近，我们每天接触的几乎每一个计算机程序都是由软件开发人员根据第一原则编写的。假设我们想编写一个管理电子商务平台的应用程序。在一块白板上呆了几个小时思考这个问题之后，我们会想出一个可行的解决方案的大致轮廓，可能看起来是这样的: (i)用户通过运行在 web 浏览器或移动应用程序中的界面与应用程序交互; (ii)我们的应用程序与商业级数据库引擎交互，以跟踪每个用户的状态并保存历史交易记录; (iii)在我们的应用程序的核心，应用程序的业务逻辑(你可能会说，大脑)详细阐述了我们的程序在任何可想到的情况下应该采取的适当行动。

为了构建应用程序的大脑，我们必须逐步处理我们预期会遇到的每一个可能的角落情况，设计适当的规则。每次客户单击向其购物车添加项时，我们都会向购物车数据库表添加一个条目，将该用户的 ID 与请求的产品的 ID 关联起来。虽然很少有开发人员第一次就完全正确(可能需要一些测试运行来解决这些问题) ，但在大多数情况下，我们可以根据第一原则编写这样的程序，并在看到真正的客户之前自信地启动它。我们能够根据第一原则设计自动化系统，驱动功能性产品和系统，通常是在新的情况下，这是一个了不起的认知壮举。当你能够设计出100%的时间都能工作的解决方案时，你不应该使用机器学习。

幸运的是，对于不断壮大的机器学习(ML)科学家群体来说，我们希望自动化的许多任务并不会如此轻易地屈从于人类的聪明才智。想象一下你和你所知道的最聪明的人挤在白板前，但是这一次你正在解决以下问题之一:

* 编写一个程序，根据地理信息、卫星图像和过去天气的跟踪窗口来预测明天的天气。
* 编写一个程序，用自由格式的文本输入一个问题，然后正确地回答它。
* 编写一个程序，给出一个图像，可以识别它所包含的所有人，并绘制每个人的轮廓。
* 编写一个程序，向用户展示他们可能喜欢但在浏览过程中不太可能遇到的产品。

在每一种情况下，即使是精英程序员也无法从头开始编写解决方案。造成这种情况的原因各不相同。有时候，我们正在寻找的程序遵循一个随着时间变化的模式，我们需要我们的程序来适应。在其他情况下，这种关系(比如像素和抽象类别之间的关系)可能过于复杂，需要成千上万个超出我们意识理解范围的计算(即使我们的眼睛可以毫不费力地完成任务)。机器学习是研究可以从经验中学习的强大技术。随着机器学习算法积累更多的经验，通常以观察数据或与环境交互的形式，其性能得到提高。与我们的确定性电子商务平台相比，我们的电子商务平台根据相同的业务逻辑运行，不管积累了多少经验，直到开发人员自己了解并决定是时候更新软件了。在这本书中，我们将教你机器学习的基础知识，特别关注深度学习，这是一套强大的技术，推动了计算机视觉、自然语言处理、医疗保健和基因组学等领域的创新。

## 一个激励人心的例子

在我们开始写作之前，这本书的作者，像许多工作人员一样，必须变得兴奋起来。我们跳进汽车，开始驾驶。亚历克斯用 iPhone 大喊“嘿，Siri” ，唤醒了手机的语音识别系统。然后，穆命令“去蓝瓶咖啡店的路”。电话很快显示出他命令的抄写。它还意识到我们在询问方向，并启动了地图应用程序以满足我们的请求。一旦启动，地图应用程序确定了一些路线。在每条路线旁边，手机显示预测的运输时间。虽然我们编造这个故事是为了教学上的便利，但它表明，在短短几秒钟内，我们与智能手机的日常交互可以使用几种机器学习模型。

想象一下，只要编写一个程序，对“ Alexa”、“ Okay，Google”或“ Siri”这样的唤醒词做出响应。如图1.1.1所示，尝试在一个房间里用一台电脑和一个代码编辑器自己编写代码。你将如何从基本原理出发编写这样一个程序？想想吧... 这个问题很难。每一秒钟，麦克风将收集大约44,000个样本。每个采样都是对声波振幅的测量。什么规则可以可靠地从一个原始音频片段映射到关于该片段是否包含唤醒词的可靠预测{是，否} ？如果你被困住了，不要担心。我们也不知道如何从头开始编写这样的程序。这就是为什么我们使用机器学习。

图1.1.1识别一个唤醒词。

诀窍是这样的。通常，即使我们不知道如何明确地告诉计算机如何从输入映射到输出，我们仍然能够自己完成认知壮举。换句话说，即使你不知道如何编程计算机来识别单词“ Alexa” ，你自己也能够识别单词“ Alexa”。有了这个能力，我们可以收集一个包含音频例子的巨大数据集，并标记那些包含或不包含唤醒词的数据集。在机器学习方法中，我们并不试图设计一个系统来显式识别唤醒词。相反，我们定义了一个灵活的程序，其行为由许多参数决定。然后，我们使用数据集来确定可能的最佳参数集，这些参数可以提高我们的程序在感兴趣的任务的性能方面的性能。

你可以把这些参数看作我们可以转动的旋钮，操纵程序的行为。为了确定参数，我们将程序称为模型。我们仅通过操作参数就可以生成的所有不同的程序(输入-输出映射)集合称为模型族。使用我们的数据集来选择参数的元程序叫做学习算法。

在我们开始学习算法之前，我们必须精确地定义问题，确定输入和输出的确切性质，并选择一个合适的模型族。在本例中，我们的模型接收音频片段作为输入，并在{yes, no}中生成一个选择作为输出。如果一切都按照计划进行，那么对于代码片段是否包含唤醒词，模型的猜测通常是正确的。

如果我们选择了正确的模型系列，那么就应该有一个旋钮设置，这样模型每次听到单词“Alexa”就会发出“是”。因为wake这个词的准确选择是随意的，我们可能需要一个足够富有的模范家庭，通过另一个旋按钮，只有在听到“杏”这个词时，它才能发出“是”的声音。我们期望相同的模型家庭应该适合“Alexa”识别和“杏”识别，因为它们看起来，直观上，是类似的任务。然而，如果我们想要处理根本不同的输入或输出，比如我们想要从图像映射到字幕，或者从英语句子映射到汉语句子，我们可能需要完全不同的模型族。

如果我们选择了正确的模型系列，那么就应该有一个旋钮设置，这样模型每次听到单词“Alexa”就会发出“是”。因为wake这个词的准确选择是随意的，我们可能需要一个足够富有的模范家庭，通过另一个旋按钮，只有在听到“杏”这个词时，它才能发出“是”的声音。我们期望相同的模型家庭应该适合“Alexa”识别和“杏”识别，因为它们看起来，直观上，是类似的任务。然而，如果我们想要处理根本不同的输入或输出，比如我们想要从图像映射到字幕，或者从英语句子映射到汉语句子，我们可能需要完全不同的模型族。

你可能会猜到，如果我们只是随机设置所有的按钮，我们的模型不太可能识别“Alexa”、“杏”或任何其他英语单词。在深度学习中，学习是一个过程，通过这个过程，我们发现正确设置的旋钮，强制我们的模型的期望行为。

如图1.1.2所示，训练过程通常是这样的:

1. 从一个随机初始化的模型开始，它不能做任何有用的事情。
2. 获取一些您的标记数据(例如，音频片段和相应的{yes, no}标签)
3. 调整一下旋钮，让模型相对于那些例子不那么糟糕
4. 重复这个过程，直到这个模型非常棒。

图1.1.2一个典型的训练过程。

总而言之，我们不是编写唤醒词识别器，而是编写一个程序，如果我们给它一个大的标记数据集，它就能学会识别唤醒词。您可以将这种决定程序行为的行为看作是使用数据集来编写程序。我们可以通过给我们的机器学习系统提供许多猫和狗的例子来“编程”一个猫探测器，例如下面的图片:

## 关键组件：数据，模型和算法

在我们的唤醒词示例中，我们描述了一个由音频片段和二进制标签组成的数据集，并对如何训练模型以逼近片段到分类的映射给出了手工的理解。这种问题，在给定已知输入的情况下，我们尝试预测指定的未知标签，在由示例组成的数据集中，已知标签的情况称为监督学习，它只是多种机器学习问题之一。在下一节中，我们将深入探讨不同的ML问题。首先，无论我们遇到哪种ML问题，我们都希望更多地了解将跟随我们的一些核心组件：

1. 我们可以学习的数据。
1. 有关如何转换数据的模型。
1. 损失函数可量化模型的缺陷。
1. 调整模型参数以最小化损失的算法。

### 数据

不用说，没有数据就无法进行数据科学。我们可能会丢失数百页来仔细考虑数据的确切构成，但是现在，我们将在实际方面犯错，并专注于要关注的关键属性。通常，我们关注的是一系列示例。为了有用地处理数据，我们通常需要提出一个合适的数字表示形式。每个示例通常都包含一组称为要素的数字属性。在上述监督学习问题中，将特殊功能指定为预测目标（有时称为标签或因变量）。然后可以将模型必须根据其进行预测的给定特征简单地称为特征（或通常称为输入，协变量或自变量）。

如果我们使用图像数据，则每张照片都可以构成一个示例，每张照片都由与每个像素的亮度相对应的数字值的有序列表表示。一张200×200彩色照片将由200×200×3 = 120000数值组成，分别对应于每个空间位置的红色，绿色和蓝色通道的亮度。在更传统的任务中，考虑到年龄，生命体征，诊断等一系列标准功能，我们可能会尝试预测患者是否能够存活。

当每个示例都用相同数量的数值表征时，我们说数据由固定长度的向量组成，我们将向量的（恒定）长度描述为数据的维数。就像您想象的那样，固定长度是一种方便的属性。如果我们想训练一个模型以在显微镜图像中识别癌症，那么固定长度的输入意味着我们不必担心什么。

但是，并非所有数据都可以轻松地表示为固定长度的向量。虽然我们可能希望显微镜图像来自标准设备，但我们不能期望从Internet上采集的图像都能以相同的分辨率或形状显示。对于图像，我们可能会考虑将它们全部裁剪为标准尺寸，但是这种策略仅能使我们走到现在。我们可能会丢失裁剪部分中的信息。而且，文本数据甚至更顽固地抵抗定长表示。考虑一下在亚马逊，IMDB或TripAdvisor之类的电子商务网站上留下的客户评论。有些简短：“很臭！”。其他人则四处逛逛。相对于传统方法，深度学习的主要优势是现代模型可以处理可变长度数据的比较宽限期。

通常，我们拥有的数据越多，我们的工作就越容易。当我们拥有更多数据时，我们可以训练功能更强大的模型，并减少对预想假设的依赖。从（相对）小数据到大数据的体制转变是现代深度学习成功的主要因素。为了说明这一点，如果没有大型数据集，深度学习中许多最令人兴奋的模型就无法工作。其他一些工作在低数据机制下，但并不比传统方法更好。

最后，仅拥有大量数据并对其进行巧妙处理是不够的。我们需要正确的数据。如果数据充满错误，或者所选功能不能预测目标目标数量，学习将失败。这种陈词滥调很好地反映了这种情况：垃圾进，垃圾出。而且，不良的预测性能不是唯一的潜在后果。在机器学习的敏感应用中，例如预测性策略，简历筛选和用于贷款的风险模型，我们必须特别警惕垃圾数据的后果。一种常见的故障模式发生在数据集中，其中训练数据中未包含某些人。想象一下，在野外使用从未见过黑皮肤的皮肤癌识别系统。如果数据不仅代表了某些群体，而且反映了社会偏见，那么失败也可能发生。例如，如果过去的招聘决定用于训练将用于筛选简历的预测模型，则机器学习模型可能会无意间捕获并自动处理历史不公。请注意，这可能在没有数据科学家积极合谋甚至意识不到的情况下发生。

### 模型

在某种意义上，大多数机器学习都涉及数据转换。我们可能想要建立一个系统，通过摄取照片来预测笑脸程度。或者，我们可能想摄入一组传感器读数，并预测读数是正常的还是异常的。通过模型，我们表示了用于摄取一种类型的数据，并输出可能不同类型的预测的计算机制。我们特别感兴趣的是可以从数据中估计的统计模型。虽然简单模型完全能够解决适当的简单问题，但我们在这本书中关注的问题扩展了经典方法的极限。深度学习与经典方法的区别主要在于它所关注的一组强大的模型。这些模型由许多连续的数据转换组成，这些数据从上到下链接在一起，因此被称为深度学习。在我们讨论深层神经网络的过程中，我们将讨论一些更传统的方法。

### 目标功能

之前，我们将机器学习引入为“从经验中学习”。通过在这里学习，我们的意思是随着时间的推移改进某些任务。但是谁能说什么构成了进步呢？ 您可能会想到我们可以建议更新我们的模型，而有些人可能对建议的更新是改善还是减少持不同意见。

为了开发学习机器的形式化数学系统，我们需要对模型的好坏进行形式化度量。在机器学习和更广泛的优化中，我们称这些目标函数。按照惯例，我们通常定义目标函数，以便越低越好。这仅仅是一个约定。您可以通过设置f'=-ff'=-f来选择较高的函数ff，然后将其转换为质量上相同但较低的函数f'f'。因为越低越好，所以这些函数有时称为损失函数或成本函数。

当试图预测数值时，最常见的目标函数是平方误差（y-y ^）2（y-y ^）2。对于分类，最常见的目标是最小化错误率，即我们的预测与基本事实不一致的实例所占的比例。一些目标（如平方误差）易于优化。其他（如错误率）由于不可微或其他复杂性而难以直接优化。在这些情况下，通常最优化替代目标。

通常，损失函数是根据模型的参数定义的，并取决于数据集。通过最大程度地减少训练集（包括一些用于训练的示例）组成的训练集，可以学习出模型参数的最佳值。但是，在训练数据上做得不好不能保证我们会在（看不见的）测试数据上做得很好。因此，我们通常希望将可用数据分为两个部分：训练数据（用于拟合模型参数）和测试数据（为评估而保留），报告以下两个数量：

* 训练错误：训练模型所依据的数据上的错误。您可以认为这就像是学生在练习考试中用来准备一些真实考试的分数。即使结果令人鼓舞，也不能保证期末考试的成功。
* 测试错误：这是由于看不见的测试集而引起的错误。这可能会明显偏离训练错误。当模型在训练数据上表现良好，但无法将其推广到看不见的数据时，我们说它是过拟合的。从现实生活的角度来看，这就像在实践考试中表现出色之外，还是使实际考试不及格。

## 机器学习的种类

在下面的部分中，我们将更详细地讨论几种机器学习问题。我们从一个目标列表开始，即。，列出了我们希望机器学习去做的事情。请注意，目标是由一组如何完成它们的技术补充的，包括数据类型、模型、培训技术等。下面的列表只是ML可以解决的一些问题的样本，这些问题可以激励读者，并为我们在本书中讨论更多问题时提供一些通用语言。

### 监督式学习

监督学习的任务是预测给定输入的目标。我们通常称目标为标签，一般用y表示，输入数据也称为特征或协变量，一般用xx表示。每个(输入、目标)对称为一个示例或实例。有时，在上下文清楚的情况下，我们可以使用术语示例来表示输入的集合，即使对应的目标是未知的。我们用下标表示任何特定的实例，通常是ii，例如(xi,yi,yi)。数据集是nn实例{xi,yi}ni=1{xi,yi}i=1n的集合。我们的目标是建立一个模型，将任何输入映射到一个预测。

以一个具体的例子为基础进行描述，如果我们在医疗行业工作，那么我们可能想要预测病人是否会心脏病发作。这个观察结果，心脏病发作或无心脏病发作，将成为我们的标签yy。输入数据xx可能是心率、舒张压、收缩压等生命体征。

监督之所以起作用，是因为我们(监督者)为模型提供了一个数据集，其中包含标记的示例(xi,yixi,yi)，其中每个示例xi都与正确的标签匹配。

在概率方面，我们通常感兴趣的是估计条件概率P(y|x)。虽然监督学习只是机器学习的几个范例之一，但它在工业上机器学习的大部分成功应用都是由监督学习完成的。这在一定程度上是因为，许多重要的任务可以简单地描述为给定一组特定的可用数据来估计未知事件发生的概率:

* 根据CT图像预测癌症和非癌症。
* 给出一个英语句子，预测正确的法语翻译。
* 根据本月的财务报告数据预测股票下个月的价格。

即使简单描述“根据输入预测目标”，受监督的学习也可以采用多种形式，并且需要进行大量的建模决策，这取决于（除其他因素外）输入，输出的类型，大小以及数量。例如，我们使用不同的模型来处理序列（例如文本字符串或时间序列数据）并处理固定长度的矢量表示形式。在本书的前9部分中，我们将深入探讨许多这些问题。

非正式地，学习过程看起来像这样：获取大量示例，这些示例的协变量是已知的，并从中选择一个随机子集，为每个变量获取基本真相标签。有时这些标签可能是已经收集的可用数据（例如，患者是否在次年内死亡？），而有时我们可能需要聘请人工注释者来标记数据（例如，将图像分配给类别）。

这些输入和相应的标签一起构成了训练集。我们将训练数据集输入到监督学习算法中，该函数将数据集作为输入并输出另一个函数，即学习模型。最后，我们可以将以前看不见的输入馈送到学习的模型中，并将其输出用作相应标签的预测。整个过程如图1.3.1所示。

图1.3.1监督学习。

### 回归

可能最简单的监督学习任务就是回归。例如，考虑从房屋销售数据库中收集的一组数据。我们可以构建一个表，其中每一行对应于不同的房屋，每一列对应于一些相关属性，例如房屋的平方英尺，卧室数，浴室数和分钟数（步行 ）到市中心。在此数据集中，每个示例将是一栋特定的房子，而对应的特征向量将是表中的一行。

如果您居住在纽约或旧金山，并且您不是Amazon，Google，Microsoft或Facebook的首席执行官，则（房屋面积，卧室数量，浴室数量，步行距离）特征向量 可能类似于：[100,0，.5,60]。但是，如果您住在匹兹堡，它可能看起来更像[3000,4,3,10]。像这样的特征向量对于大多数经典的机器学习算法都是必不可少的。我们将继续将与任何示例ii对应的特征向量表示为xixi，并且可以将包含所有特征向量的完整表紧凑地称为XX。

导致问题回归的实际上是输出。假设您正在寻找新房。鉴于这些功能，您可能需要估算房屋的公平市场价值。目标价格（销售价格）是一个实数。如果您还记得关于实物的正式定义，那么您现在可能会抓狂。房屋可能永远不会以不到一分钱的价格出售，更不用说以非理性数字表示的价格了。在这样的情况下，当目标实际上是离散的，但是在四舍五入的范围内进行舍入时，我们将稍微滥用语言，并继续将输出和目标描述为实数值。

我们表示任意单个目标依依(对应例xi)和所有目标集合yy(对应例XX)。当我们的目标在某个范围内取任意值时，我们称之为回归问题。我们的目标是产生一个模型，其预测非常接近实际的目标值。我们表示任何实例的预测目标y^iy^i。如果这个符号让您陷入困境，请不要担心。我们将在后面的章节中更彻底地展开它。

许多实际问题都被很好地描述为回归问题。预测用户将分配给一部电影的评级可以被认为是一个回归问题，如果你在2009年设计了一个出色的算法来完成这一壮举，你可能会赢得100万美元的Netflix大奖。预测患者住院时间也是一个回归问题。一个好的经验法则是多少?还是多少?问题应该建议回归。

当我们有超过两个可能的类时，我们称之为多类分类问题。常见的例子包括手写字符识别[0,1,2,3…9, a, b, c，…]当我们通过最小化L1L1或L2L2损失函数来解决回归问题时，分类问题的常见损失函数称为交叉熵。

注意，最可能的类不一定是您将用于决策的类。假设你在后院发现了这个美丽的蘑菇，如图1.3.2所示。

图1.3.2死亡帽（Death Cap）-别吃！

现在，假设您建立了一个分类器并对其进行了训练，以根据照片预测蘑菇是否有毒。假设我们的毒物检测分类器输出P（y = deathcap | image）= 0.2P（y = deathcap | image）= 0.2。换句话说，分类器确保我们的蘑菇不是死亡上限的80％80％。但是，您仍然必须是一个傻瓜才能吃它。那是因为美味晚餐的某些好处不值得20％到20％的死亡风险。换句话说，不确定风险的影响远远超过了收益。我们可以更正式地看待这一点。基本上，我们需要计算产生的预期风险，即我们需要将结果的概率乘以与之相关的收益（或损害）：

TODO:MATH

吃这个蘑菇的损失是L(a=吃|x)=0.2∗∞+0.8∗0=∞，而丢弃它的成本是L(a=丢弃|x)=0.2∗0+0.8∗1=0.8

我们的谨慎是有道理的：正如任何真菌学家告诉我们的那样，上述蘑菇实际上是一个死亡上限。分类不仅比二进制，多类甚至是多标签分类复杂得多。例如，存在一些用于处理层次结构的分类变体。层次结构假定许多类之间存在某些关系。因此，并非所有错误都相等-如果我们必须犯错，我们宁愿将其错误分类为相关类别，而不是较远的类别。通常，这称为层次分类。一个早期的例子是林奈 Linnaeus 将动物按层次组织的。

在动物分类的情况下，将贵宾犬误认为是雪纳瑞犬可能并不那么糟糕，但是如果我们的模型将贵宾犬混淆为恐龙，我们的模型将付出巨大的代价。哪个层次结构相关可能取决于您计划如何使用模型。例如，在系统发育树上可能靠近拨浪鼓蛇和吊袜带蛇，但是误以为吊袜带则是致命的。

### 标记

有些分类问题不能很好地适合于二进制或多类分类设置。例如，我们可以训练一个普通的二进制分类器来区分猫和狗。在计算机视觉的当前状态下，我们可以使用现成的工具轻松地做到这一点。尽管如此，无论我们的模型多么精确，当分类器遇到不来梅市音乐家的形象时，我们可能都会遇到麻烦。

图1.3.3一只猫、一只公鸡、一只狗和一只驴

如你所见，画中有一只猫，还有一只公鸡、一只狗、一头驴和一只鸟，背景是几棵树。这取决于我们最终想对我们的模型做什么，把它当作一个二元分类问题可能没有多大意义。相反，我们可能想给模型一个选项，说图像描绘了一只猫、一只狗、一头驴、一只公鸡和一只鸟。

学习预测非互斥类的问题称为多标签分类。自动标记问题通常被描述为多标签分类问题。想想人们可能在科技博客上发布的帖子上使用的标签，比如“机器学习”、“技术”、“小工具”、“编程语言”、“linux”、“云计算”、“AWS”。一篇典型的文章可能会应用5-10个标签，因为这些概念是相互关联的。关于“云计算”的帖子可能会提到“AWS”，而关于“机器学习”的帖子也可能涉及“编程语言”。

在处理生物医学文献时，我们也必须处理这类问题，因为正确地标注文章是很重要的，因为它允许研究者对文献做详尽的评论。在国家医学图书馆，许多专业的注释者会仔细检查每一篇被PubMed索引的文章，将其与MeSH中的相关术语关联起来。MeSH是一个大约有28k个标签的集合。这是一个耗时的过程，注释器在归档和标记之间通常有一年的延迟。这里可以使用机器学习来提供临时标记，直到每篇文章都可以进行适当的手动检查。事实上，多年来，BioASQ组织举办了一场比赛，就是为了做到这一点。

### 搜索和排名

有时我们不只是希望将每个示例分配给存储桶或实际值。在信息检索领域，我们希望对一组项目进行排名。以网络搜索为例，其目标不是确定特定页面是否与查询相关，而是确定过多搜索结果中的哪一个与特定用户最相关。我们确实关心相关搜索结果的排序，我们的学习算法需要从较大的集合中生成元素的有序子集。换句话说，如果要求我们从字母表中产生前5个字母，则返回A B C D E和C A B E D之间会有区别。即使结果集相同，集合中的顺序也很重要。

解决此问题的一种可能方法是，首先为集合中的每个元素分配一个相应的相关性分数，然后检索评分最高的元素。PageRank是Google搜索引擎背后的原始秘密之处，是这种评分系统的早期示例，但它的独特之处在于它不依赖于实际的查询。在这里，他们依靠一个简单的相关性过滤器来识别相关项的集合，然后依靠PageRank对包含查询词的结果进行排序。如今，搜索引擎使用机器学习和行为模型来获取与查询相关的相关性分数。有整个学术会议专门讨论这个问题。

### 推荐系统

推荐系统是与搜索和排名有关的另一个问题设置。就目标是向用户显示一组相关项目而言，问题是相似的。主要区别在于在推荐系统中强调特定用户的个性化。例如，对于电影推荐，科幻爱好者的结果页面和彼得·塞勒斯（Peter Sellers）喜剧鉴赏家的结果页面可能会大不相同。在其他推荐设置（例如零售产品，音乐或新闻推荐）中也会出现类似的问题。

在某些情况下，客户会提供明确的反馈，传达他们对特定产品的满意程度（例如，Amazon，IMDB，GoodReads等上的产品评分和评论）。在其他一些情况下，它们例如通过跳过播放列表上的标题来提供隐式反馈，这可能表示不满意，但可能仅表示歌曲在上下文中不合适。在最简单的公式中，对这些系统进行训练，以在给定用户uiui和产品pjpj的情况下估计一些分数yijyij，例如估计的等级或购买概率。

给定这样的模型，那么对于任何给定的用户，我们都可以检索得分最高的对象集ijijyij，然后可以将其推荐给客户。生产系统要先进得多，并且在计算此类分数时会考虑详细的用户活动和商品特征。图1.3.4是Amazon推荐的深度学习书籍的一个示例，该书籍基于调整后的个性化算法来捕捉作者的喜好。

图1.3.4亚马逊推荐的深度学习书籍。

尽管它们具有巨大的经济价值，但单纯基于预测模型构建的推荐系统仍存在一些严重的概念缺陷。首先，我们仅观察经过审查的反馈。用户优先对自己有强烈评价的电影进行评分：您可能会注意到，这些项目获得了许多5星和1星评级，但明显没有3星评级。此外，当前的购买习惯通常是当前采用的推荐算法的结果，但是学习算法并不总是将这一细节考虑在内。因此，有可能形成反馈循环，在该循环中，推荐系统优先推送某个商品，该商品随后被认为是更好的商品（由于购买量较大），进而被推荐得更加频繁。关于如何应对审查，激励机制和反馈循环的许多问题都是重要的开放研究问题。

### 序列学习

到目前为止，我们已经研究了具有固定数量的输入并产生固定数量的输出的问题。在我们考虑通过固定功能预测房价之前：平方英尺，卧室数量，浴室数量，到市区的步行时间。我们还讨论了从图像（具有固定尺寸）到它属于固定数量类的预测概率的映射，或者采用了用户ID和产品ID并预测星级。在这些情况下，一旦我们将固定长度的输入馈入模型以生成输出，模型就会立即忘记它刚刚看到的内容。

如果我们的输入实际上都具有相同的尺寸，并且连续的输入之间确实没有任何关系，那么这可能很好。但是，我们将如何处理视频片段？ 在这种情况下，每个片段可能包含不同数量的帧。如果我们考虑先前或后续的帧，则我们对每个帧中发生的情况的猜测可能会更强。语言也一样。机器翻译是一个流行的深度学习问题：机器以某种源语言摄取句子并预测另一种语言的译文。

这些问题在医学中也会发生。我们可能需要一个模型来监视重症监护室的患者，并在接下来的24小时内死亡风险超过某个阈值时发出警报。我们绝对不希望这种模型每小时抛弃其对患者病史的了解，而只是根据最新的测量结果做出预测。

这些问题是机器学习最令人兴奋的应用之一，它们是序列学习的实例。他们需要一个模型来摄取输入序列或发出输出序列（或两者！）。后面这些问题有时称为seq2seq问题。语言翻译是一个seq2seq问题。从语音中转录文本也是一个seq2seq问题。虽然不可能考虑所有类型的序列转换，但是有许多特殊情况值得一提：

标签和解析。这涉及到用属性注释文本序列。换句话说，输入和输出的数量本质上是相同的。例如，我们可能想知道动词和主语在哪里。或者，我们可能想知道哪些词是命名实体。通常，目标是基于结构和语法假设分解和注释文本，以获得一些注释。这听起来比实际要复杂得多。下面是一个非常简单的示例，使用标记注释句子，标记指出哪些单词指的是已命名的实体。

```txt
Tom has dinner in Washington with Sally.
Ent  -    -    -     Ent      -    Ent
```

自动语音识别。通过语音识别，输入序列xx是讲话者的音频记录（如图1.3.5所示），而输出yy是讲话者所说的文本记录。挑战在于音频帧（通常以8kHz或16kHz采样）比文本多得多，即音频和文本之间没有1：1的对应关系，因为成千上万的采样对应于单个口头单词。这些是seq2seq问题，其中输出比输入短得多。

图 1.3.5

文字转语音。文本语音转换（TTS）是语音识别的逆过程。换句话说，输入xx是文本，输出yy是音频文件。在这种情况下，输出比输入长得多。尽管人们很容易识别出不良的音频文件，但对于计算机而言却并非如此。

机器翻译。与语音识别不同的是，相应的输入和输出以相同的顺序(对齐后)出现，在机器翻译中，顺序反转是至关重要的。换句话说，当我们仍在将一个序列转换为另一个序列时，无论是输入和输出的数量还是相应的数据示例的顺序都不假定是相同的。下面的例子说明了德国人把动词放在句末的特殊倾向。

```txt
德语：    Haben Sie sich schon dieses grossartige Lehrwerk angeschaut?
英语：    Did you already check out this excellent tutorial?
错误排列：Did you yourself already this excellent tutorial looked-at?
```

在其他学习任务中也会出现很多相关的问题。例如，确定用户阅读网页的顺序是一个二维布局分析问题。对话问题表现出各种额外的复杂性，决定接下来说什么需要考虑到真实世界的知识和跨越时间距离的对话的先前状态。这是一个活跃的研究领域。

## 无监督学习

到目前为止，所有示例都与监督学习有关，即我们向模型提供包含特征和相应目标值的巨大数据集的情况。您可以将受监督的学习者视为具有非常专业的工作和极为严格的老板。老板站在你的肩膀上，告诉你在每种情况下该怎么做，直到你学会从情况到行动为止。为这样的老板工作听起来很无聊。另一方面，很容易取悦这个老板。您只需尽快识别出模式并模仿其动作即可。

以一种完全相反的方式，为一个不知道他们想要你做什么的老板工作可能会令人沮丧。但是，如果您打算成为一名数据科学家，则最好习惯一下。老板可能只是将大量数据交给您，并告诉您使用它进行一些数据科学！ 这听起来很模糊，因为是这样。我们称此类问题为无监督学习，我们可以提出的问题的类型和数量仅受我们的创造力的限制。在后面的章节中，我们将介绍许多无监督的学习技术。为了激发您的胃口，我们介绍一些您可能会问的问题：

我们能否找到少量能够准确汇总数据的原型？ 给定一组照片，我们可以将它们分为风景照片，狗，婴儿，猫，山峰等照片吗？ 同样，给定一系列用户的浏览活动，我们可以将他们分组为具有类似行为的用户吗？ 此问题通常称为群集。

我们是否可以找到少量参数来准确捕获数据的相关属性？ 球的轨迹可以通过球的速度，直径和质量很好地描述。裁缝已经开发了少数参数，这些参数可以相当准确地描述人体形状，以适合穿衣服的目的。这些问题称为子空间估计问题。如果相关性是线性的，则称为主成分分析。

在欧几里得空间（即RnRn中向量的空间）中是否存在（任意结构化的）对象的表示，以便可以很好地匹配符号属性？ 这称为表示学习，用于描述实体及其之间的关系，例如罗马-意大利++法国==巴黎。

是否存在对我们观察到的许多数据的根本原因的描述？ 例如，如果我们拥有有关房价，污染，犯罪，地理位置，教育，薪水等的人口统计数据，是否可以仅根据经验数据发现它们之间的关系？ 与因果关系和概率图形模型有关的领域解决了这个问题。

无监督学习中另一个重要且令人兴奋的最新进展是生成对抗网络（GAN）的出现。这些为我们提供了一种程序化的方法来合成数据，甚至是图像和音频等复杂的结构化数据。潜在的统计机制是检验真实和虚假数据是否相同的测试。我们将为他们奉献一些笔记本。

### 优化算法

一旦我们有了一些数据源和表示，一个模型，和一个定义明确的目标函数，我们需要一个算法能够搜索最好的可能参数，使损失函数最小化。最流行的神经网络优化算法遵循一种称为梯度下降的方法。简而言之，在每一步，他们都要检查，对于每个参数，训练集的损失会如何变化如果你只对这个参数施加很小的扰动。然后，他们更新参数的方向，以减少损失。

图1.3.6从环境中收集数据进行监督学习。

离线学习的这种简单性具有其魅力。好处是我们可以担心孤立地进行模式识别，而不会分心于这些其他问题。但是不利的是，问题的提法是非常有限的。如果您有更大的野心，或者您是在长大阅读《 Asimov机器人系列》，那么您可能会想到人工智能机器人，它们不仅能够做出预测，而且能够在世界范围内采取行动。我们想考虑智能主体，而不仅仅是预测模型。这意味着我们需要考虑选择行动，而不仅仅是做出预测。而且，与预测不同，行动实际上会影响环境。如果我们想训练一个智能的特工，我们必须考虑其行为可能影响该特工的未来观察的方式。

考虑与环境的交互会带来一整套新的建模问题。环境是否：

* 还记得我们以前所做的吗？
* 想要帮助我们，例如，用户将文本阅读到语音识别器中吗？
* 是否想打败我们，例如垃圾邮件过滤（针对垃圾邮件发送者）或玩游戏（相对于对手）这样的对抗性环境？
* 不在乎（在许多情况下）？
* 动态变化了吗（未来的数据是否总是像过去一样，或者模式会随着时间的推移自然地改变或响应我们的自动化工具而改变）？

最后一个问题提出了分布偏移的问题（当训练和测试数据不同时）。这是我们大多数人在由讲师撰写的考试中遇到的一个问题，而家庭作业则由他的助教编写。我们将简要描述强化学习和对抗学习，这两种设置明确考虑了与环境的交互。

## 强化学习

如果您对使用机器学习来开发与环境交互并采取行动的代理感兴趣，那么您可能最终会专注于强化学习（RL）。这可能包括机器人技术，对话系统乃至为视频游戏开发AI的应用程序。将深度神经网络应用于RL问题的深度强化学习（DRL）越来越受欢迎。突破性的深度Q网络仅在视觉输入上就能在Atari游戏中击败人类，而在棋盘游戏Go上击败世界冠军的AlphaGo程序就是两个突出的例子。

强化学习给出了关于问题的非常笼统的陈述，其中，代理在一系列时间步长上与环境交互。在每个时间步tt处，代理都会从环境中接收一些观察信息，并且必须选择一个动作，该动作随后会通过某种机制（有时称为执行器）传递回环境。最后，代理从环境中获得回报。然后，代理会收到后续观察，并选择后续操作，依此类推。RL代理的行为受策略控制。简而言之，策略只是一种功能，它从（环境）的观察结果映射到操作。强化学习的目的是制定良好的政策。

图1.3.7强化学习与环境的交互作用

很难夸大RL框架的一般性。例如，我们可以将任何监督学习问题转换为RL问题。假设我们遇到分类问题。我们可以创建一个RL代理，每个代理对应一个动作。然后，我们可以创建一个环境，该环境给出的奖励与原始监督问题产生的损失函数完全相等。

话虽如此，RL也可以解决监督学习无法解决的许多问题。例如，在监督学习中，我们始终希望培训输入与正确的标签相关联。但是在RL中，我们不假定每次观察都表明环境会告诉我们最佳行动。一般来说，我们只是得到一些奖励。此外，环境甚至可能无法告诉我们哪些行为导致了回报。

考虑例如下棋。唯一真实的奖励信号出现在游戏结束时，我们要么赢了，我们可能会分配1的奖励，要么当我们输了之后我们会给-1的奖励。因此，强化学习者必须处理学分分配问题：确定应归功于或归咎于结果的行为。对于在10月11日获得晋升的员工而言，情况也是如此。晋升很可能反映了上一年大量的精心选择的举动。将来要获得更多促销，需要弄清楚促成促销的过程中采取了哪些行动。

强化学习者可能还必须处理部分可观察性的问题。也就是说，当前观察可能无法告诉您有关当前状态的所有信息。假设有一个清洁机器人发现自己被困在房屋中许多相同的壁橱之一中。推断机器人的精确位置（并由此得出状态）可能需要在进入壁橱之前考虑其先前的观察结果。

最后，在任何给定的时间点，强化学习者可能都知道一个好的策略，但是可能还有许多其他更好的策略，agent从未尝试过。强化学习者必须不断选择是将当前最佳的策略作为一项策略，还是探索策略的空间，从而有可能放弃一些短期奖励以换取知识。

## MDPs、强盗和朋友实现

一般的强化学习问题是一个非常一般的设置。动作会影响后续的观察。奖励只与选择的行为相对应。环境可以被完全或部分观察到。一下子把所有这些复杂性都解释清楚可能对研究人员要求太高。此外，并非所有的实际问题都表现出这种复杂性。因此，研究者们研究了一些特殊的强化学习问题的案例。

当环境被完全观察时，我们将RL问题称为马尔可夫决策过程(MDP)。当国家不依赖于之前的行动时，我们称这个问题为背景强盗问题。当没有状态，只有一组具有初始未知奖励的可用动作时，这个问题就是典型的多武装强盗问题。

## 根源

尽管许多深度学习方法是最新发明，但人类一直渴望分析数据并预测未来的结果已有数百年的历史了。实际上，许多自然科学都源于此。例如，伯努利分布以雅各布·伯努利（1655-1705）命名，高斯分布由卡尔·弗里德里希·高斯（1777-1855）发现。例如，他发明了最小均方算法，该算法至今仍用于从保险计算到医疗诊断的无数问题。这些工具引发了自然科学中的实验方法，例如，电阻器中与电流和电压有关的欧姆定律可以通过线性模型完美地描述。

即使在中世纪，数学家也对估算有敏锐的直觉。例如，雅各布·科贝尔（JacobKöbel（1460-1533））的几何书籍说明了平均16位成年男子脚的长度，以获得平均脚长。

图1.4.1估算脚的长度

图1.4.1说明了此估算器的工作方式。离开教堂时，要求16名成年男子连续排队。然后将它们的总长度除以16，以获得现在等于1英尺的估计值。后来改进了这种“算法”，以处理脚部畸形—分别将脚最短和最长的2名男人送走了，只剩下剩下的几个了。这是调整后的均值估计的最早示例之一。

统计真正随着数据的收集和可用性而发展起来。它的巨人之一，罗纳德·费舍（Ronald Fisher，1890-1962年）对它的理论及其在遗传学中的应用做出了重大贡献。今天，他的许多算法（例如线性判别分析）和公式（例如Fisher信息矩阵）仍然经常使用（甚至他在1936年发布的Iris数据集有时仍用于说明机器学习算法）。费舍尔还是优生学的拥护者，这应该提醒我们，对数据科学的道德怀疑使用与在工业和自然科学中的生产性使用一样具有悠久而持久的历史。

机器学习的第二个影响力来自信息理论（Claude Shannon，1916-2001）和Alan Turing的计算理论（1912-1954）。图灵提出了一个问题：“机器可以思考吗？” 在他的著名论文《计算机械与智能》（1950年10月，思想）中。在他所说的图灵测试中，如果评估人员难以根据文本交互来区分机器和人类的回复，则可以认为机器是智能的。

神经网络的名字来源于生物学灵感。一个多世纪以来(追溯到1873年亚历山大·贝恩(Alexander Bain)和1890年詹姆斯·谢林顿(James Sherrington)的模型)，研究人员一直在尝试组装类似互动神经元网络的计算电路。随着时间的推移，对生物学的解释越来越少，但这个名字却一直沿用至今。在它的核心，存在一些关键原则，可以在今天的大多数网络中找到:

* 线性和非线性处理单元的交替，通常称为层。
* 使用链式法则(也称为反向传播)一次性调整整个网络中的参数。

在经历了最初的快速发展之后，神经网络的研究从1995年到2005年一直处于停滞状态。这是由于许多原因。训练网络在计算上是非常昂贵的。虽然在上个世纪末RAM已经很丰富了，但计算能力却很匮乏。第二，数据集相对较小。事实上，费雪1932年的虹膜数据集是测试算法有效性的一个流行工具。有6万个手写数字的MNIST被认为是巨大的。

由于缺乏数据和计算，强大的统计工具，如核方法，决策树和图形模型的经验证明了优越性。与神经网络不同的是，它们不需要几周的训练，而且提供了强大的理论保证，可预测的结果。

## 深度学习之路

这改变了唾手可得的大量的数据,由于万维网,公司服务数亿用户的出现在网上,传播的便宜,高质量的传感器,廉价的数据存储(Kryder定律),和廉价的计算(摩尔定律),特别是在gpu的形式,最初设计的电脑游戏。突然之间，似乎在计算上不可行的算法和模型变得相关起来(反之亦然)。表1.5.1很好地说明了这一点。

表1.5.1数据集与计算机内存和计算能力

很明显，RAM没有跟上数据增长的步伐。与此同时，计算能力的增长已经超过了可用数据的增长速度。这意味着统计模型需要提高内存效率(这通常是通过添加非线性实现的)，同时由于计算预算的增加，还要能够花更多的时间来优化这些参数。因此，机器学习和统计的最佳点从(广义的)线性模型和核方法转移到深度网络。这也是其中一个原因为什么许多的深度学习的支柱,如多层感知器(麦克洛克&皮茨,1943),卷积神经网络(勒存et al ., 1998),长期短期记忆(1997年,的Hochreiter &。施密德胡贝尔表示),和q学习(沃特金斯&达扬,1992),在本质上是“重新发现”在过去的十年里,铺设后相对沉寂了相当长的时间。

最近在统计模型、应用和算法方面的进展，有时被比作寒武纪大爆发:一个物种进化迅速发展的时刻。事实上，技术的进步不仅仅是可用资源应用于几十年前的算法的结果。请注意，下面的列表仅仅触及了在过去十年中帮助研究者取得巨大进步的想法的表面。

* 诸如Dropout [Srivastava等人，2014]等新的容量控制方法有助于减轻过度拟合的危险。这是通过在整个网络中应用噪声注入[Bishop，1995]来实现的，将权重替换为随机变量以进行训练。
* 注意机制解决了困扰统计学一个多世纪的第二个问题：如何在不增加可学习参数的数量的情况下增加系统的内存和复杂性。[Bahdanau et al。，2014]通过使用只能被视为可学习的指针结构的方法找到了一种优雅的解决方案。不必记住整个句子，例如对于以固定尺寸表示形式的机器翻译，只需存储指向翻译过程中间状态的指针即可。这使得长句子的准确性大大提高，因为该模型在开始生成新句子之前不再需要记住整个句子。
* 多阶段设计，例如通过记忆网络（MemNets）[Sukhbaatar等人，2015]和神经程序解释器[Reed＆DeFreitas，2015]，可以使统计建模人员描述迭代推理的方法。这些工具允许重复修改深度网络的内部状态，从而在推理链中执行后续步骤，类似于处理器如何修改内存以进行计算。
* 另一个重要的发展是GAN的发明[Goodfellow等，2014]。传统上，用于密度估计和生成模型的统计方法侧重于找到合适的概率分布以及（通常是近似的）从中进行采样的算法。结果，这些算法在很大程度上受到统计模型固有的缺乏灵活性的限制。GAN中的关键创新是用具有可区分参数的任意算法替换采样器。然后调整这些值，以使鉴别器（实际上是两个样本的测试）无法区分假数据和真实数据。通过使用任意算法来生成数据的能力，它为多种技术提供了密度估计。斑马奔腾[Zhu等人，2017]和假名人面孔[Karras等人，2017]的例子都证明了这一进展。即使是业余涂鸦者，也可以根据草图来生成逼真的图像，这些草图描述了场景的布局[Park et al。，2019]。
* 在许多情况下，单个GPU不足以处理可用于训练的大量数据。在过去的十年中，构建并行分布式训练算法的能力已大大提高。设计可伸缩算法的主要挑战之一是深度学习优化的主力，即随机梯度下降，依赖于要处理的相对较小的小批数据。同时，小批量会限制GPU的效率。因此，在1024个GPU上进行最小批量大小（例如每批32幅图像）的训练等于总计32,000张图像的最小批量。最近的工作，首先是由Li [Li，2017]，随后是[You et al。，2017]和[Jia et al。，2018]，将大小扩大到了64k观测值，从而将ImageNet上ResNet50的训练时间减少到少于 7分钟 为了进行比较，最初的培训时间以天为单位。
* 并行计算的能力也对增强学习的进步做出了至关重要的贡献，至少在每当可以选择模拟时。这导致计算机在Go，Atari游戏，Starcraft和物理模拟（例如，使用MuJoCo）中实现超人性能方面取得了显着进步。有关如何在AlphaGo中实现此目标的说明，请参见例如[Silver et al。，2016]。简而言之，如果有大量的（状态，动作，奖励）三元组可用，也就是说，只要有可能尝试很多事物来学习它们之间的相互关系，加强学习的效果就最好。仿真提供了这样的途径。
* 深度学习框架在传播思想中发挥了关键作用。便于建模的第一代框架包括Caffe，Torch和Theano。使用这些工具撰写了许多开创性的论文。到目前为止，它们已被TensorFlow取代，TensorFlow通常通过其高级API Keras，CNTK，Caffe 2和Apache MxNet使用。Chainer可以说是第三代工具，即深度学习的必要工具，它使用类似于Python NumPy的语法来描述模型。PyTorch，MXNet的Gluon API和Jax都采用了这个想法。本课程用于教授深度学习的是后者。

建立更好工具的系统研究人员和建立更好网络的统计建模人员之间的分工大大简化了事情。例如，训练线性逻辑回归模型曾经是一个重要的家庭作业问题，值得在2014年交给卡内基梅隆大学(Carnegie Mellon University)的新机器学习博士生。到目前为止，这个任务只需要不到10行代码就可以完成，将其牢牢地掌握在程序员手中。

## 成功的故事

人工智能在提供用其他方法难以实现的结果方面有着悠久的历史。例如，使用光学字符识别对邮件进行排序。这些系统从上世纪90年代就开始部署了(毕竟，这就是著名的MNIST和USPS手写数字集的来源)。同样的道理也适用于阅读银行存款支票和评估申请人的资信。金融交易被自动检查是否有欺诈行为。这形成了许多电子商务支付系统的支柱，如贝宝、Stripe、支付宝、微信、苹果、Visa、万事达。国际象棋的计算机程序已经竞争了几十年。机器学习在互联网上提供搜索、推荐、个性化和排名。换句话说，人工智能和机器学习是无处不在的，尽管它们常常被隐藏起来。

直到最近，人工智能才成为众人瞩目的焦点，这主要是由于解决了以前认为棘手的问题。

* 诸如Apple的Siri，亚马逊的Alexa或Google的助手等智能助手能够以合理的准确度回答口头问题。这包括一些艰巨的任务，例如打开电灯开关（对残疾人士来说是个福音）直到理发师的约会以及提供电话支持对话框。这可能是人工智能正在影响我们生活的最明显迹象。
* 数字助理中的关键要素是准确识别语音的能力。此类系统的准确性逐渐提高到了达到某些应用的同等水平[Xiong et al。，2018]。
* 对象识别同样已经走了很长一段路。估计图片中的对象在2010年是一项相当艰巨的任务。在ImageNet基准测试中[Lin et al。，2010]，前5位错误率达到28％。到2017年，[Hu et al。，2018]将该错误率降低到2.25％。同样，在识别鸟类或诊断皮肤癌方面也取得了惊人的结果。
* 游戏曾经是人类智慧的堡垒。从TDGammon [23]开始，一种使用时差（TD）强化学习，算法和计算进展来玩西洋双陆棋的程序已导致算法在众多应用中得到应用。与五子棋不同，国际象棋的状态空间和动作集要复杂得多。DeepBlue击败了Garry Kasparov，Campbell等。[Campbell et al。，2002]，使用大规模并行性，专用硬件和对游戏树的高效搜索。由于其巨大的状态空间，Go仍然更加困难。AlphaGo结合深度学习和蒙特卡洛树采样技术，在2015年达到了人类均等水平[Silver等，2016]。扑克面临的挑战是州空间很大，而且没有被完全观察到（我们不知道对手的牌）。Libratus使用有效的结构化策略超越了扑克中的人类表现[Brown＆Sandholm，2017]。这说明了游戏取得了令人瞩目的进步，以及先进算法在其中发挥了至关重要的作用。
* 人工智能取得进展的另一个迹象是自动驾驶汽车和卡车的出现。尽管尚无法完全实现自主性，但在这一方向上已经取得了卓越的进展，特斯拉，NVIDIA和Waymo等公司推出了至少实现部分自主性的产品。完全自治如此具有挑战性的是，适当的驾驶需要具备感知，推理和将规则整合到系统中的能力。目前，深度学习主要用于这些问题的计算机视觉方面。其余的由工程师进行了很大的调整。

再次说，上面的清单几乎没有触及机器学习影响实际应用的地方。例如，机器人技术，物流，计算生物学，粒子物理学和天文学的最新进展令人印象深刻，至少部分归功于机器学习。因此，ML（机器学习）成为工程师和科学家无处不在的工具。

通常，在关于AI的非技术文章中提出了AI启示或AI奇异性的问题。担心的是，机器学习系统将以某种方式变得有知觉，并独立于其程序员（和大师）决定对直接影响人类生计的事物。人工智能已经在某种程度上直接影响着人类的生计-信誉被自动评估，自动驾驶仪主要在车辆上行驶，关于是否准予保释的决定使用统计数据作为输入。更轻描淡写的是，我们可以要求Alexa打开咖啡机。

幸运的是，我们离可以操纵人类创造者（或烧掉咖啡）的有感觉的人工智能系统相去甚远。首先，以特定的，面向目标的方式设计，培训和部署AI系统。尽管它们的行为可能给人以一般智能的幻觉，但它是设计基础的规则，启发式和统计模型的组合。其次，目前根本不存在用于人工通用智能的工具，这些工具能够自我完善，自我推理，并且能够在尝试解决一般任务时修改，扩展和改善自己的体系结构。

更加迫切的问题是在日常生活中如何使用AI。卡车司机和售货员完成的许多繁琐的工作很可能会并且将自动执行。农场机器人可能会降低有机农业的成本，但它们还将使收割作业自动化。工业革命的这一阶段可能对社会的广大面产生深远的影响（卡车司机和售货员是许多州最常见的工作）。此外，统计模型如果不加小心地应用，会导致种族，性别或年龄偏见，如果自动做出相应的决定，则会引起对程序公平性的合理关注。确保谨慎使用这些算法很重要。凭我们今天所知道的，这比恶意的超级智能破坏人类的潜力引起了我们更为紧迫的关注。

## 小结

* 机器学习研究计算机系统如何利用经验(通常是数据)来提高特定任务的表现。它结合了统计、数据挖掘、人工智能和优化的思想。通常，它被用作实现人工智能解决方案的一种手段。
* 作为机器学习的一个类别，代表性学习关注的是如何自动找到合适的方式来表示数据。这通常是通过一系列学习转换来完成的。
* 最近在深度学习方面的许多进展，是由廉价传感器和互联网规模的应用程序产生的大量数据，以及主要通过gpu实现的计算的重大进展所引发的。
* 整个系统的优化是获得良好性能的关键组成部分。高效的深度学习框架使得设计和实现这一功能变得非常容易。

## 练习

1. 你现在正在写的代码的哪些部分是可以“学习”的。通过学习和自动确定在代码中做出的设计选择而得到改进?您的代码是否包含启发式设计选择?
1. 您遇到的哪些问题有很多例子说明如何解决它们，但没有特定的方法来自动化它们?这些可能是使用深度学习的主要候选者。
1. 将人工智能的发展视为一场新的工业革命，算法与数据之间的关系是什么?它类似于蒸汽机和煤吗(根本区别是什么)?
1. 您还可以在哪里应用端到端训练方法(如图1.1.2)?物理吗?工程?计量经济学吗?
