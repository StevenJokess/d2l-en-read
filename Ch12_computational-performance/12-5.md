

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 21:09:16
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-30 21:55:11
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_computational-performance/multiple-gpus.html
-->

# 在多个 gpu 上进行训练

到目前为止，我们讨论了如何在 CPU 和 GPU 上有效地训练模型。在12.3节中，我们甚至展示了深度学习框架如何允许自动并行化计算和它们之间的通信。最后，我们在5.6节中展示了如何使用 `nvidia-smi` 列出计算机上所有可用的 GPU。我们没有讨论的是如何实际并行化深度学习训练(我们在这里省略了关于多个 GPU 上的推理的任何讨论，因为它是一个相当少用和高级的话题，超出了本书的范围)。相反，我们在传递中暗示，一个将以某种方式在多个设备之间分割数据并使其工作。本节将填充详细信息，并演示如何从头开始并行地训练网络。关于如何利用 Gluon 中的功能的详细信息被归入第12.6节。我们假设读者熟悉小批量 SGD 算法，如第11.5节中描述的算法。

## 分解问题

让我们从一个简单的计算机视觉问题和一个略显陈旧的网络开始，例如，有多层卷积、汇集，最后可能还有几个密集的层。也就是说，让我们从一个看起来非常类似 LeNet [ LeCun 等人，1998]或 AlexNet [ Krizhevsky 等人，2012]的网络开始。给定多个 gpu (2如果是桌面服务器，4个 g4dn. 12 xlarge，8个 AWS p3.16 xlarge，或者16个 p2.16 xlarge) ，我们希望以一种能够达到良好加速的方式对训练进行分区，同时受益于简单和可重复的设计选择。毕竟，多个 gpu 同时提高了内存和计算能力。简而言之，我们有很多选择，给定一小批我们想要分类的培训数据。

图12.5.1由于 GPU 内存有限，原 AlexNet 设计中的模型并行性。

* 我们可以跨多个 gpu 对网络层进行分区。也就是说，每个 GPU 接受输入的数据流入一个特定的层，处理数据跨越一些后续层，然后发送数据到下一个 GPU。
  * 这使得我们可以在更大的网络中处理数据，而不用一个 GPU 就可以处理。
  * 每个 GPU 的内存占用量可以很好地控制(它只是整个网络占用量的一小部分)
  * 层和图形处理器之间的接口需要严格的同步。这可能很棘手，特别是如果层之间的计算工作负载不匹配。大量的 gpu 加剧了这个问题。
  * 层之间的接口需要大量的数据传输(激活、梯度)。这可能会超过 GPU 总线的带宽。
* 我们可以把工作分成各个层次。例如，与其在一个 GPU 上计算64个通道，我们可以将问题分解到4个 GPU 上，每个 GPU 为16个通道生成数据。同样，对于密集层，我们可以分离输出神经元的数量。图12.5.1说明了这种设计。这个数字取自[ Krizhevsky 等人，2012] ，在那里这个策略被用来处理内存占用非常小的 gpu (当时是2gb)。
  * 如果通道(或神经元)的数量不是太少的话，这允许在计算方面有很好的伸缩性。
  * 多个 gpu 可以处理越来越大的网络，因为可用内存是线性增长的。
  * 我们需要大量的同步/障碍操作，因为每个层依赖于所有其他层的结果。
  * 需要传输的数据量甚至可能比跨 gpu 分布图层时还要大。我们不推荐这种方法，因为它的带宽成本和复杂性。
* 最后，我们可以跨多个 gpu 对数据进行分区。这样所有的 gpu 执行相同类型的工作，尽管观察结果不同。梯度在每个小批处理后在 gpu 之间聚集。
  * 这是最简单的方法，可以应用于任何情况。
  * 增加更多的 gpu 不允许我们训练更大的模型。
  * 我们只需要同步后，每个小批处理。也就是说，当其他梯度参数仍在计算时，最好已经开始交换梯度参数。
  * 大量的 gpu 会导致非常大的小批量，从而降低培训效率。

图：在多个GPU上并行化。从左到右-原始问题，网络分区，层分区，数据并行性。

总体而言，如果我们可以访问具有足够大内存的GPU，则数据并行是最方便的处理方式。另请参见[Li等，2014]，以了解分布式培训分区的详细说明。在深度学习的早期，GPU内存曾经是一个问题。到目前为止，除了最不常见的情况之外，所有其他问题均已解决。接下来我们将重点介绍数据并行性。

## 数据并行

假设机器上有kk个GPU。给定要训练的模型，每个GPU将独立维护完整的模型参数集。训练过程如下（有关两个GPU上的数据并行训练的详细信息，请参见图12.5.3）：

* 在训练的任何迭代中，给定随机的最小批处理，我们将批处理中的示例分成k部分，并将它们均匀地分布在GPU上。
* 每个GPU根据为其分配的minibatch子集及其所维护的模型参数来计算模型参数的损失和梯度。
* 汇总每个k GPU的局部梯度，以获得当前的minibatch随机梯度。
* 聚合梯度将重新分配给每个GPU。
* 每个GPU都使用此minibatch随机梯度来更新其维护的完整模型参数集。

图12.5.3使用数据并行性和两个GPU计算小批量随机梯度

图12.5.2描绘了多个GPU上不同并行化方式的比较。请注意，实际上，在k个GPU上进行训练时，我们会增加最小批量大小k倍，以使每个GPU的工作量与仅在单个GPU上进行训练的工作量相同。在16 GPU服务器上，这可能会大大增加minibatch的大小，因此我们可能必须相应地提高学习速度。另请注意，第7.5节需要进行调整（例如，通过为每个GPU保留单独的批处理范数系数）。接下来，我们将使用6.6节作为玩具网络来说明多GPU训练。与往常一样，我们首先导入相关的软件包和模块。

## 简单网络

我们使用第6.6节中介绍的LeNet。我们从头开始定义它，以详细说明参数交换和同步。

TODO:CODE

## 数据同步

为了进行有效的多GPU训练，我们需要执行两个基本操作：首先，我们需要具有将参数列表分配给多个设备并附加渐变（`get_params`）的能力。没有参数，就无法在GPU上评估网络。其次，我们需要具有跨多个设备求和参数的能力，即我们需要一种`allreduce`函数。

TODO:CODE

让我们通过将lenet的模型参数复制到gpu（0）进行尝试。

TODO:CODE

由于我们尚未执行任何计算，因此偏向权重的梯度仍然为00。现在让我们假设我们有一个分布在多个GPU上的向量。以下`allreduce`函数将所有向量相加，并将结果广播回所有GPU。请注意，要执行此操作，我们需要将数据复制到累积结果的设备中。

TODO:CODE

让我们通过在不同设备上创建具有不同值的向量并对它们进行聚合来进行测试。

TODO:CODE

## 分布式数据

我们需要一个简单的实用程序功能，以在多个GPU之间平均分配小批量。例如，在2个GPU上，我们希望将一半的数据复制到每个GPU。由于它更加方便和简洁，因此我们在Gluon中使用了内置的split and load函数（可以在4×5矩阵上进行尝试）。

TODO:CODE

为了以后重用，我们定义了`split_batch`函数，该函数同时拆分数据和标签。

TODO:CODE

## 训练

现在，我们可以在单个minibatch上实施多GPU训练。它的实现主要基于本节中描述的数据并行性方法。我们将使用刚刚讨论的辅助功能`allreduce`和`split_and_load`在多个GPU之间同步数据。注意，我们不需要编写任何特定的代码即可实现并行性。由于计算图在小批量生产中在设备之间没有任何依存关系，因此它会自动并行执行。

TODO:CODE

现在，我们可以定义训练功能。它与前几章中使用的稍有不同：我们需要分配GPU，并将所有模型参数复制到所有设备。显然，每个批次都使用`train_batch`处理以处理多个GPU。为了方便起见（以及代码的简洁性），我们在单个GPU上计算精度（这是低效率的，因为其他GPU处于空闲状态）。

TODO:CODE

## 实验

让我们看看它在单个GPU上的效果如何。我们使用256的批量大小和0.2的学习率。

TODO:CODE

在保持批量大小和学习率不变的情况下，将GPU数量改为2个，我们可以看到测试准确率的提高与之前实验的结果大致相同。在优化算法方面，它们是相同的。不幸的是，这里并没有得到有意义的加速:模型实在太小了;此外，我们只有一个小的数据集，在这一点上，我们实现多GPU训练的略微简单的方法受到了显著的Python开销的影响。我们将遇到更复杂的模型和更复杂的并行化方法。让我们看看MNIST会发生什么。

TODO:CODE

## 小结

* 有多种方法可以在多个GPU上拆分深度网络训练。我们可以将它们分为层，跨层或跨数据。前两个需要紧密编排的数据传输。数据并行是最简单的策略。
* 数据并行训练非常简单。但是，它增加了有效的最小批处理大小以提高效率。
* 数据分布在多个GPU上，每个GPU都执行自己的正向和反向操作，随后将梯度进行聚合并将结果广播回给GPU。
* 大型迷你批可能需要稍微提高学习率。

## 练习

1. 在多个GPU上训练时，将最小批量大小从b更改为k⋅b，即按GPU的数量将其扩大。
2. 比较不同学习率的准确性。它如何随GPU数量而扩展。
3. 实施更有效的allreduce，以在不同GPU上聚合不同的参数（为什么这首先效率更高）。
4. 实现多GPU测试精度计算。
