

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-14 22:13:18
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-30 21:35:59
 * @Description:MT， improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_computational-performance/async-computation.html
-->

# 异步计算

今天的计算机是高度并行的系统，包括多个 CPU 内核(通常每个内核有多个线程) ，每个 GPU 有多个处理元件，每个设备通常有多个 GPU。简而言之，我们可以在同一时间处理许多不同的事情，通常是在不同的设备上。不幸的是，Python 并不是编写并行和异步代码的好方法，至少不需要额外的帮助。毕竟，Python 是单线程的，这在未来不太可能改变。深度学习框架(如 MXNet 和 TensorFlow)利用异步编程模型来提高性能(PyTorch 使用 Python 自己的调度程序导致不同的性能折衷)。因此，了解异步编程的工作原理有助于我们开发更高效的程序，通过前瞻性地减少计算需求和相互依赖。这使我们可以减少内存开销并提高处理器利用率。我们首先导入必要的库。

TODO:CODE

## 通过后端的异步

对于热身，考虑下面的玩具问题——我们想生成一个随机矩阵并将其乘以。让我们用 NumPy 和 MXNet NP 来看看两者的区别。

TODO:CODE

这个数量级更快。至少看起来是这样。因为两者都是在同一个处理器上执行的，所以一定有其他的原因。强制 MXNet 在返回之前完成所有计算显示了之前发生的情况: 计算由后端执行，而前端将控制权返回给 Python。

TODO:CODE

一般来说，MXNet 有一个与用户直接交互的前端，例如，通过 Python，以及系统用来执行计算的后端。后端拥有自己的线程，这些线程不断地收集和执行排队任务。请注意，为了实现这一点，后端必须能够跟踪计算图中各个步骤之间的依赖关系。因此，将不相互依赖的操作并行化是可能的。

如图12.2.1所示，用户可以用各种前端语言编写 MXNet 程序，比如 Python、R、Scala 和C++ 。无论使用哪种前端编程语言，MXNet 程序的执行主要发生在C++ 实现的后端。由前端语言发出的操作被传递到后端执行。后端管理自己的线程，这些线程不断收集和执行排队任务。请注意，为了实现这一点，后端必须能够跟踪计算图中各个步骤之间的依赖关系。也就是说，不可能将相互依赖的操作并行化。

让我们看看另一个玩具示例，以更好地理解依赖图。

TODO:CODE

上面的代码段也显示在图12.2.2中。每当Python前端线程执行前三个语句之一时，它只会将任务返回到后端队列。当需要打印最后一条语句的结果时，Python前端线程将等待C ++后端线程完成变量z的计算结果。这种设计的一个好处是Python前端线程不需要执行实际的计算。因此，无论Python的性能如何，对程序的整体性能几乎没有影响。图12.2.3说明了前端和后端如何交互。

## 障碍和阻滞剂

有许多操作会迫使Python等待完成：*最明显的是npx.waitall（）会等到所有计算完成后，无论何时发出计算指令。实际上，除非绝对必要，否则使用此运算符是个坏主意，因为它可能导致性能下降。*如果我们只想等待特定的变量可用，则可以调用z.wait_to_read（）。在这种情况下，MXNet块将返回Python，直到计算出变量z。此后，其他计算可能会继续进行。

TODO:CODE

这两个操作大约花费相同的时间来完成。除了明显的阻塞操作，我们建议读者了解隐式阻塞器。打印变量显然需要变量可用，因此它是一个阻止器。最后，通过z.asnumpy()转换到NumPy和通过z.item()转换到标量都是阻塞的，因为NumPy没有异步的概念。它需要访问值，就像打印函数一样。频繁地将少量数据从MXNet的作用域复制到NumPy，然后再复制回来，这可能会破坏原本高效的代码的性能，因为每次这样的操作都需要计算图评估所有中间结果，以获得相关的术语，然后再进行其他操作。

TODO:CODE

## 提高计算能力

在一个高度多线程的系统上(即使是普通的笔记本电脑也有4个或更多的线程，而在多套接字服务器上这个数字可能超过256)，调度操作的开销可能会变得非常大。这就是为什么我们非常希望计算和调度异步和并行地发生。为了说明这样做的好处，让我们看看如果将一个变量按顺序或异步地加1多次会发生什么。我们通过在每个添加之间插入wait_to_read()屏障来模拟同步执行。

稍微简化的Python前端线程和c++后端线程之间的交互可以总结如下:

1. 前端命令后端将计算任务y = x + 1插入队列。
1. 然后后端从队列接收计算任务并执行实际计算。
1. 然后后端将计算结果返回给前端。

假设这三个阶段的持续时间分别为t1、t2和t3。如果我们不使用异步编程，执行1000次计算所花费的总时间大约为1000(t1+t2+t3)。如果使用异步编程，执行1000次计算所花费的总时间可以减少到t1+1000t2+t3(假设1000t2>999t1)，因为前端不必等待后端为每个循环返回计算结果。

## 提高记忆足迹

想象一下一种情况，我们通过在前端执行Python代码来继续将操作插入后端。例如，前端可能会在很短的时间内插入大量的minibatch任务。毕竟，如果Python中没有进行有意义的计算，则可以很快完成。如果可以同时快速启动所有这些任务，则可能会导致内存使用量激增。给定GPU（甚至CPU）上可用的内存量有限，这可能导致资源争用甚至程序崩溃。一些读者可能已经注意到，以前的训练例程使用了同步方法，例如item甚至asnumpy。

我们建议您谨慎使用这些操作，例如，对于每个小型批处理，以平衡计算效率和内存占用量。为了说明发生了什么，让我们为深度网络实现一个简单的训练循环，并测量其内存消耗和时序。下面是模拟数据生成器和深度网络。

TODO:CODE

接下来，我们需要一个工具来测量代码的内存占用量。我们使用一个相对原始的ps调用来完成此操作（请注意，后者仅适用于Linux和MacOS）。要详细了解此处发生的情况，请使用Nvidia的Nsight或英特尔的vTune。

TODO:CODE

在开始测试之前，我们需要初始化网络参数并批量处理。否则，要查看额外的内存消耗是很难的。有关初始化的更多详细信息，请参见第5.3节。

TODO:CODE

为了确保我们不会在后端使任务缓冲区溢出，我们在每个循环的结尾插入一个对loss函数的wait_to_read调用。这迫使前向传播在开始新的前向传播之前完成。请注意，（可能更优雅的）替代方法是在标量变量中跟踪损失并通过item调用强制设置障碍。

TODO:CODE

如我们所见，小批处理的时间与优化代码的整体运行时间非常吻合。此外，内存占用量仅略有增加。现在让我们看看如果在每个小批量结束时降低障碍会发生什么。

TODO:CODE

即使为后端发出指令的时间减少了一个数量级，我们仍然需要执行计算。因此，大量的中间结果无法释放，并且可能堆积在内存中。尽管这在上面的玩具示例中没有引起任何问题，但如果在现实世界中未经检查，可能会导致内存不足的情况。

## 小结

* MXNet将Python前端与执行后端分离。这允许将命令快速异步插入后端​​以及相关的并行性。
* 异步导致响应速度相当快。但是，请注意不要过度填充任务队列，因为这可能导致过多的内存消耗。
* 建议对每个微型批处理进行同步，以使前端和后端保持大致同步。
* 请注意，从MXNet的内存管理到Python的转换将迫使后端等待，直到特定变量准备就绪。print，asnumpy和item都具有此效果。这可能是理想的，但是无心使用同步会破坏性能。
* 芯片供应商提供了复杂的性能分析工具，以更深入地了解深度学习的效率。

## 练习

1. 上面我们提到使用异步计算可以将执行10001000计算所需的总时间减少到t1 + 1000t2 + t3。为什么要在这里假设1000t2> 999t1？
2. 如果您希望每个小批量重叠一次，您将如何修改训练循环？ 即，是否要确保在批处理bt + 2开始之前完成批处理bt？
3. 如果我们要同时在CPU和GPU上执行代码，会发生什么情况？ 在发出每个小批量订单之后，您是否仍要坚持同步？
4. 测量`waitall`和`wait_to_read`之间的差异。提示：执行许多指令并进行同步以获得中间结果。
