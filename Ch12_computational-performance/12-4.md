

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-31 18:37:52
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-08-30 21:59:09
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_computational-performance/hardware.html
-->

# 硬件

构建具有良好性能的系统需要对算法和模型有很好的理解，以捕捉问题的统计方面。同时，对底层硬件至少有一点点了解也是必不可少的。目前的部分不能取代硬件和系统设计的正确课程。相反，它可以作为一个起点，用于理解为什么某些算法比其他算法更高效，以及如何获得良好的吞吐量。好的设计可以很容易地改变一个人的数量级，反过来，这也可以改变一个人是否能够训练一个网络(例如，一个星期)或者根本不训练(3个月，因此错过了最后期限)。我们将从研究电脑开始。然后我们将放大以更仔细地查看 CPU 和 GPU。最后我们缩小回顾一下如何在一个服务器中心或云中连接多台计算机。这不是一个 GPU 购买指南。19.5节。关于 AWS 的云计算的介绍可以在19.3节中找到。

不耐烦的读者也许能够通过图12.4.1。这篇文章摘自科林 · 斯科特的互动帖子，它很好地概述了过去十年的进展。最初的数据来自杰夫 · 迪恩2010年在斯坦福大学的演讲。下面的讨论解释了这些数字的一些基本原理，以及它们如何指导我们设计算法。下面的讨论是非常高的水平和粗略的。它显然不能替代正确的课程，而只是为统计建模师提供足够的信息以作出适当的设计决策。对于计算机体系结构的深入概述，我们建议读者参阅[ Hennessy & Patterson，2011]或者关于这个主题的最新课程，如 Arste Asanovic 的课程。

图12.4.1每个程序员应该知道的延迟数。

## 电脑

大多数深度学习的研究人员可以使用一台拥有相当数量内存、计算能力的计算机，或某种形式的加速器，如 GPU，或其倍数。它包括几个关键组成部分:

* 一个处理器，也称为 CPU，它能够执行我们给它的程序(除了运行操作系统和许多其他东西) ，通常由8个或更多的内核组成。
* 存储和检索计算结果的内存(RAM)，例如权重向量、激活，通常是训练数据。
* 以太网网络连接(有时是多个) ，速度从1Gbit/s 到100Gbit/s (在高端服务器上可以找到更高级的互连)。
* 一种高速扩展总线(PCIe) ，用于将系统连接到一个或多个 GPU。服务器有多达8个加速器，通常连接在一个先进的拓扑结构，桌面系统有1-2，取决于用户的预算和电源供应的大小。
* 在许多情况下，使用 PCIe 总线连接的持久存储器(如磁性硬盘驱动器(HDD)、固态硬盘(SSD))提供了将训练数据有效传输到系统并根据需要存储中间检查点的功能。

图12.4.2元件的连接性

如图12.4.2所示，大多数组件(网络、 GPU、存储器)通过 PCI Express 总线连接到 CPU。它由直接连接到 CPU 的多个通道组成。例如，AMD 的 Threadripper 3有64个 PCIe 4.0通道，每个通道都能在两个方向上进行16 Gbit/s 的数据传输。内存直接连接到 CPU，总带宽最高可达100 GB/s。

当我们在计算机上运行代码时，我们需要将数据转移到处理器(CPU 或 GPU) ，执行计算，然后将结果从处理器移回 RAM 和持久存储器。因此，为了获得良好的性能，我们需要确保这个系统能够无缝地工作，而不会使任何一个系统成为主要的瓶颈。例如，如果我们加载图像的速度不够快，处理器将不会有任何工作要做。同样地，如果我们不能快速地将矩阵移动到 CPU (或 GPU) ，那么它的处理元素就会饿死。最后，如果我们想要跨网络同步多台计算机，后者不应该减慢计算速度。一种选择是交错的通信和计算。让我们更详细地看看各个组成部分。

## 存储

我们看到RAM的一些关键特征是带宽和延迟。存储设备也是如此，只是差异可能更大。

硬盘已经使用了半个多世纪。简而言之，它们包含许多带有头部的旋转盘片，可以将其放置在任何给定的轨道上进行读取/写入。高端磁盘在9个盘片上最多可容纳16TB。HDD的主要优势之一是它们相对便宜。它们的许多缺点之一是其典型的灾难性故障模式和相对较高的读取延迟。

要了解后者，请考虑以下事实：硬盘的转速约为7200 RPM。如果它们快得多，它们将由于作用在盘片上的离心力而破碎。在访问磁盘上的特定扇区时，这有一个主要的缺点：我们需要等待，直到磁盘旋转到位（我们可以移动磁头，但不能加速实际磁盘）。因此，可能需要8毫秒以上的时间，直到请求的数据可用。表示这种情况的常见方式是说HDD可以以大约100 IOP的速度运行。在过去的二十年中，这个数字基本上保持不变。更糟糕的是，增加带宽同样困难（大约100-200 MB/s）。毕竟，每个磁头都读取一条位的轨迹，因此位速率仅与信息密度的平方根成比例。结果，HDD很快就被降级为大型数据集的档案存储和低级存储。

固态驱动器使用闪存来持久存储信息。这样可以更快地访问存储的记录。现代SSD可以以100,000至500,000 IOP的速度运行，即，比HDD快3个数量级。此外，它们的带宽可以达到1-3GB / s，即比HDD快一个数量级。这些改进听起来实在太令人难以置信了。实际上，由于SSD的设计方式，它们带有许多警告。

* ssd以块(256 KB或更大)形式存储信息。它们只能作为一个整体来写，这需要大量的时间。因此，在SSD上逐位随机写操作的性能非常差。同样地，写入数据通常会花费大量时间，因为必须读取、擦除数据块，然后使用新信息重写数据块。到目前为止，SSD控制器和固件已经开发出算法来缓解这一问题。尽管如此，写操作可能会慢得多，特别是对于QLC(四层单元格)ssd。提高性能的关键是维护一个操作队列，尽可能使用大的块进行读写。
* ssd中的内存单元消耗得相对较快(通常在几千次写操作之后就已经消耗掉了)。磨损级保护算法能够将退化扩散到多个单元。也就是说，不建议对交换文件或大型日志文件聚合使用ssd。
* 最后，带宽的大量增加迫使计算机设计者将ssd直接附加到PCIe总线上。能够处理这种情况的驱动器称为NVMe(增强的非易失性内存)，最多可以使用4个PCIe通道。这相当于高达8GB/s在PCIe 4.0。

云存储提供了可配置的性能范围。也就是说，给虚拟机分配的存储是动态的，无论是在数量上还是在速度上，都是由用户选择的。我们建议用户在等待时间太长时增加IOPs的数量，例如在使用许多小记录进行训练时。

## 缓存

考虑以下情况：如上图12.4.3所示，我们有一个中等的CPU内核，带有4个内核，以2GHz的频率运行。此外，让我们假设我们的IPC（每个时钟指令）计数为1，并且单元具有启用了256位宽度的AVX2。此外，让我们假设至少需要从内存中检索用于AVX2操作的寄存器之一。这意味着每个时钟周期CPU消耗4x256bit = 1kbit的数据。除非我们能够每秒将2⋅109⋅128=256⋅1092⋅109⋅128=256⋅109字节传输到处理器，否则处理元素将挨饿。不幸的是，这种芯片的存储器接口仅支持20-40 GB / s的数据传输，即少一个数量级。解决方法是避免尽可能从内存中加载新数据，而是将其本地缓存在CPU上。这是缓存派上用场的地方（有关入门，请参阅此Wikipedia文章）。通常使用以下名称/概念：

* 严格来说，寄存器不是缓存的一部分。他们帮助分阶段说明。也就是说，CPU寄存器是CPU可以以时钟速度访问而不会造成任何延迟损失的存储位置。CPU具有数十个寄存器。由编译器（或程序员）有效使用寄存器。例如，C编程语言具有一个注册关键字。
* L1缓存是针对高内存带宽要求的第一道防线。L1缓存很小（典型的大小可能是32-64kB），并且经常分为数据和指令缓存。在L1中找到数据时，访问速度非常快。如果在此处找不到它，则搜索将向下进行缓存层次结构。
* L2缓存是下一站。根据架构设计和处理器大小，它们可能是排他性的。它们只能由给定的内核访问，也可以在多个内核之间共享。L2缓存比L1更大（通常每个核心256-512kB），并且速度较慢。此外，要访问L2中的内容，我们首先需要检查以了解数据不在L1中，这会增加少量额外的延迟。
* L3缓存在多个内核之间共享，并且可能很大。AMD的Epyc 3服务器CPU跨多个小芯片具有高达256MB的缓存。更典型的数字在4-8MB范围内。

预测下一步将需要哪些存储元件是芯片设计中的关键优化参数之一。例如，由于大多数缓存算法将尝试向前读取而不是向后读取，因此建议朝前方向遍历内存。同样，将内存访问模式保持在本地也是提高性能的好方法。添加缓存是一把双刃剑。一方面，它们确保处理器内核不会出现数据不足的情况。同时，它们增加了芯片尺寸，占用了本来可以用于增加处理能力的面积。此外，高速缓存未命中可能代价很高。考虑最坏的情况，如图12.4.6所示。当处理器1上的线程请求数据时，会将内存位置缓存在处理器0上。要获得此信息，处理器0需要停止正在执行的操作，将信息写回到主存储器，然后让处理器1从内存中读取信息。在此操作期间，两个处理器都将等待。与有效的单处理器实现相比，此类代码在多个处理器上运行的速度可能会更慢。这是为什么缓存大小（实际大小除外）受到实际限制的另一个原因。

图12.4.6虚假共享（图片由Intel提供）

## GPU和其他加速器

声称没有GPU不会成功进行深度学习并不夸张。同样，可以说，由于深度学习，GPU制造商的财富已大大增加。硬件和算法的这种共同发展导致了这样一种情况，对于深度学习而言，更好或更差的深度学习是首选的统计建模范例。因此，有必要了解GPU和相关加速器（例如TPU）的特定好处[Jouppi等，2017]。

值得注意的是在实践中经常会做出这样的区分：加速器针对训练或推理进行了优化。对于后者，我们只需要计算网络中的前向传播。反向传播不需要存储中间数据。此外，我们可能不需要非常精确的计算（FP16或INT8通常就足够了）。另一方面，在训练期间，所有中间结果都需要存储以计算梯度。此外，累积梯度需要更高的精度，以避免数值下溢（或上溢）。这意味着FP16（或FP32的混合精度）是最低要求。所有这些都需要更快，更大的内存（HBM2与GDDR6）和更大的处理能力。例如，NVIDIA的Turing T4 GPU已针对推理进行了优化，而V100 GPU更适合进行培训。

回顾图12.4.5。将矢量单元添加到处理器内核可以使我们显着提高吞吐量（在该图中的示例中，我们能够同时执行16个操作）。如果我们添加的操作不仅优化了向量之间的运算，还优化了矩阵之间的运算，该怎么办？ 这种策略导致了Tensor Cores（稍后会详细介绍）。其次，如果我们增加更多的内核怎么办？ 简而言之，这两种策略总结了GPU中的设计决策。图12.4.7给出了基本处理块的概述。它包含16个整数和16个浮点单元。除此之外，两个Tensor核心可加速与深度学习相关的附加操作的一小部分。每个流式多处理器（SM）均由四个此类模块组成。

最后值得一提的是TensorCores。它们是最近增加对深度学习特别有效的优化电路趋势的一个例子。例如，TPU添加了一个收缩数组[Kung, 1988]用于快速矩阵乘法。这里的设计是支持非常小的数量(第一代TPUs的一个)的大型操作。另一端是肌腱核。根据它们的数值精度，它们被优化用于4x4和16x16矩阵之间的小操作。图12.4.9给出了优化的概述。

图12.4.9图灵中的英伟达TensorCores(由英伟达提供的图像)

显然，在优化计算时，我们最终会做出一定的妥协。其中之一就是GPU不太擅长处理中断和稀疏数据。但也有值得注意的例外，如Gunrock [Wang et al.， 2016]，稀疏矩阵和向量的访问模式并不适合GPU擅长的高带宽突发读取操作。匹配这两个目标是一个活跃的研究领域。例如DGL，一个为深入学习图形而调优的库。

## 网络和总线

每当单个设备不足以进行优化时，我们都需要与之进行数据传输以同步处理。这是网络和总线派上用场的地方。我们有许多设计参数：带宽，成本，距离和灵活性。一方面，我们拥有范围相当广的WiFi，它非常易于使用（毕竟没有电线），价格便宜，但它提供了相对中等的带宽和延迟。机器学习研究人员在他们的正确思维范围内不会使用它来构建服务器集群。接下来，我们将重点介绍适合于深度学习的互连。

* PCIe是专用总线，用于每个通道的超高带宽点对点连接（在PCIe 4.0上高达16 Gbs）。延迟约为个位数微秒（5μs）。PCIe链接非常宝贵。处理器数量有限：AMD的EPYC 3有128条通道，英特尔的至强处理器每芯片最多48条通道； 在台式机级CPU上，数字分别为20（Ryzen 9）和16（Core i9）。由于GPU通常具有16条通道，因此限制了可以全带宽连接到CPU的GPU数量。毕竟，他们需要与其他高带宽外围设备（如存储和以太网）共享链接。就像使用RAM访问一样，由于减少了数据包开销，因此大批量传输是可取的。
* 以太网是连接计算机的最常用方法。尽管它比PCIe慢得多，但安装起来却非常便宜且具有弹性，并且覆盖的距离更长。低级服务器的典型带宽为1 GBit / s。高端设备（例如，云中的C5实例）提供10至100 GBit / s的带宽。与所有以前的情况一样，数据传输会产生大量开销。请注意，我们几乎永远不会直接使用原始以太网，而是使用在物理互连之上执行的协议（例如UDP或TCP / IP）。这增加了额外的开销。像PCIe一样，以太网被设计为连接两个设备，例如计算机和交换机。
* 交换机允许我们以任何一对设备可以同时进行（通常为全带宽）点对点连接的方式连接多个设备。例如，以太网交换机可能以高横截面带宽连接40台服务器。请注意，交换机并不是传统计算机网络所独有的。甚至可以切换PCIe通道。发生这种情况的原因是，例如，将大量GPU连接到主机处理器，就像P2实例一样。
* 对于超高带宽互连，NVLink是PCIe的替代产品。每个链路提供高达300 Gbit / s的数据传输速率。服务器GPU（Volta V100）具有6个链接，而消费级GPU（RTX 2080 Ti）仅具有1个链接，并以降低的100 Gbit / s速率运行。我们建议使用NCCL在GPU之间实现高数据传输。

## 小结

* 设备有运营费用。因此，重要的是要瞄准小数目的大转移，而不是许多小的。这适用于RAM、ssd、网络和GPU。
* 向量化是性能的关键。确保您了解加速器的特定功能。例如，一些Intel Xeon CPU特别适合INT8操作，NVIDIA Volta GPU擅长FP16矩阵-矩阵操作，NVIDIA图灵擅长FP16, INT8和INT4操作。
* 由于较小的数据类型而导致的数字溢出可能会在培训期间成为问题(在推断期间也会在较小的程度上成为问题)。
* 混叠会显著降低性能。例如，64位CPU上的内存对齐应该按照64位边界进行。在GPU上，保持卷积大小对齐是一个好主意，例如，与张力核对齐。
* 将算法与硬件匹配(内存占用、带宽等)。极大的加速(数量级)可以实现时，拟合参数到缓存。
* 我们建议在验证实验结果之前，先在纸上勾勒出一种新算法的性能。数量级或数量级以上的差异是值得关注的原因。
* 使用分析器调试性能瓶颈。
* 培训和推理硬件在价格/性能方面有不同的甜点。

## 更多的延迟数

表12.4.1和表12.4.2中的摘要是由Eliot Eshelman提供的，他在GitHub中维护了数字的更新版本。

TODO:TABLE

表12.4.2 NVIDIA Tesla GPU的延迟编号

TODO:TABLE

## 练习

1. 编写C代码以测试访问相对于外部存储器接口对齐或未对齐的存储器之间的速度是否存在差异。提示：注意缓存效果。
1. 测试顺序访问内存或以给定跨度访问内存之间的速度差异。
1. 您如何测量CPU上的缓存大小？
1. 您如何跨多个存储通道布置数据以获得最大带宽？ 如果您有许多小线程，您将如何布置？
1. 企业级硬盘的转速为10,000 rpm。HDD在读取数据之前需要花费最坏情况的绝对最短时间是多少（您可以假设磁头几乎是瞬时移动的）？ 为什么2.5英寸HDD在商用服务器（相对于3.5英寸和5.25英寸驱动器）中变得越来越流行？
1. 假设HDD制造商将存储密度从每平方英寸1 Tbit增加到每平方英寸5 Tbit。您可以在2.5英寸HDD的环上存储多少信息？ 内轨道和外轨道之间有区别吗？
1. AWS P2实例具有16个K80开普勒GPU。在p2.16xlarge和p2.8xlarge实例上使用lspci，以了解GPU如何连接到CPU。提示：请注意PCI PLX桥。
1. 从8位数据类型变为16位数据类型会使硅的数量大约增加4倍。为什么？ NVIDIA为什么要在其Turing GPU中添加INT4操作。
1. 给定GPU之间的6条高速链接（例如Volta V100 GPU），您将如何连接其中8条？ 查找在P3.16xlarge服务器中使用的连接性。
1. 通过内存向前读取与向后读取相比快多少？ 不同的计算机和CPU供应商之间的数字是否有所不同？ 为什么？ 编写C代码并进行试验。
1. 您可以测量磁盘的缓存大小吗？ 典型的硬盘有什么用？ SSD是否需要缓存？
1. 在通过以太网发送消息时，测量数据包开销。查找UDP和TCP / IP连接之间的区别。
1. Direct Memory Access（直接内存访问）允许除CPU以外的其他设备直接向（从）内存写入（和读取）。为什么这是个好主意？
1. 查看Turing T4 GPU的性能数据。当您从FP16升级到INT8和INT4时，为什么性能“仅”会翻倍？
1. 在旧金山和阿姆斯特丹之间往返时最短的时间是什么？ 提示：您可以假设距离为10,000 km。
