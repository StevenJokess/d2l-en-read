

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-09-13 20:33:44
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-09-13 20:33:46
 * @Description:http://preview.d2l.ai/d2l-en/master/chapter_recurrent-neural-networks/bptt.html
 * @TODO::
 * @Reference:
-->
5 replies
10 Jul
witx
Correct me if i’m wrong, in (8.7.11), looks like the tanh activation was omitted, shouldn’t it be like below?

z[t] = W_hh . h[t-1] + W_hx . x[t] + b
h[t] = tanh(z[t])
Thus in (8.7.16), there is also a missing component of tanh derivative for each term in the sum which is product of (1 - tanh^2(z[i])) for i from t - j + 1 to t, i.e.

(1 - h[t]^2)(1 - h[t - 1]^2)(1 - h[t - 2]^2) … (1 - h[t - j + 1]^2)
For reference, in Deepmind x UCL lecture on RNN, there is a similar formula with tanh component.

1 reply
10 Jul▶ witx
goldpiggy
Hi @witx, great catch on the tanh function. We will fix it!

As for the missing bias term, we omit it for the sake of simplicity (similarly to https://d2l.ai/chapter_multilayer-perceptrons/backprop.html). You can think W = [b, w1,w2,…], and X = [1, x1, x2, …].

26 Aug
Sharmi_​​Banerjee
Hello,

Please correct me if I am wrong: in (8.7.16) if j runs from 1 to (t-1), shouldn’t the index be h_(j-1) instead of h_j from the previous equations (8.7.11). Would really appreciate the help!

1 reply
26 Aug▶ Sharmi_Banerjee
goldpiggy
Hi @Sharmi_Banerjee, great catch! Would you like to be a contributor and post a PR for this at github?

26 Aug
Sharmi_​​Banerjee
Sure I would love to. Submitted a PR. Thank you!

Continue Discussion
