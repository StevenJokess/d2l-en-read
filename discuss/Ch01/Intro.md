

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-09-13 21:24:01
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-09-13 21:24:11
 * @Description:http://preview.d2l.ai/d2l-en/master/chapter_introduction/index.html
 * @TODO::
 * @Reference:
-->
Continue Discussion
16 replies
9 Jun
manuel-â€‹arno-â€‹korfmann
Others (like error rate) are difficult to optimize directly, owing to non-differentiability or other complications. In these cases, it is common to optimize a surrogate objective

Itâ€™s not quite clear to me from reading what exactly is meant with â€œerror rateâ€. I think it would be great if an example could be given.

1 reply
9 Junâ–¶ manuel-arno-korfmann
goldpiggy
Hi @manuel-arno-korfmann, â€œerror rateâ€ means â€œhow much mistake the model makesâ€. Is that more clear?

3 replies
9 Junâ–¶ goldpiggy
syedmechâ€‹47
I am still unable to understand error rate.
â€œHow much mistake to model makesâ€ is not clear enough, did you mean how much mistake â€˜theâ€™ model makes, which is L1 distance(y-yÂ¹).
Also can you please explain what is surrogate objective?

1 reply
9 Jun
manuel-â€‹arno-â€‹korfmann
Iâ€™m having a difficult time understanding

Hence, the loss ğ¿L incurred by eating the mushroom is ğ¿(ğ‘=eat|ğ‘¥)=0.2âˆ—âˆ+0.8âˆ—0=âˆL(a=eat|x)=0.2âˆ—âˆ+0.8âˆ—0=âˆ, whereas the cost of discarding it is ğ¿(ğ‘=discard|ğ‘¥)=0.2âˆ—0+0.8âˆ—1=0.8L(a=discard|x)=0.2âˆ—0+0.8âˆ—1=0.8.

Is it possible to explain it in more depth via 1 or 2 paragraphs?

9 Junâ–¶ goldpiggy
manuel-â€‹arno-â€‹korfmann
Ok, so a person in the reading group explained that the error rate is the accumulated loss for all examples, is that correct?

1 reply
10 Junâ–¶ syedmech47
goldpiggy
Hey @syedmech47, Sorry for the typo here. Yes you got the idea here - the error rate is to measure the distance between y (the truth) and the $\hat{y}$ (the estimate). However the measurement metrics (which measure the error) does not limit to L1 distance, but also can accuracy, precision, recall, f1, etc.

A surrogate is a function that approximates an objective function. There are lots of measurement metrics are not differentiable (like f1 etc.), hence we need some other functions (i.e., the loss function ) to approximate the objective function.

Let me know if this is clear enough!

10 Junâ–¶ manuel-arno-korfmann
goldpiggy
It can be the accumulated loss, or average loss. It doesnâ€™t make a lot difference here for optimization.

10 Jun
syedmechâ€‹47
Thanks a lot. It totally made sense.

Side Note: I just want to thank each and every personâ€™s effort in making this wonderful resource open for all and also providing such wonderful support through discussion forums.

1 reply
10 Junâ–¶ syedmech47
goldpiggy
Fantastic! Itâ€™s our pleasure to enable more talents learn, apply and benefit from deep learning!

1 Aug
manuel-â€‹arno-â€‹korfmann
The last exercise mentions â€œthe end-to-end learning approachâ€, but it is nowhere explained in the section what is â€œend-to-end learningâ€.

1 reply
3 Augâ–¶ manuel-arno-korfmann
goldpiggy
Great call @manuel-arno-korfmann. I suspect it refers to " Fig. 1.1.2 A typical training process".

16 Augâ–¶ goldpiggy
zeuslawyer
Hi @goldpiggy, iâ€™m reading this thread and Iâ€™d like to further clarify. My understanding is:

there is a difference between error rate and cost/loss function

error rate is the number of errors in predictions for a given set of inputs X. Perhaps its the total number of errors divided by the total examples in the input data?

the loss function is a quantification of the â€œdistanceâ€ between right and wrong predictions, which is different from the error rate which is percentage of errors as described in 2 above?

Thank you for these resources and your guidance.

2 replies
23 Augâ–¶ zeuslawyer
meetashok
@zeuslawyer

My understanding is as follows -

Loss function and cost functions (in this context) mean the same thing
Loss functions are a family of functions that could be relevant for a problem. For classification problems, the error rate is one such loss function. But as it isnâ€™t differentiable. So, cross-entropy is used as a surrogate loss function
Ashok

23 Aug
meetashok
Tailors have developed a small number of parameters that describe human body shape fairly accurately for the purpose of fitting clothes. These problems are referred to as subspace estimation problems. If the dependence is linear, it is called principal component analysis.

In the above sentence, what is the word dependence referring to - dependence between what and what?

Ashok

1 reply
24 Aug
goldpiggy
 zeuslawyer:
there is a difference between error rate and cost/loss function

yes.

 zeuslawyer:
error rate is the number of errors in predictions for a given set of inputs X. Perhaps its the total number of errors divided by the total examples in the input data?

yes, we also referred it as the average error rate

 zeuslawyer:
the loss function is a quantification of the â€œdistanceâ€ between right and wrong predictions, which is different from the error rate which is percentage of errors as described in 2 above?

Sometimes this two can be the same if error rate function is differentiable. While most of the time, it is not differentiable. For example, most of the classification error rate function is not differentiable, so we use a loss function. Check more details here.

24 Augâ–¶ meetashok
goldpiggy
Hi @meetashok, great question! The dependence between the principal components and the original data. So PCA transforms the data to a new coordinate system (ordering by the principal components) by orthogonal linear transformation. It tries to capture and recreate the new features from the data.

Continue Discussion
