

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-09-13 20:01:48
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-09-13 20:02:56
 * @Description:https://discuss.d2l.ai/u/stevenjokes/activity/replies
 * @TODO::
 * @Reference:
-->

D2L Discussion
Sign Up
Log In

StevenJokes
https://github.com/StevenJokes
Summary
Activity
Badges
All
Topics
Replies
Likes
2 days
Auto Differentiation
mxnet
You can try pytorch code too. It will work.

2 days
Single Shot Multibox Detection (SSD)
mxnet
@iotsharing_dotcom can‚Äôt understand you‚Ä¶ Do you mean latexÔºü

2 days
Auto Differentiation
mxnet
@asadalam For your first question, the reason is that in your code, you didn‚Äôt use anything related to pytorch. So it is unnecessary to import torch For your second question, https://mxnet.apache.org/versions/1.6/api/python/docs/api/autograd/index.html#mxnet.autograd.backward Check for source c‚Ä¶

2 days
Do these before you ask!
d2l-en
mxnet new discussion here: https://github.com/apache/incubator-mxnet/discussions?page=1 [image] https://stackoverflow.com/help/asking is the right way to ask question. http://www.catb.org/~esr/faqs/smart-questions.html

2 days
Auto Differentiation
mxnet
@asadalam You can use [image] or a github URLto show code‚Ä¶ And we should aviod import torch and mxnet at the same time‚Ä¶ It is confusing‚Ä¶

2 days
Fully Convolutional Networks (FCN)
mxnet
[image] ‚Ä¶

2 days
Neural Style Transfer
mxnet
For colab users: Look this first: http://preview.d2l.ai/d2l-en/master/chapter_appendix-tools-for-deep-learning/colab.html Follow my discussion to download all colab code: Using Google Colab Then follow my code: https://github.com/StevenJokes/D2L_enread/blob/master/Ch13_CV/neural_style.ipynb [i‚Ä¶

2 days
Image Classification (CIFAR-10) on Kaggle
mxnet
I trained this in colab 1st try: [image] 2st try: [image] Why can‚Äôt I find train loss here? 1st try: [image] 2nd try: [image] Why can‚Äôt I find valid loss? On the book, [image] Both acc disappeared. Why?

3 days
Auto Differentiation
mxnet
@rammy_vadlamudi chain rule

3 days
Calculus
pytorch
Discussion is the only way now. @rammy_vadlmudi

3 days
Installation
pytorch
@kuil google first‚Ä¶ And convert your language to English next time

4 days
Generative Adversarial Networks
pytorch
GAN: [image] DCGAN: [image] I forgot the book name‚Ä¶ Too many books I have read‚Ä¶ I found „ÄäÊ∑±ÂÖ•ÊµÖÂá∫PyTorchÔºö‰ªéÊ®°ÂûãÂà∞Ê∫êÁ†Å„Äãwriten by Âº†Ê†°Êç∑ GAN: https://github.com/zxjzxj9/PyTorchIntroduction/blob/master/Chapter4/gan.py 4.7 ÁîüÊàêÊ®°ÂûãÔºöVAEÂíåGAN ÂèØ‰ª•ÁúãÂà∞ÔºåÁîüÊàêÂô®ÁöÑÊçüÂ§±ÂáΩÊï∞ÁöÑÁõÆÁöÑÊòØËÆ©ÁîüÊàêÂô®ÁöÑËæìÂá∫Âú®Âà§Âà´Âô®‰∏äËæìÂá∫ÁöÑÊ¶ÇÁéáÂ∞ΩÂèØËÉΩÂ§ßÔºåÂà§Âà´Âô®ÁöÑÊçüÂ§±ÂáΩÊï∞ÁõÆÁöÑÊòØËÆ©ÁúüÂÆûÊï∞ÊçÆÂú®Âà§Âà´Âô®‰∏äËæìÂá∫ÁöÑÊ¶ÇÁéáÂ∞ΩÂèØ‚Ä¶

Sep 6
Installation
tensorflow
Open your cmd(win+r then cmd) Do next [image]

Sep 5
Padding and Strides
pytorch
@Nicholas_Kim I guess it is. I‚Äôm not sure about it.

Sep 5
On building the latest version (1.7.0) of mxnet with cuda 11 from source
mxnet
I agree with you. Already told the developer.

Sep 5
Training on Multiple GPUs
mxnet
Why? I think LR should increase to catch up with batch size increasing.

Sep 5
Softmax Regression from Scratch
mxnet
@gpk2000 You can test by your own next time‚Ä¶ Just try to input some numbers to see the outputs‚Ä¶ And use the search button as possible. [image] http://preview.d2l.ai/d2l-en/master/chapter_appendix-mathematics-for-deep-learning/information-theory.html?highlight=cross%20entropy%20loss [image] ‚Ä¶

Sep 5
Concise Implementation of Multilayer Perceptron
mxnet
I guess it is caused by testing what you haven‚Äôt trained well accidently‚Ä¶ @goldpiggy

Sep 5
Weight Decay
mxnet
code please. @rezahabibi96

Sep 5
Model Selection, Underfitting and Overfitting
mxnet
@rezahabibi96 You are right about it. And [image] Check here: http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/gan.html We can use init.Normal() to initialize the parameter @goldpiggy check my answer‚Ä¶

Sep 4
Generative Adversarial Networks
pytorch
http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/gan.html The loss of discriminator is better to be bigger. The loss of generator is better to be smaller. I can‚Äôt figure out a question: https://stackoverflow.com/questions/63763835/why-is-my-loss-of-generator-0-why-is-l‚Ä¶

Sep 4
On building the latest version (1.7.0) of mxnet with cuda 11 from source
mxnet
As I know, cuda 11 is not supported now! Am I right ? [image] @goldpiggy There is a way to build mxnet: https://mxnet-tqchen.readthedocs.io/en/latest/how_to/build.html @Chun_Li I can‚Äôt garantee it is useful‚Ä¶ From my message from a developer, I knew that version 2.0 will support cuda11‚Ä¶

Sep 4
Implementation of Multilayer Perceptron from Scratch
pytorch
[image] let stddev = 0.1 @Abinash_Sahu

Sep 4
How in depth is this book?
d2l-book
From my experience, I don‚Äôt think the book is deep‚Ä¶ You need to dive into by your hand, not by just asking. @kusur‚Ä¶michaelnielsen, I just knew this guy‚Ä¶ Famous‚Ä¶

Sep 4
Weight Decay
pytorch
‚Ä¶still can‚Äôt understand ‚Äúthe affine aspect‚Äù. Some related papers? @kusur

Sep 4
The newest book and d2l package
d2l-en
Sorry, you shouldn‚Äôt delete it. Only try flag‚Ä¶

Sep 3
Deep Convolutional Generative Adversarial Networks
pytorch
I know it has been merged. I don‚Äôt know when it will show I think it needs to be optim. Usually pytorch is quicker than mxnet. Help me if you can. And find some ways to delete [image] @yoderj BTW, I‚Äôm translating it to tensorflow stuck one night‚Ä¶

Sep 3
Weight Decay
pytorch
[image] Is it? @kusur

Sep 3
Document
tensorflow
[image] [image]

Sep 3
Weight Decay
pytorch
Why ‚Äúthe network would end up learning a very small value of bias term‚Äù? What does your ‚Äúlambda‚Äù mean?

Aug 31
Linear Regression from Scratch
pytorch
show your code next time. @oliver

Aug 31
A problem in chapter 3.2, linear regression implementation
pytorch
Ask behind the chapter please. @oliver

Aug 30
Recurrent Neural Networks
pytorch
No need‚Ä¶

Aug 30
Deep Convolutional Generative Adversarial Networks
pytorch
now it is not released. http://preview.d2l.ai/d2l-en/PR-1309/chapter_generative-adversarial-networks/dcgan.html merge now! http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/dcgan.html

Aug 30
Deep Convolutional Generative Adversarial Networks
mxnet
It was my code I forget to add [image] The code didn‚Äôt not stop. Only running. I‚Äôm so excited to tell you and thanks for your code too. Now you can check http://preview.d2l.ai/d2l-en/PR-1309/chapter_generative-adversarial-networks/dcgan.html It runs well. @yoderj If you like to research, my GA‚Ä¶

Aug 29
Deep Convolutional Generative Adversarial Networks
mxnet
epoch is too small? Why so slow? You need to train more epochs by GPU. It is quicker. And thanks for your code BTW. I think I already figure DCGAN out. [image]

Aug 29
Deep Convolutional Generative Adversarial Networks
mxnet
OK. I‚Äôll learn. What is ‚Äúit‚Äù you talked about? Do you have ipynb file? It is easy to watch the result of every step. I tried to translate but failed. You can take a look at this PR.

Aug 28
Softmax Regression
d2l-en
@Abinash_Sahu l (yÔºåy _ hat) Cross entropy loss Only one type of these losses we often use. https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html [image]

Aug 28
Softmax Regression
d2l-en
[image] The green thing is same.

Aug 27
Gated Recurrent Units (GRU)
mxnet
Now you have it! @huangxiaoshuo

Aug 27
Implementation of Multilayer Perceptron from Scratch
pytorch
@Gavin For a standard normal distribution (i.e. mean=0 and variance=1 ), you can use torch.randn For your case of custom mean and std , you can use torch.normal

Aug 26
Installation
pytorch
@Shashwat You‚Äôre trying to run the pytorch code that we don‚Äôt have now‚Ä¶ Please look this for more details.

Aug 26
Installation
tensorflow
@melo :rofl: I‚Äôm new to pytorch and mxnet too. Then I have translated GAN from pytorch and mxnet. If you follow the book step by step, I think it won‚Äôt be hard for you. And I found Chapter 17. Representation Learning and Generative Learning Using Autoencoders and GANs: https://learning.oreilly.com‚Ä¶

Aug 26
Installation
tensorflow
@melo You can check the chapter DCGAN, then you will find that there isn‚Äôt tensorflow code now. :sweat_smile: You can join us to translate from mxnet to tensorflow. I‚Äôm translating DCGAN from mxnet to pytorch, but still stuck.

Aug 25
Softmax Regression from Scratch
pytorch
Give the helpful reply a love. It will make forum more active. :blush:

Aug 25
Softmax Regression from Scratch
pytorch
@Gavin Have you googled it? You can try pip uninstall numpy, then pip install -U numpy from https://www.youtube.com/watch?reload=9&v=RhpkTBvb-WU If you have any other questions, try to solve it by googling it. If you still have problem, then publish all your code or give me a github URL, and‚Ä¶

Aug 25
Networks Using Blocks (VGG)
pytorch
@Nish Wow, I learnt a lot from your github and know I need to learn more. Have stared your repo. Keep going and communicating. :rofl:

Aug 24
Deep Recurrent Neural Networks
@astonzhang you forget to add tag

Aug 24
Softmax Regression
d2l-en
@goldpiggy. ok‚Ä¶ just log [image] to [image]

Aug 24
Single Shot Multibox Detection (SSD)
mxnet
where do you run the code? You need to download img in github.com/d2l-ai/d2l-en [image] Then put it like this. [image] @manuel-arno-korfmann

Aug 23
Use GPUs
mxnet
https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html

Aug 23
Networks Using Blocks (VGG)
pytorch
Can you publish all your code, and throw me a github URL? I still wonder the part that you don‚Äôt show. @Nish

Aug 23
Softmax Regression
d2l-en
Really? @Gavin [image] What is it related to [image] ? Could you explain it?

Aug 23
Deep Convolutional Neural Networks (AlexNet)
pytorch
I‚Äôll try. @Nish Thanks.

Aug 22
Installation
mxnet
Ha, I found an cute user. If you are Josiah Yoder, please tell me it‚Äôs ok to publish it

Aug 22
Softmax Regression
d2l-en
@goldpiggy The simple answer seems to be Tautology. I have read URL you give. But I think it didn‚Äôt solve this question. I can‚Äôt find anything in it.

Aug 22
`pip search d2l` These packages' function
Thanks. Why can‚Äôt I find these github repo by googling? oh I can search these in https://pypi.org/project/ https://pypi.org/project/quizmake/ [image] d2lmf: https://pypi.org/project/d2lmf/ [image] d2lmf looks like related to d2l course, but it isn‚Äôt.

Aug 21
Deep Convolutional Generative Adversarial Networks
mxnet
[image] Why do we need /2? The code I‚Äôm contrasting with is from https://aws.amazon.com/cn/blogs/china/easily-build-pytorch-generative-adversarial-networks-gan/ loss_real‰∏∫ÁúüÂÆûÊ†∑Êú¨ÁöÑÈâ¥Âà´Âô®ÊçüÂ§±(ÈôÑÂä†ÂÖ∂ËÆ°ÁÆóÂõæ)Ôºåloss_fake‰∏∫ËôöÂÅáÊ†∑Êú¨ÁöÑÈâ¥Âà´Âô®ÊçüÂ§±(ÂèäËÆ°ÁÆóÂõæ)„ÄÇPyTorchËÉΩÂ§ü‰ΩøÁî®+ËøêÁÆóÁ¨¶Â∞ÜËøô‰∫õÂõæÂΩ¢ÁªÑÂêàÊàê‰∏Ä‰∏™ËÆ°ÁÆóÂõæÂΩ¢„ÄÇÁÑ∂ÂêéÊàë‰ª¨Â∞ÜÂèçÂêë‰º†Êí≠ÂíåÂèÇÊï∞Êõ¥Êñ∞Â∫îÁî®Âà∞ÁªÑÂêàËÆ°ÁÆóÂõæ„ÄÇ https://blog.csdn.net/m‚Ä¶

Aug 21
Overview of Recommender Systems
d2l-en
Exercise: I‚Äôm a Chinese user. I use Zhihu‚Äôs recommender system to get the news about your new version‚Äôs book. domestic animal mating. :rofl:

Aug 21
Softmax Regression
d2l-en
@goldpiggy, I can‚Äôt understand too. :sweat_smile:

Aug 21
Can i print the book for personal use?
d2l-en
You can print mxnet. Now you still don‚Äôt have tensorflow codes. :sweat_smile: I still suggest not reading, but running the code and trying to change them. Then you maybe understand more.

Aug 21
`pip search d2l` These packages' function
But I need more details about them‚Ä¶

Aug 21
`pip search d2l` These packages' function
goldmermaid commented 11 hours ago Hey @StevenJokes great questions and answers! :wink: I suggest you to ask the similar questions on the forum since the question will valuable to other readers. Please close the issue if there is no other doubt.

Aug 20
[Information Theory] Potential issue with part 3 of 3rd exercise in the section 18.11.7
d2l-en
Sorry. I can‚Äôt reply you. @goldpiggy is the author. Maybe she can. Maybe we need to open more new topics for remaining chapters ASAP to avoid these type to happen. @kusur After we created that matching discussion, please ask your question again in matching discussion. Thanks for your patience.

Aug 20
Deep Convolutional Neural Networks (AlexNet)
pytorch
@Nish I have known it. But 12 hours will disconnect. :mask: And you can find why colab is not so friendly in my discussion: http://d2l.ai/chapter_appendix-tools-for-deep-learning/colab.html And there is an issue of python 3.6:

Aug 19
Installation
pytorch
I still can‚Äôt find your real problem. Have you run other .ipynb that is not in this book files before? @Shashwat But there is what you can try: update your jupyter read again: https://d2l.ai/chapter_appendix-tools-for-deep-learning/jupyter.html [image] Have you run jupyter notebook first, then‚Ä¶

Aug 19
Deep Convolutional Neural Networks (AlexNet)
pytorch
@ChenYangyao thank for your reply. I don‚Äôt have gpus. I‚Äôm using colab for learning. And now I can‚Äôt find any exercitations because of my finance undergraduate diploma.

Aug 19
Implementation of Multilayer Perceptron from Scratch
pytorch
@ccpvirus We use softmax to calculate probablility first, and then find the max probabillity one.

Aug 17
Implementation of Multilayer Perceptron from Scratch
pytorch
What‚Äôs your IDE? @Xiaomut I am using vscode, and I can‚Äôt find this. @Kushagra_Chaturvedy

Aug 17
Run Codes on Spyder Environment
I think the thing is that he didn‚Äôt want to install jupyter.

Aug 17
Use GPUs
pytorch
awsblog ‚Äì Áé©ËΩ¨GPUÂÆû‰æã‰πãÁ≥ªÁªüÂ∑•ÂÖ∑ ‚Äì NVIDIA ÁØá

Aug 17
Bug reporting
Site Feedback
It didn‚Äôt happen in my experience‚Ä¶ Please more detail. [image]

Aug 17
Run Codes on Spyder Environment
I don‚Äôt recommend you to run code on Spyder. Jupyter Notebook is more easy and convient to run unit test for code. http://preview.d2l.ai/d2l-en/master/chapter_appendix-tools-for-deep-learning/jupyter.html For me, I like vscode which is open source and so powerful. But if you like Spyder, you can‚Ä¶

Aug 16
Deep Convolutional Generative Adversarial Networks
mxnet
I‚Äôm not sure about DCGAN‚Äôs Discriminator‚Äòs structure: @goldpiggy, am I right?

Aug 16
Concise Softmax Regression
pytorch
Testing also has cross entrophy loss to calculate. :grinning: @ccpvirus

Aug 16
Bug reporting
Site Feedback
Maybe the discussion URL is wrong. Please tell me which chapters have ‚Äúerror embedding‚Äù.

Aug 15
Using Amazon SageMaker
d2l-en
https://aws.amazon.com/blogs/machine-learning/ Reduce inference costs on Amazon EC2 for PyTorch models with Amazon Elastic Inference

Aug 15
Installation
tensorflow
Recommend my post: Do these before you ask The newest book and d2l package My repo recording all my learning: https://github.com/StevenJokes/d2l-en-read

Aug 15
Selecting Servers and GPUs
d2l-en
[image] from https://alexiej.github.io/deepnn/#gpu-servers [image] from https://course.fast.ai/index.html https://docs.nvidia.com/deeplearning/frameworks/ https://docs.nvidia.com/deeplearning/frameworks/mxnet-release-notes/index.html https://docs.nvidia.com/deeplearning/frameworks/pytorch-rel‚Ä¶

Aug 15
Using Google Colab
d2l-en
PyTorch is already installed. Both the CPU and GPU should work: pytorch-latest-cpu pytorch-latest-gpu [image] [image] For more details on using PyTorch on Google Cloud, see: https://cloud.google.com/deep-learning-vm/docs/images from https://github.com/rmunro/pytorch_active_learning !pip sho‚Ä¶

Aug 15
Using AWS EC2 Instances
d2l-en
mxnet: use ec2 For more details on AWS EC2 Docs AWSEC2 UserGuide Áé©ËΩ¨ GPU ÂÆû‰æã ‚Äì ÊàëÁöÑ Linux Â∑•ÂÖ∑ÁÆ±‰πã‰∫å ‚Äì Âü∫Á°ÄËÆæÁΩÆ Buy AWS Deep Learning AMI using EC2 console using Jupyter book MXnet, PyTorch, TensorFlow2 are already installed and can be activated with: source activate mxnet_p36 source activate pytorch_p‚Ä¶

Aug 15
Concise Implementation of Multilayer Perceptron
pytorch
@goldpiggy I think hyperparameter also are just some numbers needed to optim. AutoML is for them?

Aug 15
Installation
pytorch
@Shashwat My first search from google: https://github.com/jupyter/help/issues/67 It may helps you. Can you tell me where do you download there ipynb files? And more details? Do you install jupyter? Maybe you don‚Äôt install jupyter book. Read https://jupyter.org/install. And then you can find ‚Ä¶

Aug 15
Weight Decay
pytorch
I think it‚Äôs possible. Can we add? @goldpiggy

Aug 15
Weight Decay
pytorch
You are maybe right. Does that mean we can use w and b as bias penalty at the same time or separately?

Aug 15
Linear Regression from Scratch
pytorch
What do you compare with? I can‚Äôt see any relation between them.

Aug 15
Linear Regression from Scratch
pytorch
@Steven_Hearnt I still feel confused about what you are talking. In my understanding, linear relationship is just our idea to simplify a relationship, that maybe we get a hint from Newton ‚ÄúF=ma‚Äù. And then we use words like ‚Äúnoise‚Äù to present what isn‚Äôt no linear. So if you have distibution havin‚Ä¶

Aug 14
Notation
d2l-en
Recommend a repo: https://github.com/Mayuyu/suggested-notation-for-machine-learning#notation-table

Aug 14
Using Jupyter
d2l-en
install jupyter notebook by conda install notebook Measure A‚ä§BA‚ä§B vs. ABAB for two square matrices in R1024√ó1024R1024√ó1024. Which one is faster? I think it depends on data? Do we use SVD or sparse matrix when multiplying? Hinter: We can use the Hinterland plugin to give a hint without using ta‚Ä¶

Aug 14
Probability
mxnet
@Prateek_Vyas You‚Äôre right. Win10? Check: https://discuss.mxnet.io/t/probability-np-random-multinomial/5667/6 issue: https://github.com/apache/incubator-mxnet/issues/15383 Wait for fixing‚Ä¶@mli

Aug 14
Linear Regression from Scratch
pytorch
@Steven_Hearnt Sorry, I‚Äôm an idiot. Can you explain more or give me more reference? :sweat_smile:

Aug 14
Pdf with pytorch code as opposed to mxnet
@vincent I‚Äôm not the official employee. But as I know, they use d2lbook to build pdf https://book.d2l.ai/develop/pipeline.html [image] But I suggest you read by running the code. It will be convient to search the source code and any docs related

Aug 13
Using Google Colab
d2l-en
[colab]read img: No such file: solved git clone whole repo(include /img): Go to https://colab.research.google.com/ Create a new notebook [image] [image] Mount it to your google Drive [image] [image] Create a new folder: [image] name it d2l-en: [image] git clone : reference %‚Ä¶

Aug 13
Deep Convolutional Generative Adversarial Networks
mxnet
@goldpiggy Do you mean this? [image] Got you.

Aug 12
Single Shot Multibox Detection (SSD)
mxnet
Is there some way to avoid downloading again bananas.zip which have downloaded in last section [image] Or in another word, choose to read local directory data\.. first?

Aug 12
Generative Adversarial Networks
mxnet
Does time.stop() mean that only first epoch time?

Aug 12
Deep Convolutional Generative Adversarial Networks
mxnet
But why is /2 + 0.5 , rather than other ways to normalize? [image] Why we need , 1, 1 [image]

Aug 12
Generative Adversarial Networks
mxnet
PyTorch version: http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/gan.html Exercises An equilibrium exists when the generator catch up with all real data.

Aug 12
Deep Factorization Machines
mxnet
Sorry to miss a number‚Ä¶ GPU is quicker.

Aug 12
Do these before you ask!
d2l-en
So please please give more feedback about what I need to improve and how, rather than ‚Äúpassion and keep working hard‚Äù these nonsense words. I am not finding some job for school‚Äôs internship, but just for surviving. So I am more anxious and crazy. @goldpiggy It‚Äôs real ‚Äúdeadline‚Äù for me. I don‚Äôt have‚Ä¶

Aug 12
Sequence-Aware Recommender Systems
mxnet
[image] Why are the examples so small? 9.4 VS 8220.1 :roll_eyes: Factorization Machines

Aug 12
Factorization Machines
mxnet
[image]

Aug 12
Deep Factorization Machines
mxnet
[image] Why is my cpu quicker(more examples each second) than your two GPUs? [image] Sorry to miss a number‚Ä¶

Aug 9
Deep Convolutional Generative Adversarial Networks
mxnet
conjuct: [image] separateÔºö[image] Why didn‚Äôt the discriminator loss go up and the generator go down? Why isn‚Äôt same as the 17.1 losses‚Äô going together finally? I think there is a bug: [image] After switching, [image] issue: https://github.com/d2l-ai/d2l-en/issues/1326 new PR:https://g‚Ä¶

Aug 9
Do these before you ask!
d2l-en
pip install git+https://github.com/d2l-ai/d2l-en You will get the newest pypi package to aviod [image] By the way, if someone really helps you, you‚Äôd better give a love for him/her. It will encourage more replies and make the forum active. For win10 users: activate pytorch didn‚Äôt work in Powers‚Ä¶

Aug 8
Probability
mxnet
Help me too.:face_with_raised_eyebrow:

Aug 8
Pdf with pytorch code as opposed to mxnet
It will be done, please wait and watch.

Aug 8
Predicting House Prices on Kaggle
pytorch
HiÔºå @goldpiggy AnalogyÔºö Initialization is more like talent? lr is more like efforts to understand world?

Aug 8
Deep Convolutional Generative Adversarial Networks
mxnet
hashlib.sha1() SHA1 hash. I got it .hashlib read file: data = f.read(1048576) // MAX rows in EXCEL if not data: // test if file is empty break 1,048,576 rows‚Äî‚Äî a string (in text mode)= 2^20 rows = 2M rows After 2007, EXCEL have 1,048,576 r‚Ä¶

Aug 7
Padding and Strides
pytorch
Q: 1. 4 * 4: [image] 2 * 2: [image] 2. TODO: 3. stride each 2*sampling rate? 4. Look big picture. To avoid overfitting and accerlate the training.

Aug 7
Using Google Colab
d2l-en
Using a GPU . Using a GPU is as simple as switching the runtime in Colab. Specifically, click Runtime -> Change runtime type -> Hardware Accelerator -> GPU and your Colab instance will automatically be backed by GPU compute. from https://cs231n.github.io/setup-instructions/#installing-packages S‚Ä¶

Aug 7
Deep Convolutional Generative Adversarial Networks
mxnet
[image] Why my image is not so clear?

Aug 7
Installation
mxnet
@Ali_coder Recommend my post: Do these before you ask You can try to google it before you ask. I‚Äôm a AMD user, but all Nvidia GPU support cuda in my mind. If you use mxnet, go to https://mxnet.apache.org/versions/1.6/get_started/ [image] [image] You can use CPU version or CUDA version in your‚Ä¶

Aug 7
Statistics
mxnet
Just be quicker. Or can I help you? How to add these?

Aug 7
Installation
pytorch
I have done this long long ago. Just curious about how to do all this type stuff advancely. Have found the solution :conda env create -f environment.yml from handson-ml Or can we add library ‚Äúrequests‚Äù to d2l package?

Aug 6
Installation
mxnet
Recommend my post: Do these before you ask mxnet installation is frastruating. 1st-3th Try: (mxnet) C:\Users\a8679>pip install mxnet==1.6.0 Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple Collecting mxnet==1.6.0 Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6c/3c/c800c230‚Ä¶

Aug 5
Deep Convolutional Generative Adversarial Networks
mxnet
PyTorch version: http://preview.d2l.ai/d2l-en/master/chapter_generative-adversarial-networks/dcgan.html I noticed that c065c0e2593b8b161a2d7873e42418bf6a21106c in [image] And all DATA_HUB have such numbers. Where did these numbers come from? @goldpiggy How can I produce these numbers

Aug 5
Using Amazon SageMaker
d2l-en
@astonzhang , you need to add mxnet tab for this and add corresponding pytorch.

Aug 5
Using Amazon SageMaker
d2l-en
mxnet:Run on Amazon SageMaker generating-cats-with-stylegan-on-aws-sagemaker sagemaker docs An example: ËΩªÊùæÊûÑÂª∫ PyTorch ÁîüÊàêÂØπÊäóÁΩëÁªú(GAN) https://aws.amazon.com/mxnet/ Amazon Sagemaker ‰∏é NVIDIA Jetson Âπ≥Âè∞ÁªìÂêàÊâìÈÄ†Êô∫ËÉΩËæπÁºò Áé©ËΩ¨ GPU ÂÆû‰æã‰πãÁªàÁªìÁØá ‚Äì Ê∑±Â∫¶Â≠¶‰π†ÁöÑÂ∑•ÂÖ∑‰∏éÊ°ÜÊû∂ https://github.com/aws/sagemaker-tensorflow-training-toolkit I‚Ä¶

Aug 5
Statistics
mxnet
I notice that we don‚Äôt have the corresponding discussion of pytorch and tensorflow versions to last chapters. And I tried to add some. Is it right? @astonzhang, @goldpiggy https://discuss.d2l.ai/t/how-to-create-a-discussion-about-the-books-chapter/552/3

Aug 5
Statistics
mxnet
[image] https://keras.rstudio.com/articles/tutorial_overfit_underfit.html I agree that a high variance model lacks of generalization , if it means that the model doesn‚Äôt work in other patterns. But I still feel confused about the meaning of flexibility, if it means that the model doesn‚Äôt work i‚Ä¶

Aug 5
Possible error in Adadelta code
d2l-book
:face_with_raised_eyebrow: I know the tradeoff. But the thing is that the project doesn‚Äôt have many contributors to communicate with so I feel lonely.

Aug 4
Possible error in Adadelta code
d2l-book
I think the most important problem is to release not so often. One month is too long versus the AI‚Äòs so quick development. I maybe lose patience to the project. :expressionless:

Aug 2
Forward Propagation, Backward Propagation, and Computational Graphs
d2l-en
The most important thing, in my opinion, is to find out which elements we need to calculate the partial derivation. Back propogation is only the way to find it. Am I right?

Aug 2
Forward Propagation, Backward Propagation, and Computational Graphs
d2l-en
I think we could do merely the partial derivation and value calculation during the forwarding pass and multiplication during the back propagation. What is different with BP?

Aug 1
Possible error in Adadelta code
d2l-book
I already have suggested we publish preview version in https://github.com/d2l-ai/d2l-en/pull/1241 to avoid this type to happen. But it is closed‚Ä¶

Jul 31
From Dense Layers to Convolutions
d2l-en
Couldn‚Äôt understand your meaning. Do you mean whether we keep order of examples or not? If you ask this, the answer is not.

Jul 29
Softmax Regression
d2l-en
Just a statistical speaking!

Jul 28
Linear Regression (exercise)
d2l-en
Next time maybe you can ask below the chapter‚Äôs question to get more attention from others. I guess the function must be convex to assure global optima.

Jul 25
How to create a discussion about the book's chapter?
pytorch
First time to create discussion, I‚Äôm curious about [image] It is auto? Or we need to add CSS for it? <span class="badge badge-notification clicks" title="1 click">1</span> not work. I‚Äôm not a frontend engineer, so I just want to get a help here. And I tried to view page source and noticed that h‚Ä¶

Jul 25
Softmax Regression from Scratch
pytorch
What does " No. of correct predictions, no. of predictions" mean?

Jul 24
Do these before you ask!
d2l-en
How to pin it? @goldpiggy

Jul 24
Contributing to This Book
d2l-en
We can get more information in CONTRIBUTING, STYLE_GUIDE. mxnet.py, torch.py and tensorflow.py in /d2l are automatically generated by d2lbook build lib., please don‚Äôt edit directly. #@save must need d2lbook build lib , if you are win10 users, then you can call others to help you, for example, h‚Ä¶

Jul 22
Linear Regression from Scratch
pytorch
You should use l.sum().backward(). Try to contrast with the code given before you ask please, and it is quicker and useful usually. [image]

Jul 22
Installation
pytorch
from d2l import torch as d2l --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) in 1 get_ipython ( ) . run_line_magic ( ‚Äòmatplotlib‚Äô , ‚Äòinline‚Äô ) ----> 2 from d2l import torch as d2l 3 import torch 4 import random ~‚Ä¶

Jul 22
Installation
pytorch
After run conda activate d2l # sanity check that the path to the python # binary matches that of the anaconda env # after you activate it which python To deactivate the environment, either run conda deactivate d2l or exit the terminal. Note that every time you want to work on the assignment, you‚Ä¶

Jul 22
Dropout
pytorch
After we dropouted, the corresponding hidden layers have already changed. Enable/Disable is confusing. Test is just a process of calculating a prediction by weights after bp. We don‚Äôt need to disable Dropout! Yes, matrix multiplication is the reason of corresponding to multiple hidden units.

Jul 22
Forward Propagation, Backward Propagation, and Computational Graphs
d2l-en
The purpose of backward propagation is to get near to, even same as the labels we gave. We only calulate gradients when we know the distance of prediction and reality. This is the backward propagation what really does.

Jul 21
Linear Regression from Scratch
pytorch
I can‚Äôt give your answer without the whole code you ran. Please publish your all code you ran.

Jul 21
Dropout
pytorch
We don‚Äôt allow Dropout in the test. The first answer in How does dropout work during testing in neural network? can explain well. However, there are two main reasons you should not use dropout to test data: Dropout makes neurons output ‚Äòwrong‚Äô values on purpose Because you disable neurons r‚Ä¶

Jul 20
Softmax Regression
d2l-en
As we saw in ‚ÄúFreshness and Distribution Shift‚Äù, if production data is different from the data a model was trained on, a model may struggle to perform. To help with this, you should check the inputs to your pipeline. In https://learning.oreilly.com/library/view/building-machine-learning/978149204‚Ä¶

Jul 19
How to do backpropogation with Softmax and Mean Square Error?
pytorch
Thanks. Divided by a same number won‚Äôt change the value. I forgot it!

Jul 19
How to do backpropogation with Softmax and Mean Square Error?
pytorch
I got it, But if the $x - max(x)$ is a too small negative number, it also could cause numerical overflow. Or we should judge how to avoid numerical overflow from the overall numbers‚Äô distribution Because $x - any_number$ works too. any_number should be close to any x.

Jul 19
Weight Decay
pytorch
Is b in b^2 same as the b in wx+b?

Jul 16
From Dense Layers to Convolutions
d2l-en
6.1.1 formulas make use of the u[i,j] term but there is no explanation of what is u . Wait for explanation.

Jul 16
From Dense Layers to Convolutions
d2l-en
[6-1-1] No missing. ?

Jul 16
From Dense Layers to Convolutions
d2l-en
6.1.6.2 When might it not make sense to allow for pigs to fly? What does ‚Äúwhen‚Äù mean?

Jul 16
How to do backpropogation with Softmax and Mean Square Error?
pytorch
And I‚Äôm confused about your softmax(x) def softmax(x): func = np.exp(x - np.max(x)) return func / func.sum(axis=0) And my softmax(x) is different. def softmax(x): func = np.exp(x) return func / func.sum(axis=0) But the result is same: Why?

Jul 15
How to do backpropogation with Softmax and Mean Square Error?
pytorch
I guess, the reason is that pytorch‚Äôs internal encapsulation reduces the margin of error of float grad. So it goes down quickly. I haven‚Äôt seen the source code to confirm it.

Jul 15
Model Selection, Underfitting and Overfitting
pytorch
I think what @Kushagra_Chaturvedy said is right. https://github.com/d2l-ai/d2l-en/pull/1181

Jul 15
Implementation of Multilayer Perceptron from Scratch
pytorch
What‚Äôs your IDE? I‚Äôm curious. Thanks.

Jul 15
Softmax Regression
d2l-en
I already have understand $y_j$ is the true label, such as one-hot. But the diverce of our thinking is that I think the true label has a certain relationship with $o_j$, so I think $y_j$ is also $o_j$'s function. When we get same $o_j$, we get only one and $y_j$. Doesn‚Äôt it conform the defination of‚Ä¶

Jul 14
Softmax Regression
d2l-en
Please check https://github.com/d2l-ai/d2l-en/issues/1141 quickly, I think it maybe makes all eval wrong.

Jul 14
Softmax Regression
d2l-en
I think it is a function in reality. But newton‚Äôs calculus can‚Äôt calculate its derivative, just because the function is discrete.

Jul 14
Softmax Regression
d2l-en
How do we judge whether $a$ is a function of $b$ or not? Or we just judge by that we haven‚Äôt defined it before, rather than whether $a$ has a relationship with $b$ in reality or not.

Jul 13
Softmax Regression
d2l-en
When we calculate the derivative of $y_j$ is 0, does it mean that we think $y_j$ has no relationship about $o_j$.

Jul 13
Softmax Regression
d2l-en
I got it. Thanks @goldpiggy

Jul 11
Softmax Regression
d2l-en
We only need to calculate the the derivative of softmax(o)_j Ôºàj is in or out?) to get the second derivative of the cross-entropy loss ùëô(ùê≤,ùê≤ÃÇ) ? I noticed that the second derivative of the cross-entropy loss ùëô(ùê≤,ùê≤ÃÇ) is exactly the derivative of softmax(o)_j . The derivative of y_j is 0 ? y_j is 1 o‚Ä¶

Jul 11
Softmax Regression
d2l-en
[image] j should be in the bracket? or When o is vector,j should be outside? When o_j,j should be in? [image]

Jul 8
Softmax Regression
d2l-en
If we can fully use the past experience to guide our action in future, then how similar of past and future is the next question that we should take attention to. In my understanding, over-fitting problems focus more on difference of past and future, while under-fitting on similarity.

Jul 8
Softmax Regression
d2l-en
So, we only wait for the result of the second to happen? Probability, in the begining, more depends on what we think how many categories result are. And, because we have freedom to say, inconsistency is so common, such as famous ‚ÄúBayesian method‚Äù. Bayesian method actually looks like a good way t‚Ä¶

Jul 5
Concise Implementation of Multilayer Perceptron
pytorch
I don‚Äôt know. You can post your all code on github. And give us URL to figure it out. The random init still seems like metaphysics to me.

Jul 5
Softmax Regression
d2l-en
No memory is too abstract. What does it mean? I need more examples of identical distrbutions. What can we call that they are identical distributions? For example, students in a class are identical distributions? But they usually have similiar ages and live near the school. I read an example of ‚Ä¶

Jul 4
Concise Implementation of Multilayer Perceptron
pytorch
It didn‚Äôt happen when I train.Cost me an hour. :joy:(find some bugs and search on web) [image] Please let me know what you ran in the previous 8 code cells. What is the function to find out what exactly happened in every layer. @anirudh

Jul 4
Softmax Regression
d2l-en
And we can‚Äôt predict any if we can‚Äôt describe what will happen and what situation we are in now. And different ways to describe a thing usually confuses each other, then another story will happen. The attempts to simplify our world by concluding everything maybe just deduce a contrary result that ‚Ä¶

Jul 4
Softmax Regression
d2l-en
Thanks. But I think I‚Äôm still confused. Are we sure about the independence? How can we? Or we just naively think that all samples are independent? I think that independence and the establish of $P(Y | X) = P(y_1, y_2, ‚Ä¶ | x_1,x_2,‚Ä¶) = \prod{P(y_i | x_i)}$ 's equal sign are mutual causation. In othe‚Ä¶

Jul 4
Softmax Regression
d2l-en
Are we sure about the independence? How can we? Or we just naively think that all samples are independent? I think that independence and the establish of $P(Y | X) = P(y_1, y_2, ‚Ä¶ | x_1,x_2,‚Ä¶) = \prod{P(y_i | x_i)}$ 's equal sign are mutual causation.In other words, just logical rename.

Jul 3
Softmax Regression
d2l-en
Thanks. I already have known what -log does. But I still wonder the math meaning and, more importantly , realistic meaning of the definition of the joint likelihood function. [image] Why would we need to multiply all P(y|x) [image] ? Do their parameters independent surely? [image] If we mult‚Ä¶

Jul 2
Batch Normalization
pytorch
It is so quick and it only spent <5 mins to train on my cpu. :thinking: loss 0.246, train acc 0.909, test acc 0.878 3313.6 examples/sec on cpu

Jul 2
Calculus
pytorch
some apis: plt.gca Get the current Axes instance on the current figure matching the given keyword args, or create one. Examples: To get the current polar axes on the current figure: plt.gca(projection='polar') If the current axes doesn‚Äôt exist, or isn‚Äôt a polar one, the appropriate axes will‚Ä¶

Jun 30
Concise Linear Regression
pytorch
However, which one is quicker ? I think (10,) is quicker than (10,1).

Jun 30
Concise Linear Regression
pytorch
Oh. I see it. :upside_down_face:

Jun 30
Concise Linear Regression
pytorch
New too. Wait for pro.

Jun 30
Concise Linear Regression
pytorch
? There is no labels = labels.reshape(-1,1) in the section‚Äôs first line. There is the newest version!: http://preview.d2l.ai/d2l-en/PR-1080/chapter_preliminaries/autograd.html#grad.zero_() So in your version, reshape is necessary for net(X) but not for labels.

Jun 30
Concise Linear Regression
pytorch
I am quite new too. Like what Kushagra_Chaturvedy said, data preprocessing is important for using pytorch API.

Jun 30
Concise Linear Regression
pytorch
I think what you say is good but not better. In example, [image] You‚Äôd better use loss(net(X),y.reshape(net(X).shape). ?Am I right :slightly_smiling_face:

Jun 30
Linear Regression from Scratch
pytorch
# PyTorch accumulates the gradient in default, we need to clear the previous # values. x.grad.zero_() in http://preview.d2l.ai/d2l-en/PR-1080/chapter_preliminaries/autograd.html! You can use the searching [image] to find whether a function have mentioned before.

Jun 30
Model Selection, Underfitting and Overfitting
pytorch
I‚Äôm learning pytorch.https://pytorch.org/docs/stable/data.html Am I right?DataLoader(drop_last = False) https://beta.mxnet.io/guide/getting-started/to-mxnet/pytorch.html is helpful!

Jun 29
Anaconda Install in Jetson Nano
So if you can record your study‚Äôs process,new users will be helped a lot. :innocent: Look forward to your blog.

Jun 28
Sequence Models
pytorch
8.1.2. A Toy Example features = d2l.zeros((T-tau, tau)) AttributeError : module ‚Äòd2l.torch‚Äô has no attribute ‚Äòzeros‚Äô Then I search http://preview.d2l.ai/d2l-en/PR-1077/search.html?q=d2l.zeros&check_keywords=yes&area=default No source code: http://preview.d2l.ai/d2l-en/PR-1077/chapter_appendix-to‚Ä¶

Jun 28
Anaconda Install in Jetson Nano
I didn‚Äôt have Jetson Nano. However, can you give us more details of not working, such as screenshots?

Jun 28
Model Selection, Underfitting and Overfitting
pytorch
The functions to your methods( :thinking:Am I right?): Corresponding with the first method is DataLoader(Drop_Last = True)? How to fill? DataLoader(Drop_Last = False) the necessity of float() in def add , self.data = [a+float(b) for a, b in zip(self.data, args)]? Why metric.add(l.sum(), y.numpy().‚Ä¶

Jun 27
Dropout
pytorch
We use loss function just for training‚Äôs Backpropagation, and we don‚Äôt need to train anymore when we are in test. Similiarly, we just care the final scores of exam instead of the concrete answers. Do I understand it rightly?

Jun 26
Weight Decay
pytorch
So it just means that bias penalty is not negative? b is not same as the b in wx+b?

Jun 26
Model Selection, Underfitting and Overfitting
pytorch
But, I am still curious about the necessity of float() and what args represents. Why metric.add(l.sum(), y.numpy().size) or metric.add(l*len(y), y.numpy().size) ? why we so care about size ? Does metric[1] represent epoch_num?

Jun 26
Image Classification Dataset
pytorch
%%time is better. One time is enough :grinning:

Jun 26
Forward Propagation, Backward Propagation, and Computational Graphs
d2l-en
n√óm [4-7-e] How to compute the memory footprint? What is in the memory? Computational Graph of Backpropagation? Try to track all latest two times‚Äô calculate. Now, I can‚Äôt. How to parallelly train? Good for updating quickly.Easy to fall into a local extremum.

Jun 26
Use GPUs
pytorch
5.6.5. Exercises you should see almost linear scaling? I‚Äôm confused what is linear scaling.

Jun 26
Use GPUs
pytorch
I didn‚Äôt use nvidia.So I didn‚Äôt install GPU version and cuda. torch.cuda.device('cuda') AssertionError : Torch not compiled with CUDA enabled torch.cuda.device_count() 0 Why didn‚Äôt AssertionError happen when I run the following code? torch.cuda.device('cuda:1') <torch.cuda.device at 0x16349f‚Ä¶

Jun 26
Dropout
pytorch
we use .reset_parameters()? And I found that searching in pytorch docs is so slow. Do you have some good advices? How similar can we say that these pics are similar? We can‚Äôt expect exactly the same outputs when we have the same inputs?

Jun 26
Deep Convolutional Neural Networks (AlexNet)
pytorch
lr, num_epochs = 0.01, 10 d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr) training on cpu no other results? no pic? [7-1] so slow?

Jun 25
Dropout
pytorch
Two pics in 4.6.4.3. Training and Testing and 4.6.5. Concise Implementation are so different? 4.6.7. Exercises I have tried to switch the dropout probabilities for layers 1 and 2 by switching dropout1 and dropout2. Sequential( (0): Flatten() (1): Linear(in_features=784, out_features=256, bias=T‚Ä¶

Jun 25
Dropout
pytorch
I noticed that nn.Flatten() haven‚Äôt mentioned. https://pytorch.org/docs/master/generated/torch.nn.Flatten.html Why do we need to nn.Flatten() first?

Jun 25
Dropout
pytorch
I noticed there isn‚Äôt test loss. Why? I have read as following. https://kharshit.github.io/blog/2018/12/07/loss-vs-accuracy

Jun 24
Weight Decay
pytorch
4.5.1. Squared Norm Regularization Whether we include a corresponding bias penalty b^2 can vary across implementations, and may vary across layers of a neural network. Often, we do not regularize the bias term of a network‚Äôs output layer. Can you explain b^2 in detail?

Jun 24
Weight Decay
pytorch
Animator: http://d2l.ai/chapter_linear-networks/softmax-regression-scratch.html?highlight=d2l%20animator http://d2l.ai/_modules/d2l/torch.html#set_axes https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.axes.Axes.set_ylim.html

Jun 24
Weight Decay
pytorch
In 4.5.4. Concise Implementation [4-5] Typographical errorÔºÅÔºü

Jun 24
Model Selection, Underfitting and Overfitting
pytorch
I don‚Äôt know what exactly means? (1) By model complexity increasing, the training error will decrease. TODO: (2) TODO: (3)a function of the amount of data? what function you mean? I haven‚Äôt figure out the meaning of the question. ? The new polynomial features will multiply i!. Just as s‚Ä¶

Jun 24
Model Selection, Underfitting and Overfitting
pytorch
In my pics, the distance of the orange line and the blue line is so near. [4-4-1] [4-4-2] And overfitting that test loss increases did‚Äôt happen! [4-4-3] Why?

Jun 24
Model Selection, Underfitting and Overfitting
pytorch
In 4.4.4.2. Training and Testing Model metric = d2l.Accumulator(2) # sum_loss, num_examples I didn‚Äôt understand what Accumulator really do? So I tried to read http://d2l.ai/_modules/d2l/torch.html#Accumulator. In def add, self.data = [a+float(b) for a, b in zip(self.data, args)] And I trie‚Ä¶

Jun 24
Concise Softmax Regression
pytorch
I will learn it next time !

Jun 24
Probability
pytorch
Maybe it is just a coincidence that almost 90 groups of experiments is ‚Äúdie = 6‚ÄùÔºü It would be more clear if you counts / 1000 # Relative frequency as the estimate.

Jun 23
Implementation of Multilayer Perceptron from Scratch
pytorch
1.When test accuaracy increases most quickly and high, can we say that this hyperparameter is the best value?

Jun 23
Multilayer Perceptrons
pytorch
the derivative of the tanh: dtanh(x)/dx=1‚àítanh^(2)= {2exp(‚àí2x)-[exp(‚àí2x)]^2}/(1+exp(‚àí2x))^2. https://www.math24.net/derivatives-hyperbolic-functions/ the derivative of the pReLU(x): dpReLU(x)/dx = 1 (if x > 0);Œ± (if x < 0);doesn‚Äôt exist (if x = 0) h= ReLU(x) = max(x, 0) y = ReLU(h) = max(h, 0‚Ä¶

Jun 23
Concise Softmax Regression
pytorch
batch_size num_epochs lr It didn‚Äôt happen in my training. Why? [3-7] I guess that the reason of ‚Äúthe test accuracy decrease again after a while‚Äù is that SGD updates too often. Maybe we can try MBGD. for more:

Jun 18
Softmax Regression from Scratch
pytorch
3.6.9 Nothing happened!? And, max number of 64float is 2^1024 - 2^(1023-52). So e^1024 will overflowÔºü X = torch.tensor([[50., 51., 52.], [54., 55., 56.]]) X_prob = softmax(X) X_prob tensor([[0.0900, 0.2447, 0.6652], [0.0900, 0.2447, 0.6652]]) log(0) will error! Use RELU to replace softmax? I‚Ä¶

Jun 18
Softmax Regression from Scratch
pytorch
In class Accumulator:: self.data = [a+float(b) for a, b in zip(self.data, args)] What is the meaning of a+float(b)? It couldn‚Äôt be better, if you can combine these to explain what happened behind metric.add(float(l)*len(y), float(accuracy(y_hat, y)), len(y)) and metric.add(l_sum, accuracy(y_h‚Ä¶

Jun 18
Softmax Regression from Scratch
pytorch
In 3.6.4, y_hat[range(len(y_hat)), y] What did y_hat[y] mean? print(y_hat[y]) IndexError Traceback (most recent call last) in ----> 1 print(y_hat[y]) IndexError: index 2 is out of bounds for dimension 0 with size 2 And I found other styles: def cross_entropy(‚Ä¶

Jun 18
Image Classification Dataset
pytorch
batch size = 1, stochastic gradient descent (SGD) batch size = 256, mini-batch gradient descent (MBGD) Because using GPU to parallel read data, so MBGD is quicker. Reducing the batch_size will make overall read performance slower. :face_with_monocle:Does my guess right? I‚Äôm a Windows user. Try it next time! ht‚Ä¶

Jun 18
Softmax Regression
d2l-en
Sorry, I can‚Äôt recognize the math formulas in 3.4.6. :cold_face:

Jun 18
Auto Differentiation
pytorch
Q2. b = b + 1000 Why does it make False? I found that print(b.grad) is None? Why did it happen?

Jun 16
Linear Regression from Scratch
pytorch
You meant my github‚Äôs issue? My issue is about " 2.5.2 does‚Äôt have PyTorch‚Äôs version." I have heard that Variable has merge into tensor from zhihu. Is it right? If so, 2.5.2 doesn‚Äôt need PyTorch‚Äôs version.

Jun 15
Linear Regression from Scratch
pytorch
Thanks a lot. I got it why I was worry.

Jun 15
Linear Regression from Scratch
pytorch
I just knew that gradient is dy/dw instead of dy/dx. :joy:

Jun 15
Concise Linear Regression
pytorch
CLASS torch.nn.SmoothL1Loss(size_average=None, reduce=None, reduction='mean') [image] print(net[0].weight.grad()) For more: Search for accessing the gradient:https://d2l.ai/chapter_deep-learning-computation/parameters.html

Jun 15
Concise Linear Regression
pytorch
I‚Äôm confused of "size_average and reduce are in the process of being deprecated" . I need to ask you to confirm that I‚Äôm right about reduction ( string , optional ) can replace the others.

Jun 15
Linear Regression from Scratch
pytorch
set y as voltage and set x as current. No, I can‚Äôt. [image] I can‚Äôt separate variables v and T in e^(hv/KT). y.backward(retain_graph=True) true_w has one row and len(w) columns, but w has len(w) rows and one column. set lr = your_num In the last loop of for i in range(0, num_examples, batch_‚Ä¶

Jun 15
Linear Regression from Scratch
pytorch
I didn‚Äôt understand the relation between for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) # Minibatch loss in X and y and with torch.no_grad(): train_l = loss(net(features, w, b), labels) Why did we use features to replace x and use labels to repla‚Ä¶

Jun 15
Linear Regression from Scratch
pytorch
If y = b , then gradient is 0. I think that gradient is 0 when W is zeros. Why moving in the right direction ? I have understand that convex cost function has only one minima . But param.data.sub_(lr*param.grad/batch_size) will make sure that param doesn‚Äôt change, if grad == 0. :sweat_smile: Then how to ge‚Ä¶

Jun 14
Linear Algebra
pytorch
@manuel-arno-korfmann I think @goldpiggy is right. A_mn: [A_mn] AT_nm: [AT_nm]

Jun 13
Auto Differentiation
pytorch
Q1. Does it mean that if I ‚Äòclone‚Äô a tensor or a Variable that has requires_grad attribute, then I don‚Äôt need to .requires_grad() for the new one? Use detach() to remove a tensor? However, why can I still judge x.grad() == u.grad? ‚ÄúRemove‚Äù doesn‚Äôt mean that x will not exist? I think that x and‚Ä¶

Jun 13
Linear Regression from Scratch
pytorch
Does it mean that it shouldn‚Äôt work when using the correct l.sum? :thinking: I think that gradient is 0 when W is ones. Why moving in the right direction?

Jun 12
Linear Regression from Scratch
pytorch
w = torch.zeros(size=(2,1), requires_grad=True) w tensor([[0.], [0.]], requires_grad=True) lr = 0.03 # Learning rate num_epochs = 3 # Number of iterations net = linreg # Our fancy linear model loss = squared_loss # 0.5 (y-y')^2 for epoch in range(num_epochs): # Assuming the number of ‚Ä¶

Jun 12
Linear Regression
pytorch
b = mean(x), when derivative of ‚àëi(xi‚àíb)2 equals 0,. MLE (maximum likelihood estimation). When each parameter‚Äôs partial derivative of lnL equals 0, we can get the optimal value. Ê¶ÇÁéáËÆ∫‰∏éÊï∞ÁêÜÁªüËÆ°.ÊµôÂ§ßÁ¨¨ÂõõÁâà P154 [3-1] y=mean(W_T*X)+b, W_T,X:matrix, *:matrix multiply 2.TODO: 3.TODO:

Jun 12
Probability
pytorch
def experiment_fig(n, m): counts = torch.from_numpy(np.random.multinomial(n, fair_probs, size=m)) cum_counts = counts.type(torch.float32).cumsum(axis=0) estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True) d2l.set_figsize((6, 4.5)) for i in range(6): d2l.plt.‚Ä¶

Jun 12
Document
pytorch
2.7.5. Exercises QÔºö Pytorch: Look up ones_like and autograd on the PyTorch website. What are all the possible outputs after running np.random.choice(4, 2) ? Can you rewrite np.random.choice(4, 2) by using the np.random.randint function? q1. In my github. q2. import numpy as n‚Ä¶

Jun 12
Data Preprocessing
pytorch
After running pip install -U d2l -f https://d2l.ai/whl.html, I can directly run from d2l import torch as d2l :relaxed: Thanks :shushing_face: But I‚Äôm confused about the bug when I directly run the imtorch.py(rename from d2l/torch.pyÔºâ $ /usr/bin/env python "d:\onedrive\ÊñáÊ°£\read\d2l\d2l\imtorch.py" Traceback (most recent ‚Ä¶

Jun 12
Auto Differentiation
pytorch
After running pip install -U d2l -f https://d2l.ai/whl.html, I can directly run from d2l import torch as d2l :relaxed: Thanks :shushing_face: But I‚Äôm confused about the bug when I directly run the imtorch.py(rename from d2l/torch.pyÔºâ $ /usr/bin/env python "d:\onedrive\ÊñáÊ°£\read\d2l\d2l\imtorch.py" Traceback (most recent ‚Ä¶

Jun 12
Auto Differentiation
pytorch
The first reply is my 2.5.2‚Äôs pytorch code. I use ‚Äúdetach‚Äù and ‚Äúclone‚Äù to simulate what ‚Äúcopy‚Äù do in MXNET. But which one is best, ‚Äúdetach‚Äù or ‚Äúclone‚Äù? The second reply is about "the meaning of d / a " in 2.5.4. I tried another ‚Äúb‚Äù.The return of ‚Äúb.grad == (d / b)‚Äù is false rather than true. w‚Ä¶

Jun 12
Data Preprocessing
pytorch
Thanks. I will read them later. Now I have some issues about d2l/pytorch.py I have renamed it as impytorch.py to avoid same name with package. $ /usr/bin/env python "d:\onedrive\ÊñáÊ°£\read\d2l\d2l\imtorch.py" Traceback (most recent call last): File "d:\onedrive\ÊñáÊ°£\read\d2l\d2l\imtorch.py", line 2‚Ä¶

Jun 12
Calculus
pytorch
Is there a way to open it in VScode? Or how to make it openable in VScode? :sweat_smile: The doc is not friendly to other users. :joy: https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.savefig.html

Jun 11
Auto Differentiation
pytorch
I‚Äôm not very clear. Maybe second derivative is more easy to overflow the scope of storages. y.backward() RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time. y = 2 * torc‚Ä¶

Jun 11
Auto Differentiation
pytorch
2.5.4. ‚ÄúConsequently d / a allows us to verify that the gradient is correct.‚Äù I didn‚Äôt understand the meaning of d / a. So I code as follow. But now, I‚Äôm more confused. b = torch.randn(size=(1,), requires_grad=True) b = b + 1000 # record the calculation d = f(b) d.backward() b.grad == (d / b) F‚Ä¶

Jun 11
Auto Differentiation
pytorch
I noticed that 2.5.2 does‚Äôt have PyTorch, so I tried to convert it. I used ‚Äú.detach()‚Äù and ‚Äú.clone()‚Äù to simulate ‚Äú.copy()‚Äù in MXNET. But I‚Äôm confused with differences from ‚Äú.detach()‚Äù and ‚Äú.clone()‚Äù. Which one is more like to ‚Äú.copy()‚Äù in MXNET? from torch.autograd import Variable x = torch.‚Ä¶

Jun 11
Calculus
pytorch
x = np.arange(0, 3, 0.1) plot(x, [x ** 3 - 1 / x, 4 * x - 4], 'x', 'f(x)', legend=['f(x) = x ** 3 - 1 / x ', 'Tangent line (x=1) : y = 4 * x - 4 ']) [2-4-e1] According to the power rule and multiple rule," 3 * x ** 2 + 1 / (x ** 2) "is the derivative function of f (x). So x == 1,f‚Äô(1) ==3 * 1 **2 ‚Ä¶

Jun 11
Data Manipulation
pytorch
It is understandable. :sweat_smile: The original examples are built by MXNet. :joy:( created by mli)

Jun 11
Data Preprocessing
pytorch
0.I have already done by pytorch‚Äôs api. 1.The best way to read pytorch‚Äôs source code?Please give me some tips. 2.how to loop by dataframe‚Äôs colomns?I‚Äôm trying to use loop to calculate data.isnull().sum().

Jun 11
Linear Algebra
pytorch
Jun 11
Linear Algebra
pytorch
pass pass yes,pass first dimension:2 first dimension not match! A = torch.arange(20, dtype = torch.float32).reshape(5, 4) A / A.sum(axis=1) RuntimeError: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 1 It will be fine. B = torch.a‚Ä¶

Jun 10
Data Preprocessing
pytorch
I‚Äôm a Chinese student. If I have wrong expressions, please forgive me. :sob: Because new users only can add two links. Check answer to 2:

Jun 10
Data Preprocessing
pytorch
Tried an hour to search some related contents and understand functions of pandas. :cold_face: import os data_file = '../data/results11.csv' import pandas as pd data = pd.read_csv(data_file) print(data.head()) # calculate the max of the NaN numbers of all columns m = max(data.isnull().sum(axis = 0)) data_d‚Ä¶

Jun 10
Data Manipulation
pytorch
a % b tensor([[0., 1.], [0., 0.], [0., 1.], [0., 0.], [0., 1.]])

Jun 10
Data Manipulation
pytorch
I have got it from the doc, but thanks anyway. I‚Äôm a new bee of pytorch.

Jun 10
Data Manipulation
pytorch
1. x = torch.arange(12, dtype=torch.float32).reshape((3,4)) y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) x,y,x == y,x < y,x > y (tensor([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]]), tensor([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]]), tensor‚Ä¶
