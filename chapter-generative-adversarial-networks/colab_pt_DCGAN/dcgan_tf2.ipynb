{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dcgan-tf2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3pPWY723ZB0",
        "outputId": "2d2dd182-7bc2-40f0-f11d-05a00c020e7d"
      },
      "source": [
        "!pip install git+https://github.com/d2l-ai/d2l-en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/d2l-ai/d2l-en\n",
            "  Cloning https://github.com/d2l-ai/d2l-en to /tmp/pip-req-build-akbvqqa0\n",
            "  Running command git clone -q https://github.com/d2l-ai/d2l-en /tmp/pip-req-build-akbvqqa0\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.2) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.2) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.2) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.2) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from d2l==0.16.2) (1.1.5)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (5.0.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (7.6.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter->d2l==0.16.2) (5.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.2) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->d2l==0.16.2) (2.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.2) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->d2l==0.16.2) (2.10)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->d2l==0.16.2) (2018.9)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.2) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.2) (5.3.5)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.2) (5.0.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter->d2l==0.16.2) (5.1.1)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (5.1.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (0.9.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (4.7.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (1.5.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter->d2l==0.16.2) (0.2.0)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.16.2) (22.0.3)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.16.2) (2.6.1)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter->d2l==0.16.2) (1.9.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (0.4.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (3.3.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter->d2l==0.16.2) (0.8.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.2) (1.0.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter->d2l==0.16.2) (3.5.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-console->jupyter->d2l==0.16.2) (1.0.18)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->d2l==0.16.2) (1.15.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.2) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.2) (54.2.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.2) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.2) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter->d2l==0.16.2) (0.8.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat->notebook->jupyter->d2l==0.16.2) (2.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter->d2l==0.16.2) (1.1.1)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter->d2l==0.16.2) (0.7.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.16.2) (20.9)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter->d2l==0.16.2) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.16.2) (0.2.5)\n",
            "Building wheels for collected packages: d2l\n",
            "  Building wheel for d2l (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for d2l: filename=d2l-0.16.2-cp37-none-any.whl size=77221 sha256=193405527de3312984602f464460fbd10ec120f5d090faf2fcced43d7f5aa7de\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-m9hul6gj/wheels/82/6f/b8/8422c7ed664272685e3d139b127f89df39d934704a14b2156c\n",
            "Successfully built d2l\n",
            "Installing collected packages: d2l\n",
            "Successfully installed d2l-0.16.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqT4qIkE3bG-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from d2l import tensorflow as d2l"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t__6FiSy3y90",
        "outputId": "7d4f99ed-f1c5-4b27-b49a-65157ac433c5"
      },
      "source": [
        "#@save\n",
        "d2l.DATA_HUB['pokemon'] = (d2l.DATA_URL + 'pokemon.zip',\n",
        "                           'c065c0e2593b8b161a2d7873e42418bf6a21106c')\n",
        "\n",
        "data_dir = d2l.download_extract('pokemon')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading ../data/pokemon.zip from http://d2l-data.s3-accelerate.amazonaws.com/pokemon.zip...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pPIsJ465MA1"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/load_data/images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "Isi421rn30ty",
        "outputId": "f8c99cbc-70ed-4326-91cc-1bf0e18186fc"
      },
      "source": [
        "img_raw = tf.io.read_file(data_dir)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FailedPreconditionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b83e4d6c6017>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file\u001b[0;34m(filename, name)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m       return read_file_eager_fallback(\n\u001b[0;32m--> 559\u001b[0;31m           filename, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m    560\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_io_ops.py\u001b[0m in \u001b[0;36mread_file_eager_fallback\u001b[0;34m(filename, name, ctx)\u001b[0m\n\u001b[1;32m    595\u001b[0m   \u001b[0m_attrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m   _result = _execute.execute(b\"ReadFile\", 1, inputs=_inputs_flat,\n\u001b[0;32m--> 597\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    598\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     _execute.record_gradient(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFailedPreconditionError\u001b[0m: ../data/pokemon; Is a directory [Op:ReadFile]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqmDatr15lZJ"
      },
      "source": [
        "https://www.tensorflow.org/datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGmDQGpR5QOv"
      },
      "source": [
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM1D8gqT5qwm",
        "outputId": "ea413a04-f33a-47a0-cb2d-098a7fa01609"
      },
      "source": [
        "tfds.folder_dataset.ImageFolder(data_dir)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_datasets.core.folder_dataset.image_folder.ImageFolder at 0x7f66449dd990>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jRYbr1H51EP"
      },
      "source": [
        "pokemon = tfds.folder_dataset.ImageFolder(data_dir)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUV8c9v357Cj"
      },
      "source": [
        "https://www.tensorflow.org/datasets/api_docs/python/tfds/folder_dataset/ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDamu-hU5wsG",
        "outputId": "896a9b6f-54d8-4c74-8f2c-6ee88599ecc0"
      },
      "source": [
        "print(pokemon.info)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='image_folder',\n",
            "    version=1.0.0,\n",
            "    description='Generic image classification dataset.',\n",
            "    homepage='https://www.tensorflow.org/datasets/catalog/image_folder',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'image/filename': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=0),\n",
            "    }),\n",
            "    total_num_examples=0,\n",
            "    splits={\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHuC9Vqw6D2I"
      },
      "source": [
        "batch_size = 256"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJNO85Ar6Mw4",
        "outputId": "259338ea-8e98-4618-9248-5f570875d484"
      },
      "source": [
        "help(pokemon)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on ImageFolder in module tensorflow_datasets.core.folder_dataset.image_folder object:\n",
            "\n",
            "class ImageFolder(tensorflow_datasets.core.dataset_builder.DatasetBuilder)\n",
            " |  ImageFolder(root_dir: str, *, shape: Union[Tuple[Union[int, NoneType], ...], List[Union[int, NoneType]], NoneType] = None, dtype: Union[tensorflow.python.framework.dtypes.DType, NoneType] = None)\n",
            " |  \n",
            " |  Generic image classification dataset created from manual directory.\n",
            " |  \n",
            " |  `ImageFolder` creates a `tf.data.Dataset` reading the original image files.\n",
            " |  \n",
            " |  The data directory should have the following structure:\n",
            " |  \n",
            " |  ```\n",
            " |  path/to/image_dir/\n",
            " |    split_name/  # Ex: 'train'\n",
            " |      label1/  # Ex: 'airplane' or '0015'\n",
            " |        xxx.png\n",
            " |        xxy.png\n",
            " |        xxz.png\n",
            " |      label2/\n",
            " |        xxx.png\n",
            " |        xxy.png\n",
            " |        xxz.png\n",
            " |    split_name/  # Ex: 'test'\n",
            " |      ...\n",
            " |  ```\n",
            " |  \n",
            " |  To use it:\n",
            " |  \n",
            " |  ```\n",
            " |  builder = tfds.ImageFolder('path/to/image_dir/')\n",
            " |  print(builder.info)  # num examples, labels... are automatically calculated\n",
            " |  ds = builder.as_dataset(split='train', shuffle_files=True)\n",
            " |  tfds.show_examples(ds, builder.info)\n",
            " |  ```\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      ImageFolder\n",
            " |      tensorflow_datasets.core.dataset_builder.DatasetBuilder\n",
            " |      tensorflow_datasets.core.registered.RegisteredDataset\n",
            " |      abc.ABC\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, root_dir: str, *, shape: Union[Tuple[Union[int, NoneType], ...], List[Union[int, NoneType]], NoneType] = None, dtype: Union[tensorflow.python.framework.dtypes.DType, NoneType] = None)\n",
            " |      Construct the `DatasetBuilder`.\n",
            " |      \n",
            " |      Args:\n",
            " |        root_dir: Path to the directory containing the images.\n",
            " |        shape: Image shape forwarded to `tfds.features.Image`.\n",
            " |        dtype: Image dtype forwarded to `tfds.features.Image`.\n",
            " |  \n",
            " |  download_and_prepare(self, **kwargs)\n",
            " |      Downloads and prepares dataset for reading.\n",
            " |      \n",
            " |      Args:\n",
            " |        download_dir: `str`, directory where downloaded files are stored.\n",
            " |          Defaults to \"~/tensorflow-datasets/downloads\".\n",
            " |        download_config: `tfds.download.DownloadConfig`, further configuration for\n",
            " |          downloading and preparing dataset.\n",
            " |      \n",
            " |      Raises:\n",
            " |        IOError: if there is not enough disk space available.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  VERSION = Version('1.0.0')\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  name = 'image_folder'\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  as_dataset(self, split=None, *, batch_size=None, shuffle_files=False, decoders=None, read_config=None, as_supervised=False)\n",
            " |      Constructs a `tf.data.Dataset`.\n",
            " |      \n",
            " |      Callers must pass arguments as keyword arguments.\n",
            " |      \n",
            " |      The output types vary depending on the parameters. Examples:\n",
            " |      \n",
            " |      ```python\n",
            " |      builder = tfds.builder('imdb_reviews')\n",
            " |      builder.download_and_prepare()\n",
            " |      \n",
            " |      # Default parameters: Returns the dict of tf.data.Dataset\n",
            " |      ds_all_dict = builder.as_dataset()\n",
            " |      assert isinstance(ds_all_dict, dict)\n",
            " |      print(ds_all_dict.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            " |      \n",
            " |      assert isinstance(ds_all_dict['test'], tf.data.Dataset)\n",
            " |      # Each dataset (test, train, unsup.) consists of dictionaries\n",
            " |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            " |      #  'text': <tf.Tensor: .. dtype=string, numpy=b\"I've watched the movie ..\">}\n",
            " |      # {'label': <tf.Tensor: .. dtype=int64, numpy=1>,\n",
            " |      #  'text': <tf.Tensor: .. dtype=string, numpy=b'If you love Japanese ..'>}\n",
            " |      \n",
            " |      # With as_supervised: tf.data.Dataset only contains (feature, label) tuples\n",
            " |      ds_all_supervised = builder.as_dataset(as_supervised=True)\n",
            " |      assert isinstance(ds_all_supervised, dict)\n",
            " |      print(ds_all_supervised.keys())  # ==> ['test', 'train', 'unsupervised']\n",
            " |      \n",
            " |      assert isinstance(ds_all_supervised['test'], tf.data.Dataset)\n",
            " |      # Each dataset (test, train, unsup.) consists of tuples (text, label)\n",
            " |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            " |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            " |      \n",
            " |      # Same as above plus requesting a particular split\n",
            " |      ds_test_supervised = builder.as_dataset(as_supervised=True, split='test')\n",
            " |      assert isinstance(ds_test_supervised, tf.data.Dataset)\n",
            " |      # The dataset consists of tuples (text, label)\n",
            " |      # (<tf.Tensor: ... dtype=string, numpy=b\"I've watched the movie ..\">,\n",
            " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            " |      # (<tf.Tensor: ... dtype=string, numpy=b\"If you love Japanese ..\">,\n",
            " |      #  <tf.Tensor: ... dtype=int64, numpy=1>)\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |        split: Which split of the data to load (e.g. `'train'`, `'test'`\n",
            " |          `['train', 'test']`, `'train[80%:]'`,...). See our\n",
            " |          [split API guide](https://www.tensorflow.org/datasets/splits).\n",
            " |          If `None`, will return all splits in a `Dict[Split, tf.data.Dataset]`.\n",
            " |        batch_size: `int`, batch size. Note that variable-length features will\n",
            " |          be 0-padded if `batch_size` is set. Users that want more custom behavior\n",
            " |          should use `batch_size=None` and use the `tf.data` API to construct a\n",
            " |          custom pipeline. If `batch_size == -1`, will return feature\n",
            " |          dictionaries of the whole dataset with `tf.Tensor`s instead of a\n",
            " |          `tf.data.Dataset`.\n",
            " |        shuffle_files: `bool`, whether to shuffle the input files. Defaults to\n",
            " |          `False`.\n",
            " |        decoders: Nested dict of `Decoder` objects which allow to customize the\n",
            " |          decoding. The structure should match the feature structure, but only\n",
            " |          customized feature keys need to be present. See\n",
            " |          [the guide](https://github.com/tensorflow/datasets/tree/master/docs/decode.md)\n",
            " |          for more info.\n",
            " |        read_config: `tfds.ReadConfig`, Additional options to configure the\n",
            " |          input pipeline (e.g. seed, num parallel reads,...).\n",
            " |        as_supervised: `bool`, if `True`, the returned `tf.data.Dataset`\n",
            " |          will have a 2-tuple structure `(input, label)` according to\n",
            " |          `builder.info.supervised_keys`. If `False`, the default,\n",
            " |          the returned `tf.data.Dataset` will have a dictionary with all the\n",
            " |          features.\n",
            " |      \n",
            " |      Returns:\n",
            " |        `tf.data.Dataset`, or if `split=None`, `dict<key: tfds.Split, value:\n",
            " |        tfds.data.Dataset>`.\n",
            " |      \n",
            " |        If `batch_size` is -1, will return feature dictionaries containing\n",
            " |        the entire dataset in `tf.Tensor`s instead of a `tf.data.Dataset`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            " |  \n",
            " |  builder_config\n",
            " |      `tfds.core.BuilderConfig` for this builder.\n",
            " |  \n",
            " |  builder_configs\n",
            " |      classmethod(function) -> method\n",
            " |      \n",
            " |      Convert a function to be a class method.\n",
            " |      \n",
            " |      A class method receives the class as implicit first argument,\n",
            " |      just like an instance method receives the instance.\n",
            " |      To declare a class method, use this idiom:\n",
            " |      \n",
            " |        class C:\n",
            " |            @classmethod\n",
            " |            def f(cls, arg1, arg2, ...):\n",
            " |                ...\n",
            " |      \n",
            " |      It can be called either on the class (e.g. C.f()) or on an instance\n",
            " |      (e.g. C().f()).  The instance is ignored except for its class.\n",
            " |      If a class method is called for a derived class, the derived class\n",
            " |      object is passed as the implied first argument.\n",
            " |      \n",
            " |      Class methods are different than C++ or Java static methods.\n",
            " |      If you want those, see the staticmethod builtin.\n",
            " |  \n",
            " |  canonical_version\n",
            " |  \n",
            " |  code_path\n",
            " |      classmethod(function) -> method\n",
            " |      \n",
            " |      Convert a function to be a class method.\n",
            " |      \n",
            " |      A class method receives the class as implicit first argument,\n",
            " |      just like an instance method receives the instance.\n",
            " |      To declare a class method, use this idiom:\n",
            " |      \n",
            " |        class C:\n",
            " |            @classmethod\n",
            " |            def f(cls, arg1, arg2, ...):\n",
            " |                ...\n",
            " |      \n",
            " |      It can be called either on the class (e.g. C.f()) or on an instance\n",
            " |      (e.g. C().f()).  The instance is ignored except for its class.\n",
            " |      If a class method is called for a derived class, the derived class\n",
            " |      object is passed as the implied first argument.\n",
            " |      \n",
            " |      Class methods are different than C++ or Java static methods.\n",
            " |      If you want those, see the staticmethod builtin.\n",
            " |  \n",
            " |  data_dir\n",
            " |  \n",
            " |  info\n",
            " |      `tfds.core.DatasetInfo` for this builder.\n",
            " |  \n",
            " |  supported_versions\n",
            " |  \n",
            " |  url_infos\n",
            " |      classmethod(function) -> method\n",
            " |      \n",
            " |      Convert a function to be a class method.\n",
            " |      \n",
            " |      A class method receives the class as implicit first argument,\n",
            " |      just like an instance method receives the instance.\n",
            " |      To declare a class method, use this idiom:\n",
            " |      \n",
            " |        class C:\n",
            " |            @classmethod\n",
            " |            def f(cls, arg1, arg2, ...):\n",
            " |                ...\n",
            " |      \n",
            " |      It can be called either on the class (e.g. C.f()) or on an instance\n",
            " |      (e.g. C().f()).  The instance is ignored except for its class.\n",
            " |      If a class method is called for a derived class, the derived class\n",
            " |      object is passed as the implied first argument.\n",
            " |      \n",
            " |      Class methods are different than C++ or Java static methods.\n",
            " |      If you want those, see the staticmethod builtin.\n",
            " |  \n",
            " |  version\n",
            " |  \n",
            " |  versions\n",
            " |      Versions (canonical + availables), in preference order.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from tensorflow_datasets.core.dataset_builder.DatasetBuilder:\n",
            " |  \n",
            " |  BUILDER_CONFIGS = []\n",
            " |  \n",
            " |  MANUAL_DOWNLOAD_INSTRUCTIONS = None\n",
            " |  \n",
            " |  RELEASE_NOTES = {}\n",
            " |  \n",
            " |  SUPPORTED_VERSIONS = []\n",
            " |  \n",
            " |  __annotations__ = {'RELEASE_NOTES': typing.ClassVar[typing.Dict[str, s...\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            " |  \n",
            " |  __init_subclass__(skip_registration=False, **kwargs) from abc.ABCMeta\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from tensorflow_datasets.core.registered.RegisteredDataset:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cmo4JTWAS0Y"
      },
      "source": [
        "https://www.tensorflow.org/guide/keras/preprocessing_layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZS_CpQc77OS"
      },
      "source": [
        "from tensorflow.keras.layers.experimental import preprocessing"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3hmEIOKAwTo",
        "outputId": "71f1fff7-4728-4f0b-c641-dfe35d2c184c"
      },
      "source": [
        "help(preprocessing)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on package tensorflow.keras.layers.experimental.preprocessing in tensorflow.keras.layers.experimental:\n",
            "\n",
            "NAME\n",
            "    tensorflow.keras.layers.experimental.preprocessing - Public API for tf.keras.layers.experimental.preprocessing namespace.\n",
            "\n",
            "PACKAGE CONTENTS\n",
            "\n",
            "\n",
            "FILE\n",
            "    /usr/local/lib/python3.7/dist-packages/tensorflow/keras/layers/experimental/preprocessing/__init__.py\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf-DdPBHA8Fj"
      },
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehlbd1TEBF-Y"
      },
      "source": [
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/Resizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "wrLIGstvDX-M",
        "outputId": "523d0e46-babd-4133-ef05-b16b53682a47"
      },
      "source": [
        "pokemon_data = pokemon.load_data()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-54426fd1bbb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpokemon_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpokemon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'ImageFolder' object has no attribute 'load_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "RAVmRFrhAUf9",
        "outputId": "dd1aeb48-0374-471c-c087-faf8b3b90b93"
      },
      "source": [
        "transformer = keras.Sequential(\n",
        "    [\n",
        "     preprocessing.Resizing(64, 64),\n",
        "     preprocessing.Normalization(mean = 0.5, variance = 0.5),\n",
        "    ])\n",
        "transformer.adapt(pokemon)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-9a850a284591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m      \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     ])\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokemon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'adapt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-kWLSKaExCx"
      },
      "source": [
        "resize = preprocessing.Resizing(64, 64)\n",
        "resize.adapt(pokemon)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMU3uH_wFLE3",
        "outputId": "b3d7cec5-8e6d-4a75-ec03-70dc8ab54c2b"
      },
      "source": [
        "print(pokemon)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow_datasets.core.folder_dataset.image_folder.ImageFolder object at 0x7f6645219e90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "qkGN_R59Fhyu",
        "outputId": "4bb0159f-eb83-4147-a1c5-fd9fcf2bbcb8"
      },
      "source": [
        "normal = preprocessing.Normalization(mean = 0.5, variance = 0.5)\n",
        "normal.adapt(pokemon)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-555a34926b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnormal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnormal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokemon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_preprocessing_layer.py\u001b[0m in \u001b[0;36madapt\u001b[0;34m(self, data, reset_state)\u001b[0m\n\u001b[1;32m    159\u001b[0m           \u001b[0;34m'`adapt()` requires a batched Dataset, a Tensor, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m           \u001b[0;34m'or a Numpy array as input, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m           'got {}'.format(type(data)))\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: `adapt()` requires a batched Dataset, a Tensor, or a Numpy array as input, got <class 'tensorflow_datasets.core.folder_dataset.image_folder.ImageFolder'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "n7_z4MTTF6qb",
        "outputId": "25f2d5a2-1a3b-4d31-db74-c970b98a7d41"
      },
      "source": [
        "totensor = preprocessing.ToTensor()\n",
        "totensor.adapt(pokemon)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-44980baf20a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtotensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madapt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokemon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers.experimental.preprocessing' has no attribute 'ToTensor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "UF0iB5hJKKwx",
        "outputId": "3e6a9b94-b50a-45e8-eb4c-e2fc7d1f86b0"
      },
      "source": [
        "pokemon = pokemon.as_dataset(split='train', shuffle_files=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-edc2d7d59ed6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpokemon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpokemon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mas_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mas_supervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_supervised\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     )\n\u001b[0;32m--> 534\u001b[0;31m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_nested\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_single_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_tuple)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_build_single_dataset\u001b[0;34m(self, split, shuffle_files, batch_size, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mshuffle_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mdecoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mread_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m     )\n\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Auto-cache small datasets which are small enough to fit in memory.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/folder_dataset/image_folder.py\u001b[0m in \u001b[0;36m_as_dataset\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    146\u001b[0m           \u001b[0;34m'Unrecognized split {}. Subsplit API not yet supported for {}. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m           'Split name should be one of {}.'.format(\n\u001b[0;32m--> 148\u001b[0;31m               split, type(self).__name__, list(self.info.splits.keys())))\n\u001b[0m\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;31m# Extract all labels/images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unrecognized split train. Subsplit API not yet supported for ImageFolder. Split name should be one of []."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRVYw5aAKsZI"
      },
      "source": [
        "pokemon = pokemon.as_dataset(shuffle_files=True)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "0YHS2BgsKuou",
        "outputId": "0e3a01f4-e32d-4737-f660-1e861c766f1b"
      },
      "source": [
        "tfds.show_examples(pokemon, builder.info)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7567af5ce133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'builder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "TVueuivZK01w",
        "outputId": "3380b991-7bdd-45c5-923b-352e7cf4a0bd"
      },
      "source": [
        "tfds.show_examples(pokemon, pokemon.info)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-b9c79ad95e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpokemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpokemon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'info'"
          ]
        }
      ]
    }
  ]
}