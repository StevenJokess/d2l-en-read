{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitpytorchconda70fdc7f787194f4c972bb3207dd25917",
   "display_name": "Python 3.8.3 64-bit ('pytorch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/StevenJokes/d2l-en-read/blob/master/chapter-generative-adversarial-networks/PR1308.ipynb\n",
    "\n",
    "https://www.geeksforgeeks.org/python-range-function/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/master/generated/torch.normal.html\n",
    "no device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim, device=d2l.try_gpu()):\n",
    "    loss = BCELoss()\n",
    "    for w in net_D.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "        \n",
    "    for w in net_G.parameters():\n",
    "        nn.init.normal_(w, 0, 0.02)\n",
    "    net_D.zero_grad()\n",
    "    net_G.zero_grad()\n",
    "    trainer_hp = {'learning_rate': lr, 'beta1': 0.5}\n",
    "    trainer_D = torch.optim.Adam(net_D.parameters(), trainer_hp)\n",
    "    trainer_G = torch.optim.Adam(net_G.parameters(), trainer_hp)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "                            legend=['discriminator', 'generator'])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for X, _ in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = torch.random.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n",
    "            X, Z = X.to(device), Z.to(device)\n",
    "                        metric.add(update_D(X, Z, net_D, net_G, loss, trainer_D),\n",
    "                       update_G(Z, net_D, net_G, loss, trainer_G),\n",
    "                       batch_size)\n",
    "            # Show generated examples\n",
    "            Z = np.random.normal(0, 1, size=(21, latent_dim, 1, 1))        \n",
    "            # Normalize the synthetic data to N(0, 1)\n",
    "            fake_x = net_G(Z).permute(0, 2, 3, 1) / 2 + 0.5\n",
    "            imgs = np.concatenate(\n",
    "            [np.concatenate([fake_x[i * 7 + j] for j in range(7)], axis=1)\n",
    "             for i in range(len(fake_x)//7)], axis=0)\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].imshow(imgs.numpy())\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", force_reinit=True, ctx=device)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "animator.axes[1].imshow(imgs.numpy())\n",
    "# Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net_D, net_G, data_iter, num_epochs, lr, latent_dim,\n",
    "          device=d2l.try_gpu()):\n",
    "    loss = BCELoss()\n",
    "    net_D.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)\n",
    "    net_G.initialize(init=init.Normal(0.02), force_reinit=True, ctx=device)\n",
    "    trainer_hp = {'learning_rate': lr, 'beta1': 0.5}\n",
    "    trainer_D = gluon.Trainer(net_D.collect_params(), 'adam', trainer_hp)\n",
    "    trainer_G = gluon.Trainer(net_G.collect_params(), 'adam', trainer_hp)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs], nrows=2, figsize=(5, 5),\n",
    "                            legend=['discriminator', 'generator'])\n",
    "    animator.fig.subplots_adjust(hspace=0.3)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Train one epoch\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(3)  # loss_D, loss_G, num_examples\n",
    "        for X, _ in data_iter:\n",
    "            batch_size = X.shape[0]\n",
    "            Z = np.random.normal(0, 1, size=(batch_size, latent_dim, 1, 1))\n",
    "            X, Z = X.as_in_ctx(device), Z.as_in_ctx(device),\n",
    "            metric.add(d2l.update_D(X, Z, net_D, net_G, loss, trainer_D),\n",
    "                       d2l.update_G(Z, net_D, net_G, loss, trainer_G),\n",
    "                       batch_size)\n",
    "        # Show generated examples\n",
    "        Z = np.random.normal(0, 1, size=(21, latent_dim, 1, 1), ctx=device)\n",
    "        # Normalize the synthetic data to N(0, 1)\n",
    "        fake_x = net_G(Z).transpose(0, 2, 3, 1) / 2 + 0.5\n",
    "        imgs = np.concatenate(\n",
    "            [np.concatenate([fake_x[i * 7 + j] for j in range(7)], axis=1)\n",
    "             for i in range(len(fake_x)//7)], axis=0)\n",
    "        animator.axes[1].cla()\n",
    "        animator.axes[1].imshow(imgs.asnumpy())\n",
    "        # Show the losses\n",
    "        loss_D, loss_G = metric[0] / metric[2], metric[1] / metric[2]\n",
    "        animator.add(epoch, (loss_D, loss_G))\n",
    "    print(f'loss_D {loss_D:.3f}, loss_G {loss_G:.3f}, '\n",
    "          f'{metric[2] / timer.stop():.1f} examples/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.jianshu.com/p/06f605cad8fb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_iter, valid_iter, num_epochs, lr, wd, device, lr_period,\n",
    "          lr_decay):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.fc.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
    "    net = net.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, n, start = 0.0, 0, time.time()\n",
    "        if epoch > 0 and epoch % lr_period == 0:  # 每lr_period个epoch，学习率衰减一次\n",
    "            lr = lr * lr_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "        time_s = \"time %.2f sec\" % (time.time() - start)\n",
    "        if valid_iter is not None:\n",
    "            valid_loss, valid_acc = evaluate_loss_acc(valid_iter, net, device)\n",
    "            epoch_s = (\"epoch %d, train loss %f, valid loss %f, valid acc %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n, valid_loss, valid_acc))\n",
    "        else:\n",
    "            epoch_s = (\"epoch %d, train loss %f, \"\n",
    "                       % (epoch + 1, train_l_sum / n))\n",
    "        print(epoch_s + time_s + ', lr ' + str(lr))"
   ]
  }
 ]
}