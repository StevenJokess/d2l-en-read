

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-06-29 22:51:53
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-08-05 22:07:20
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/PR-1081/chapter_generative-adversarial-networks/gan.html
-->

# 生成对抗网络

在这本书的大部分时间里，我们都在讨论如何做出预测。以这样或那样的形式，我们使用深层神经网络学习从数据点到标签的映射。这种学习方式被称为辨别学习，比如，我们希望能够区分猫的照片和狗的照片。分类器和回归因子都是区分性学习的实例。通过反向传播训练的神经网络已经颠覆了我们对大型复杂数据集上的区分性学习的认识。高分辨率图像的分类精度在短短5-6年内已经从无用到人类水平(带有一些警告)。我们将让你省去另一个关于所有其他区分性任务的高谈阔论，深层神经网络在这些任务中表现出色得惊人。

但是机器学习不仅仅是解决鉴别性任务。例如，给定一个没有任何标签的大型数据集，我们可能希望学习一个能够精确捕获此数据特征的模型。给定这样一个模型，我们可以对类似于训练数据分布的合成数据点进行抽样。例如，给定一个大的人脸照片语料库，我们可能希望能够生成一个新的真实感图像，看起来似乎是来自同一数据集。这种学习被称为生成模型。

直到最近，我们还没有一种方法可以合成出新颖的真实感图像。但是深层神经网络的成功为区别性学习开辟了新的可能性。在过去的三年里，一个大的趋势就是应用有区别的深度网来克服我们通常不认为是监督式学习问题的挑战。递归神经网络语言模型就是使用判别网络的一个例子(训练用来预测下一个人物) ，一旦训练成功，这个网络就可以充当生成模型。

2014年，一篇突破性的论文介绍了生成对抗网络(GANs)[ Goodfellow et al. 2014](http://preview.d2l.ai/d2l-en/PR-1081/chapter_references/zreferences.html#goodfellow-pouget-abadie-mirza-ea-2014) ，这是一种利用判别模型的力量来获得良好生成模型的聪明的新方法。甘斯的核心思想是，如果我们不能区分虚假数据和真实数据，那么数据生成器就是好的。在统计学中，回答数据集 x = { x1，... ，xn } 和 x ′ = { x ′1，... ，x ′ n } 是否来自同一分布的问题称为双样本检验-a 检验。大多数统计论文与广义网络的主要区别在于后者以建设性的方式使用这一思想。换句话说，他们不仅仅是训练一个模型说“嘿，这两个数据集看起来不像是来自同一个分布” ，他们使用双样本测试来为生成模型提供训练信号。这使我们能够改进数据生成器，直到它生成类似于真实数据的东西。至少，它需要欺骗分类器。即使我们的分类器是最先进的深层神经网络。

TODO:FIG

图17.1.1生成对抗性网络

GAN 结构如图17.1.1所示。正如你所看到的，在 GAN 架构中有两个部分——首先，我们需要一个设备(比方说，一个深层网络，但是它可以是任何东西，比如游戏渲染引擎) ，这个设备有可能生成看起来像真实的东西的数据。如果我们正在处理图像，这需要生成图像。如果我们正在处理语音，它需要生成音频序列，等等。我们称之为发电机网络。第二部分是鉴别器网络。它试图区分假数据和真数据。这两个网络互相竞争。生成器网络试图欺骗鉴别器网络。在这一点上，鉴别器网络适应新的伪造数据。这些信息，依次被用来改善发电机网络等等。

鉴别器是一个二进制分类器，用来区分输入的 x 是真实的(来自真实数据)还是虚假的(来自生成器)。对于输入 x，判别器输出一个标量预测 o ∈ r，例如使用一个隐含大小为1的稠密层，然后应用 S形函数方法得到预测概率 d (x) = 1 / (1 + e-o)。假设真实数据的标签 y 为11，伪数据的标签 y 为0。我们训练鉴别器使交叉熵损失最小，即,

TODO:MATH

对于生成元，首先从随机源中提取一些参数 z ∈ Rd，例如，正态分布 z ∼∼ n (0,1)。我们通常称 z 为潜变量。然后应用一个函数生成 x ′ = g (z)。生成器的目标是欺骗鉴别器将 x ′ = g (z)分类为真实数据，即 d (g (z))≈1 d (g (z))≈1。换句话说，对于给定的鉴别器 d d，我们更新生成器 g 的参数，使得当 y = 0时交叉熵损失最大化，即,

TODO:MATH

如果生成器工作正常，则 d (x ′)≈1，因此上述损失接近于0，导致梯度太小，无法使鉴别器取得良好的进展。因此，我们通常尽量减少以下损失:

TODO:MATH

只是将x'= G（z）x'= G（z）馈入鉴别器，但给出标签y = 1y = 1。

综上所述，DD和GG正在玩具有综合目标功能的“ minimax”游戏：

TODO:MATH

许多GANs应用程序都在图像上下文中。作为演示目的，我们首先满足于拟合一个更简单的分布。我们将说明如果我们使用GANs来建立世界上最低效的高斯参数估计会发生什么。让我们开始吧。

TODO:CODE

## 生成一些“真”数据

尽管这将是世界最无聊的例子，我们简单地从高斯分布生成些例子。

TODO:CODE

让我们看看我们得到了什么。这应该是用平均b和协方差矩阵ATA任意地进行高斯偏移。

TODO:CODE

## 生成器

我们的生成网络将是最简单的网络-一个单层线性模型。这是因为我们将用高斯数据生成器驱动线性网络。因此，它实际上只需要学习参数来完美地伪造东西。

TODO:CODE

## 鉴别器

对于鉴别器，我们将更有鉴赏力：我们用一个3层的多层感知器来使事情更有趣。

TODO:CODE

## 训练

首先我们定义一个函数来更新鉴别器。

TODO:CODE

生成器的更新也是类似的。这里我们重用了交叉熵损失，但将伪数据的标签从0改为1。

TODO:CODE

鉴别器和产生器都对交叉熵损失进行二元逻辑回归。我们使用亚当使训练过程平稳。在每次迭代中，我们首先更新鉴别器，然后更新生成器。我们将损失和生成的示例可视化。

TODO:CODE

现在我们指定超参数来拟合高斯分布。

TODO:CODE

## 小结

- 生成对抗网络由两个深度网络组成，即生成网络和鉴别网络。
- 该发生器通过最大的交叉熵损失，即maxlog(D(x'))，来产生尽可能接近真实图像的图像来欺骗鉴别器。
- 该鉴别器通过最大限度地减小交叉熵损失，即min-ylogD(x) (1-y) log(1-d (x))，来最大限度地减小交叉熵损失，将生成的图像与真实的图像进行鉴别。

## 练习

当生成器赢的时候，均衡存在吗？即，在有限样本上，鉴别器最终无法区分这两个分布。
