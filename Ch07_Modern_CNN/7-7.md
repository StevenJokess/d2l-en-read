

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-17 17:50:48
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-29 20:23:28
 * @Description:MT, improve
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-modern/densenet.html
 * https://zh.d2l.ai/chapter_convolutional-neural-networks/densenet.html
-->

# 稠密连接网络（DenseNet）

ResNet极大地改变了如何参数化深度网络功能的观点。从某种程度上说，DenseNet是这一理论的逻辑延伸。为了理解如何实现它，让我们在理论上绕个小弯。回想一下函数的泰勒展开式。对于点x=0，它可以写成

TODO:MATH

关键在于它将一个函数分解成越来越高阶的项。同样，ResNet将函数分解为

最后，所有这些功能被合并到MLP中以再次减少特性的数量。就实现而言，这相当简单:我们将它们连接起来，而不是添加术语。DenseNet这个名字源于变量之间的依赖关系图变得非常密集。这样的链的最后一层与之前的所有层紧密相连。密集连接如图7.7.2所示。

构成DenseNet的主要组件是密集的块和过渡层。前者定义如何连接输入和输出，而后者控制通道的数量，以使其不太大。

## 从 ResNet 到 DenseNet

回想一下函数的泰勒展开式，对于点 x = 0，可以写成

TODO:MATH

关键是它将功能分解为越来越高级的阶。同样，ResNet将功能分解为

f（x）= x + g（x）。

也就是说，ResNet将ff分解为一个简单的线性项和一个更复杂的非线性项。如果我们想超越两个条款怎么办？ [Huang et al。，2017]

图7.7.1在跨层连接中，ResNet (左)和 DenseNet (右)的主要区别: 使用加法和串联。

TODO:MATH

最后，在 MLP 中将所有这些功能结合起来，再次减少了特征的数量。就实现而言，这非常简单: 我们不添加术语，而是将它们连接起来。DenseNet 这个名字来源于变量之间的依赖关系图变得非常密集这一事实。这种链的最后一层与之前的所有层密切相连。密集连接如图7.7.2所示。

图7.7.2 DenseNet 的致密连接。

组成 DenseNet 的主要成分是致密块体和过渡层。前者定义输入和输出如何连接，而后者控制通道数量，使其不太大。

以DenseNet的形式提出了一种解决方案，该体系结构报告了ImageNet数据集的记录性能。

TODO:PIC

## 稠密块

DenseNet使用ResNet的经过修改的“批处理规范化，激活和卷积”体系结构（请参见7.6节中的练习）。首先，我们在conv_block函数中实现此体系结构。

TODO:CODE

密集块由多个conv_block单元组成，每个单元都使用相同数量的输出通道。但是，在正向计算中，我们将每个块的输入和输出在通道维度上连接起来。

TODO:CODE

在下面的示例中，我们定义了一个卷积块（DenseBlock），其中有两个包含10个输出通道的块。当使用具有3个通道的输入时，我们将获得3 + 2×10 = 233 + 2×10 = 23个通道的输出。卷积块通道的数量控制输出通道数量相对于输入通道数量的增加。这也称为增长率。

TODO:CODE

## 过渡层

由于每个密集块都会增加通道数量，因此添加过多通道会导致模型过于复杂。过渡层用于控制模型的复杂性。它通过使用1×11×1卷积层来减少通道数，并将平均池化层的高度和宽度减半，步幅为2，从而进一步降低了模型的复杂性。

TODO:CODE

在前面的示例中，将具有10个通道的过渡层应用于密集块的输出。这样可以将输出通道的数量减少到10，并使高度和宽度减半。

TODO:CODE

## DenseNet模型

接下来，我们将构建一个DenseNet模型。DenseNet首先使用与ResNet相同的单一卷积层和最大池化层。

然后，类似于ResNet使用的四个残差块，DenseNet使用四个密集块。与ResNet相似，我们可以设置每个密集块中使用的卷积层数。在这里，我们将其设置为4，与上一节中的ResNet-18一致。此外，我们将密集块中卷积层的通道数（即增长率）设置为32，因此将128个通道添加到每个密集块中。

在ResNet中，每个模块之间的高度和宽度通过步长为2的残差块减小。在这里，我们使用过渡层将高度和宽度减半，将通道数减半。

TODO:CODE

与ResNet相似，最后将全局池层和完全连接的层连接起来以产生输出。

TODO:CODE

## 数据获取和训练

由于我们在这里使用的是更深的网络，因此在本节中，我们将输入高度和宽度从224减少到96，以简化计算。

TODO:CODE

## 实验结果[3]

DenseNet只需要较小的Growth rate(12,24)便可以实现state-of-art的性能,结合了Bottleneck和Compression的DenseNet-BC具有远小于ResNet及其变种的参数数量,且无论DenseNet或者DenseNet-BC,都在原始数据集和增广数据集上实现了超越ResNet的性能.

## 小结

* 就跨层连接而言，与ResNet不同，后者将输入和输出添加到一起，DenseNet在通道维度上连接输入和输出。
* 构成DenseNet的主要单元是密集块和过渡层。
* 组成网络时，我们需要通过添加过渡层来再次控制通道的数量，以保持尺寸的可控性。

## 练习

1. 为什么在过渡层中使用平均池而不是最大池？
1. DenseNet论文中提到的优点之一是其模型参数小于ResNet的模型参数。为什么会这样呢？
1. DenseNet被批评的一个问题是其高内存消耗。
   1. 真的是这样吗？ 尝试将输入形状更改为224×224，以查看实际的（GPU）内存消耗。
   2. 您能想到一种减少内存消耗的替代方法吗？ 您将如何更改框架？
1. 实现[Huang et al。，2017]表1中提出的各种DenseNet版本。
1. 如果我们只对ResNet的x和f(x)感兴趣，为什么不需要连接术语？ 为什么在DenseNet中需要两层以上空间？
1. 为完全连接的网络设计一个DenseNet，并将其应用于房屋价格预测任务。

---

ResNet的问题在于训练了一个深层的网络，但是网络中许多层可能贡献很少或根本没有信息。为了解决此问题，DenseNet使用了跨层连接，以前馈的方式将每一层连接到更深的层中，假设网络有l层，建立了(l(l+1))/2个直接连接，并且DenseNet对先前层的特征使用了级联而不是相加，可以显式区分网络不同深度的信息。

[2]: https://0809zheng.github.io/2020/06/03/CNN-architecture.html
[3]: https://www.zhihu.com/org/bei-jing-zhang-liang-wu-xian-ke-ji-you-xian-gong-si/posts?page=7
