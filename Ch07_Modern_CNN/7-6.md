

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-17 17:31:01
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-17 17:46:17
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-modern/resnet.html
 * https://zh.d2l.ai/chapter_convolutional-neural-networks/resnet.html
-->

# 残差网络（ResNet）

随着我们设计越来越深的网络，必须了解增加层如何增加网络的复杂性和表现力。 更重要的是设计网络的能力，其中添加层使网络严格更具表现力，而不仅仅是不同。 为了取得一些进展，我们需要一些理论。

## 函数类
、
考虑FF，特定网络体系结构（以及学习率和其他超参数设置）可以达到的功能类别。 也就是说，对于所有f∈Ff∈F，都有一些参数WW可以通过在合适的数据集上进行训练而获得。 让我们假设f ∗ f ∗是我们真正想要找到的函数。 如果它是FF，则我们的状态很好，但是通常我们不会那么幸运。 相反，我们将尝试找到一些f * FfF *，这是FF中最好的选择。 例如，我们可以尝试通过解决以下优化问题来找到它：

TODO:MATH

只有合理的假设是，如果我们设计一个不同且功能更强大的体系结构F'F'，我们应该会获得更好的结果。 换句话说，我们期望f ∗ F′fF′∗比f ∗ FfF ∗“更好”。 但是，如果F⊈F′F⊈F′不能保证甚至发生这种情况。 实际上，f ∗ F′fF′∗可能更糟。 这是我们在实践中经常遇到的情况-添加层不仅使网络更具表现力，而且有时以不太可预测的方式改变网络。 图7.6.1稍微抽象地说明了这一点。

TODO:PIC 左：非嵌套函数类。 实际上，随着复杂度的增加，距离可能会增加。 正确：使用嵌套函数类不会发生这种情况。

只有当较大的函数类包含较小的函数类时，我们才能保证增加函数类会严格增加网络的表达能力。这是何等人在2016年研究超深度计算机视觉模型时考虑的问题。ResNet 的核心思想是，每一个额外的层都应该包含作为其元素之一的恒等函数。这意味着，如果我们能够将新添加的层训练成一个标识映射 f (x) = x f (x) = x，那么新模型将和原来的模型一样有效。由于新模型可以得到更好的解决方案，以适应训练数据集，增加的层可能使它更容易减少训练错误。更好的是，在一个层中，恒等函数(而不是 null f (x) = 0f (x) = 0)应该是最简单的函数。

这些考虑是相当深刻的，但他们导致了一个令人惊讶的简单的解决方案，残留的块。凭借它，[ He et al. ，2016a ]赢得了2015年 ImageNet 视觉识别挑战赛。该设计对如何构建深层神经网络有着深远的影响。

## 残差块

让我们集中讨论局部神经网络，如下所示。用 x 表示输入。我们假设我们想要通过学习获得的理想映射是 f (x) f (x) ，用作激活函数的输入。左图中虚线框中的部分必须直接适合映射 f (x) f (x)。如果我们不需要这个特定的图层，我们更愿意保留输入的 x，这可能会很棘手。右图中虚线框中的部分现在只需要参数化与恒等式的偏差，因为我们返回 x + f (x) x + f (x)。在实践中，残差映射通常更容易优化。我们只需要设置 f (x) = 0 f (x) = 0。图7.6.2中的右图说明了 ResNet 的基本残留块。类似的体系结构后来被提出用于序列模型，我们将在后面研究。

TODO:PIC 常规块（左）和残余块（右）之间的差异。 在后一种情况下，我们可以使卷积短路。

ResNet遵循VGG完整的3×3卷积层设计。 残留块具有两个3×3卷积层，具有相同数量的输出通道。 每个卷积层之后是批处理归一化层和ReLU激活函数。 然后，我们跳过这两个卷积运算，并在最终的ReLU激活函数之前直接添加输入。 这种设计要求两个卷积层的输出与输入的形状相同，以便可以将它们相加。 如果要更改通道数或步幅，则需要引入一个额外的1×1卷积层，以将输入转换为所需的形状以进行加法运算。 让我们看看下面的代码。

TODO:CODE

此代码生成两种类型的网络：一种类型，在每次使用_1x1conv = False时，在应用ReLU非线性之前将输入添加到输出，另一种类型是在添加之前通过1×11×1卷积调整通道和分辨率。 图7.6.3说明了这一点：

TODO:PIC 左：常规ResNet块； 右：具有1x1卷积的ResNet块

现在让我们看一下输入和输出具有相同形状的情况。

TODO:CODE

我们还可以选择将输出高度和宽度减半，同时增加输出通道的数量。

TODO:CODE

## ResNet模型

ResNet的前两层与我们之前介绍的GoogLeNet相同：7×7卷积层具有64个输出通道，跨度为2，其后是3×3最大池化层，具有2的步幅。区别是在ResNet中的每个卷积层之后添加了批处理规范化层。

TODO:CODE

GoogLeNet使用由Inception块组成的四个块。 但是，ResNet使用由残差块组成的四个模块，每个模块都使用具有相同数量输出通道的几个残差块。 第一个模块中的通道数与输入通道数相同。 由于已经使用了步幅为2的最大合并层，因此不必减小高度和宽度。 在每个后续模块的第一个残差块中，通道数是前一个模块的两倍，高度和宽度减半。

现在，我们实现此模块。 注意，已经在第一个模块上执行了特殊处理。

TODO:CODE

然后，我们将所有剩余块添加到ResNet。 在此，每个模块使用两个剩余块。

TODO:CODE

最后，就像GoogLeNet一样，我们添加一个全局平均池层，然后添加完全连接的层输出。

TODO:CODE

每个模块中有4个卷积层（不包括1×11×1卷积层）。 与第一卷积层和最后的全连接层一起，总共有18层。 因此，此模型通常称为ResNet-18。 通过在模块中配置不同数量的通道和剩余块，我们可以创建不同的ResNet模型，例如更深的152层ResNet-152。 尽管ResNet的主要架构与GoogLeNet相似，但是ResNet的架构更简单，更易于修改。 所有这些因素导致ResNet的快速和广泛使用。 图7.6.4是完整的ResNet-18的示意图。

在训练ResNet之前，让我们观察一下ResNet中不同模块之间输入形状的变化。 与所有以前的体系结构一样，分辨率降低，而通道数却增加，直到全局平均池化层聚合所有功能为止。

TODO:CODE

## 数据采集​​与训练

和以前一样，我们在Fashion-MNIST数据集上训练ResNet。 唯一改变的是由于更复杂的体系结构，学习率再次下降。

TODO:CODE


## 小结

* 残留块允许相对于恒等函数f（x）= xf（x）= x进行参数化。
* 添加残差块会以明确定义的方式增加功能的复杂性。
* 我们可以通过让残差块穿过跨层数据通道来训练有效的深度神经网络。
* 对于卷积和顺序性质，ResNet对后续深度神经网络的设计产生了重大影响。

## 练习

1. 请参阅[He et al。，2016a]中的表1以实现不同的变体。
1. 对于更深层的网络，ResNet引入了“瓶颈”架构来降低模型的复杂性。 尝试实现它。
1. 在后续版本的ResNet中，作者将“卷积，批处理规范化和激活”体系结构更改为“批处理规范化，激活和卷积”体系结构。 自己进行改进。 有关详细信息，请参见[He et al。，2016b]中的图1。
1. 证明如果xx是由ReLU生成的，则ResNet块确实包含身份函数。
1. 为什么即使嵌套函数类也不能无限制地增加函数的复杂性呢？
