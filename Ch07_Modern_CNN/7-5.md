

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-17 01:27:33
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-09-24 19:02:20
 * @Description:MT
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-modern/batch-norm.html
 * https://zh.d2l.ai/chapter_convolutional-neural-networks/batch-norm.html
-->

# 批量归一化

训练深度神经网络很困难。 使它们在合理的时间内收敛可能很棘手。 在本节中，我们描述批处理规范化，这是一种流行且有效的技术，可不断加速深层网络的融合[Ioffe＆Szegedy，2015]。 批处理归一化与剩余块（在7.6节中稍后介绍）一起使练习者可以常规地训练100层以上的网络。

## 训练深度网络

为了激发批处理规范化，让我们回顾一下在训练机器学习模型尤其是神经网络时出现的一些实际挑战。

首先，有关数据预处理的选择通常会对最终结果产生巨大的影响。 回顾我们在预测房价中使用MLP的应用（第4.10节）。 当处理真实数据时，我们的第一步是将我们的输入特征标准化，以使每个特征的平均值为零，方差为1。 直观地讲，这种标准化可以与我们的优化程序很好地配合，因为它以类似的比例将参数放在先验的位置。

其次，对于我们训练的典型MLP或CNN，中间层中的变量（例如，MLP中的仿射变换输出）可能会采用幅度变化很大的值：从输入到输出的各个层，在整个单元中 同一层，随着时间的流逝，由于我们对模型参数进行了更新。 批量归一化的发明人非正式地假设，此类变量分布中的这种漂移可能会阻碍网络的收敛。 凭直觉，我们可以推测，如果一层的变量值是另一层的100倍，则可能需要对学习率进行补偿性调整。

第三，更深层次的网络很复杂，很容易过拟合。 这意味着正则化变得更加关键。

批量归一化应用于各个层（可选地，适用于所有层），其工作方式如下：在每次训练迭代中，我们首先将输入（批量归一化）减去它们的均值并除以它们的标准偏差来归一化 基于当前小批量的统计数据。 接下来，我们应用比例系数和比例偏移。 正是由于这种基于批处理统计信息的规范化，批处理规范化才获得了它的名称。

请注意，如果我们尝试对大小为1的微型批次应用批量归一化，我们将无法学习任何内容。 这是因为在减去均值之后，每个隐藏单位的取值为0！ 您可能会猜到，由于我们将整个部分专门用于批量标准化，并且具有足够大的微型批处理，因此该方法被证明是有效且稳定的。 这里的一个要点是，在应用批量归一化时，批量大小的选择可能比没有3批量归一化时更为重要。

形式上，用x∈Bx∈B表示一个来自于一个小批量BB的输入的批次标准化(BNBN)，批次标准化将xx按照如下表达式进行变换:

TODO:MATH

在(7.5.1)中，为样本均值，为抽样标准偏差，为取样标准偏差。应用标准化后，得到的小批量的均值和单位方差为零。因为单位方差的选择(相对于其他一些神奇的数字)是一个任意的选择，我们通常包括元素明智的尺度参数(scale parameter):注意，需要与其他模型参数一起学习。

因此，在训练过程中，中间层的可变量不会发散，因为批处理归一化会积极地对它们进行中心，并重新将它们调整回给定的平均值和大小(通过的方法是:通过B B B B B和B B j B j B B)。一个实践者的直觉或智慧是批处理规范化似乎允许更积极的学习率。

形式上，我们在(7.5.1)中计算出了(B ^B)和(B ^B) (B ^B)

TODO:MATH

请注意，即使在经验方差估计可能消失的情况下，我们也要向方差估计值添加一个小的常数ϵ> 0ϵ> 0，以确保我们永远不会尝试除以零。 估计值μ^Bμ^ B和σ^Bσ^ B通过使用均值和方差的嘈杂估计来抵消缩放问题。 您可能会认为这种嘈杂应该是一个问题。 事实证明，这实际上是有益的。

事实证明，这是深度学习中经常出现的主题。 由于理论上尚未很好表征的原因，优化中的各种噪声源通常会导致更快的训练和更少的过度拟合：这种变化似乎是一种形式化的正规化。 在一些初步研究中，[Teye等人，2018]和[Luo等人，2018]分别将批处理归一化的属性与贝叶斯先验和惩罚联系起来。 尤其是，这为为什么批量归一化最适合50-10050-100中等大小的小批量生产提供了难题。

修复训练有素的模型，您可能会认为我们更喜欢使用整个数据集来估计均值和方差。 训练完成后，为什么要根据碰巧驻留的批次来对同一幅图像进行不同的分类？ 在训练期间，这种精确的计算是不可行的，因为每次更新模型时，所有数据示例的中间变量都会更改。 但是，一旦训练好模型，我们就可以基于整个数据集计算每个图层变量的均值和方差。 实际上，这是采用批处理归一化模型的标准做法，因此批处理归一化层在训练模式（通过小批量统计信息进行归一化）和预测模式（通过数据集统计信息进行归一化）中具有不同的功能。

现在，我们准备看一下批处理规范化在实践中如何工作。

## 批量归一化层

全连接层和卷积层的批处理规范化实现略有不同。 我们在下面讨论这两种情况。 回想一下，批处理规范化和其他层之间的一个主要区别是，因为批处理规范化同时在一个完整的小批处理上运行，所以我们不能像引入其他层时那样仅仅忽略批处理维。

### 全连接层

将批量归一化应用于完全连接的层时，原始论文会在仿射变换之后和非线性激活函数之前插入批量归一化（以后的应用可能会在激活函数之后立即插入批量归一化）[Ioffe＆Szegedy，2015]。 用xx表示完全连接层的输入，用Wx + bWx + b（权重参数WW和偏置参数bb）表示仿射变换，用ϕϕ表示激活函数，我们可以表示批处理的计算， 启用标准化的全连接层输出hh，如下所示：

（7.5.3）

h ＝ ϕ（BN（Wx + b））。

回想一下，均值和方差是在应用转换的同一小批量上计算的。

### 卷积层

同样，对于卷积层，我们可以在卷积之后和非线性激活函数之前应用批量归一化。 当卷积具有多个输出通道时，我们需要对这些通道的每个输出进行批量归一化，并且每个通道都有其自己的scale和shift参数，两者均为标量。 假设我们的小批处理包含mm个示例，并且对于每个通道，卷积的输出具有高度pp和宽度qq。 对于卷积层，我们同时对每个输出通道的m⋅p⋅qm⋅p⋅q元素执行每批归一化。 因此，我们在计算均值和方差时收集所有空间位置上的值，因此在给定通道内应用相同的均值和方差以标准化每个空间位置处的值。

### 预测期间批量归一化

正如我们前面提到的，批处理标准化在训练模式和预测模式中通常表现不同。首先，当我们训练了模型后，样本均值中的噪声和小批量估计产生的样本方差就不再可取了。其次，我们可能没有能力计算每批标准化统计数据。例如，我们可能需要应用我们的模型一次做出一个预测。

通常，在训练之后，我们使用整个数据集计算变量统计量的稳定估计，然后在预测时间固定它们。因此，批处理归一化在训练和测试时表现不同。回想一下，dropout也展示了这个特征。

## 从头开始实施

下面，我们从头开始使用张量实现批处理归一化层。

现在，我们可以创建一个适当的BatchNorm层。 我们的图层将保持适当的比例系数gamma和shiftβ，这两个参数都将在训练过程中进行更新。 此外，我们的层将保持均值和方差的移动平均值，以供模型预测期间后续使用。

撇开算法细节，请注意我们实现该层的基础设计模式。 通常，我们在单独的函数（例如batch_norm）中定义数学。 然后，我们将此功能集成到自定义层中，该层的代码主要处理簿记事项，例如将数据移至正确的设备上下文，分配和初始化任何必需的变量，跟踪移动平均值（此处指均值和方差）等等。 。 这种模式可以使数学与样板代码完全分开。 另请注意，为方便起见，我们在此无需担心自动推断输入形状，因此我们需要在整个过程中指定特征数量。 不用担心，深度学习框架中的高级批处理规范化API将为我们解决此问题，我们将在以后进行演示。


## 在LeNet中应用批处理规范化

要了解如何在上下文中应用BatchNorm，下面我们将其应用于传统的LeNet模型（第6.6节）。 回想一下，在卷积层或完全连接的层之后但在相应的激活函数之前应用了批量归一化。

TODO:CODE

和以前一样，我们将在Fashion-MNIST数据集上训练我们的网络。这段代码实际上与我们第一次训练LeNet时的代码完全相同(第6.6节)。主要的区别是学习速度要大得多。

TODO:CODE

## 简洁的实现

与我们自己刚刚定义的BatchNorm类相比，我们可以直接使用深度学习框架的高级api中定义的BatchNorm类。代码看起来与我们上面的实现的应用程序几乎相同。

TODO:CODE


下面，我们使用相同的超参数来训练我们的模型。请注意，像往常一样，高级API变体运行得更快，因为它的代码已经编译成c++或CUDA，而我们的定制实现必须用Python解释。

TODO:CODE

## 争议

直观上，批处理规范化可以使优化过程更加平滑。 但是，在训练深度模型时，我们必须谨慎区分投机直觉和对现象的真实解释。 回想一下，我们甚至都不知道为什么更简单的深度神经网络（MLP和传统的CNN）首先能很好地泛化。 即使存在辍学和体重下降的情况，它们仍然具有很高的灵活性，以至于无法通过常规的学习理论上的泛化保证来解释其泛化为看不见的数据的能力。

在提出批量归一化的原始论文中，除了引入了强大而有用的工具外，作者还解释了其起作用的原因：通过减少内部协变量偏移。 推测是通过内部协变量移位，作者的意思类似于上面所表达的直觉，即变量值的分布在训练过程中会发生变化的概念。 但是，这种解释存在两个问题：i）这种漂移与协变量漂移有很大不同，这使名称不正确。 ii）解释提供的直觉不够明确，但是留下了一个问题，即为什么该技术确实需要一个需要严格解释的开放性问题。 在本书中，我们旨在传达从业人员用来指导其深层神经网络发展的直觉。 但是，我们认为将这些指导直觉与既定的科学事实分开很重要。 最终，当您掌握了这些材料并开始撰写自己的研究论文时，您将需要清楚地在技术主张和预感之间进行区分。

随着批量归一化的成功，它在内部协变量偏移方面的解释已反复出现在技术文献的辩论中，以及关于如何进行机器学习研究的广泛讨论。 在2017年NeurIPS会议上接受时间测试奖的纪念演讲中，阿里·拉希米（Ali Rahimi）以内部协变量移位为焦点，将现代深度学习与炼金术相提并论。 随后，在立场文件中详细回顾了该示例，概述了机器学习的令人担忧的趋势[Lipton＆Steinhardt，2018]。 其他作者为批处理规范化的成功提出了其他解释，一些人声称尽管其行为与原始论文所宣称的行为在某些方面相反，但批处理规范化的成功来自[Santurkar et al。，2018]。

我们注意到，与技术机器学习文献中每年提出的数千个类似模糊的主张中的任何一个相比，内部协变量的转变不值得批评。 之所以将其作为这些辩论的焦点，是因为其对目标受众的广泛认可。 批量归一化已被证明是必不可少的方法，几乎​​在所有已部署的图像分类器中都应用了该方法，从而获得了介绍该技术的论文数以万计的引文。


## 小结

* 在模型训练期间，批次归一化通过利用小批量的均值和标准偏差连续调整神经网络的中间输出，从而使整个神经网络中每一层的中间输出值更加稳定。
* 全连接层和卷积层的批处理归一化方法略有不同。
* 像辍学层一样，批归一化层在训练模式和预测模式下具有不同的计算结果。
* 批量归一化有很多有益的副作用，主要是正则化。另一方面，减少协变量偏移的原始动机似乎不是有效的解释。


## 练习

1. 在卷积计算中进行批量归一化或偏置参数之前，我们可以删除完全连接的仿射变换吗？
   1. 查找在完全连接的层之前应用的等效转换。
   2. 重新制定有效吗？ 为什么不）？
2. 比较有无批处理归一化的LeNet的学习率。
   1. 绘制训练和测试误差的减少图。
   2. 收敛区域呢？ 您可以将学习率提高多少？
3. 我们是否需要在每一层进行批量标准化？ 尝试一下吗？
4. 您可以用批量标准化替换Dropout吗？ 行为如何改变？
5. 固定系数beta和gamma，并观察和分析结果。
6. 查看BatchNorm的在线文档，以查看其他应用程序的Batch Normalization。
7. 研究思路：是否想到了可以应用的其他规范化转换？ 可以应用概率积分变换吗？ 全面协方差估计如何？
