

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-17 01:27:33
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-17 01:29:39
 * @Description:
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_convolutional-modern/batch-norm.html
 * https://zh.d2l.ai/chapter_convolutional-neural-networks/batch-norm.html
-->

## 小结

* 在模型训练期间，批次归一化通过利用小批量的均值和标准偏差连续调整神经网络的中间输出，从而使整个神经网络中每一层的中间输出值更加稳定。
* 全连接层和卷积层的批处理归一化方法略有不同。
* 像辍学层一样，批归一化层在训练模式和预测模式下具有不同的计算结果。
* 批量归一化有很多有益的副作用，主要是正则化。另一方面，减少协变量偏移的原始动机似乎不是有效的解释。


## 练习

1. 在卷积计算中进行批量归一化或偏置参数之前，我们可以删除完全连接的仿射变换吗？
   1. 查找在完全连接的层之前应用的等效转换。
   2. 重新制定有效吗？ 为什么不）？
2. 比较有无批处理归一化的LeNet的学习率。
   1. 绘制训练和测试误差的减少图。
   2. 收敛区域呢？ 您可以将学习率提高多少？
3. 我们是否需要在每一层进行批量标准化？ 尝试一下吗？
4. 您可以用批量标准化替换Dropout吗？ 行为如何改变？
5. 固定系数beta和gamma，并观察和分析结果。
6. 查看BatchNorm的在线文档，以查看其他应用程序的Batch Normalization。
7. 研究思路：是否想到了可以应用的其他规范化转换？ 可以应用概率积分变换吗？ 全面协方差估计如何？
