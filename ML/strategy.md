

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-12-27 19:34:09
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-27 19:35:13
 * @Description:
 * @TODO::
 * @Reference:https://kangcai.github.io/2018/11/30/ml-overall-7-strategy-regularization/
-->

策略部分就是评判“最优模型”（最优参数的模型）的准则和方法，图1是目标函数的函数形式，

目标函数包含了表征经验风险的代价函数和表征结构风险的正则化项，上篇文章已经介绍了正则化系数和损失函数：正则化系数是设置代价函数和正则化项约束权重的常量，代价函数是样本集的损失函数之和。

机器学习中正则化是什么？

正则化即Regularization，是对 y=wx+b（分类任务中的判别函数，回归问题中的拟合函数）中参数 w 的约束，在目标函数中体现就是关于 w 的函数项，通常按照惯例是不管b的，但如果在实际应用时将b也加上的话，其实影响也不大，预测效果基本没差别。wiki中给出了对正则化的很好的解释：

理论依据是试图将奥卡姆剃刀原则加入到求解过程
从性质上看，正则化是对模型的复杂性施加惩罚
从贝叶斯的观点来看，不同的正则化实现对应着对模型参数施加不同的先验分布
从作用上看，可以将规则化作为一种技术来提高学习模型的泛化能力，即提高模型对新数据的预测能力
后文将会对以上2-4点会进行详细的解释。

2）参数越稀疏代表模型越简单、越能防止过拟合吗？

是的。因为特征中真正重要的维度可能并不多，反映到模型中就是真正重要的模型参数可能并不多，保留尽可能少的重要的参数可以减弱非重要特征的波动带来的模型预测影响，防止过拟合，使模型的泛化效果更好。另一个好处是参数变少可以使整个模型获得更好的可解释性，而且还可以用做特征选择。

3）参数值越小代表模型越简单、越能防止过拟合吗？

是的。因为越复杂的模型，越是会尝试对所有的样本进行拟合，这就容易造成在较小的区间里预测值产生较大的波动，对于一些异常样本点波动就更大，这种较大的波动也反映了在这个区间里的导数很大，而只有较大的参数值才能产生较大的导数，因此复杂的模型，其参数值会比较大。相反地，参数小，则导数小，则对于样本特征的波动具有鲁棒性，那么模型能更好地防止过拟合，泛化效果会更好。

TODO:范数
