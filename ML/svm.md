

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-09 15:27:10
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-12-27 17:47:26
 * @Description:
 * @TODO::
 * @Reference:https://www.nowcoder.com/tutorial/95/f2446e6a55c244859d7a9bd0b24a6650
-->

1995年【正式SVM】，Vapnik和Cortes发表软间隔支持向量机（SVM算法），开启了随后的机器学习领域NN和SVM两大社区的竞争
## SVM的硬间隔，软间隔表达式；

$\min _{w, b} \frac{1}{2}\|w\|^{2}$
st. $\quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1$

$\min _{w, b} \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i}$
st. $\quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i} \quad \xi_{i} \geq 0$

左边为硬间隔；右边为软间隔

解析：不同点在于有无引入松弛变量

## SVM中什么时候用线性核什么时候用高斯核?

参考回答：
当数据的特征提取的较好,所包含的信息量足够大,很多问题是线性可分的那么可以采用线性核。若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用高斯核来达到更好的效果。

## 什么是支持向量机,SVM与LR的区别?

支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解。

LR是参数模型,SVM为非参数模型。LR采用的损失函数为logisticalloss,而SVM采用的是hingeloss。在学习分类器的时候,SVM只考虑与分类最相关的少数支持向量点。LR的模型相对简单,在进行大规模线性分类时比较方便。

## SVM使用对偶计算的目的是什么，如何推出来的，手写推导；

目的有两个：一是方便核函数的引入；二是原问题的求解复杂度与特征的维数相关，而转成对偶问题后只与问题的变量个数有关。由于SVM的变量个数为支持向量的个数，相较于特征位数较少，因此转对偶问题。通过拉格朗日算子发使带约束的优化目标转为不带约束的优化函数，使得W和b的偏导数等于零，带入原来的式子，再通过转成对偶问题。

## 如果给你一些数据集，你会如何分类（我是分情况答的，从数据的大小，特征，是否有缺失，分情况分别答的）；

根据数据类型选择不同的模型，如Lr或者SVM，决策树。假如特征维数较多，可以选择SVM模型，如果样本数量较大可以选择LR模型，但是LR模型需要进行数据预处理；假如缺失值较多可以选择决策树。选定完模型后，相应的目标函数就确定了。还可以在考虑正负样例比比，通过上下集采样平衡正负样例比。
解析：需要了解多种分类模型的优缺点，以及如何构造分类模型的步骤

https://github.com/0809zheng/UCAS-MachineLearning-homework/blob/master/SVM.py
