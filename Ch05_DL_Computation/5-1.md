

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-22 22:18:31
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-22 22:29:28
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai.s3-website-us-west-2.amazonaws.com/d2l-en/master/chapter_deep-learning-computation/model-construction.html
-->

# 层和块

当我们第一次引入神经网络时，我们集中于单输出的线性模型。在这里，整个模型只包含一个神经元。注意，单个神经元(i)需要一组输入; (ii)生成相应的标量输出; (iii)具有一组相关参数，可以更新这些参数以优化感兴趣的某些目标函数。然后，一旦我们开始考虑多输出的网络，我们利用向量算法来描述整个神经元层。就像单个神经元一样，层(i)取一组输入，(ii)产生相应的输出，(iii)由一组可调参数描述。当我们通过软最大回归，一个单一层本身的模型。然而，即使我们随后引入了 mlp，我们仍然可以认为模型保留了同样的基本结构。

有趣的是，对于 mlp，整个模型和它的组成层都共享这个结构。整个模型接受原始输入(特征) ，生成输出(预测) ，并拥有参数(来自所有组成层的组合参数)。同样，每一个单独的层输入(由前一层提供)生成输出(输入到后一层) ，并拥有一组可调参数，这些参数根据从后一层返回的信号进行更新。

虽然你可能认为神经元、层和模型给了我们足够的抽象来进行我们的业务，但事实证明，我们经常发现谈论比单个层大但比整个模型小的组件是很方便的。例如，在计算机视觉领域广泛流行的 ResNet-152体系结构拥有数百个层次。这些层是由一组又一组的重复模式组成的。一次只实现一个网络层可能会变得单调乏味。这种担心不仅仅是假设ーー这种设计模式在实践中很常见。上面提到的 ResNet 架构赢得了2015年 ImageNet 和 COCO 计算机视觉竞赛的识别和检测[ He et al. ，2016a ] ，并且仍然是许多视觉任务的首选架构。在其他领域，包括自然语言处理和语音处理，现在普遍存在着以各种重复模式排列的类似结构。

为了实现这些复杂的网络，我们引入了神经网络块的概念。一个块可以描述一个单一的层，一个由多个层组成的组件，或者整个模型本身！使用块抽象的一个好处是，它们可以组合成更大的工件，通常是递归的。图5.1.1说明了这一点。通过定义代码来根据需要生成任意复杂的块，我们可以编写出令人惊讶的紧凑代码，同时仍然可以实现复杂的神经网络。

图5.1.1多层结合成块，形成较大模型的重复模式。

从编程的角度来看，一个块由一个类来表示。它的任何子类都必须定义一个向前传播函数，将其输入转换为输出，并且必须存储任何必要的参数。请注意，有些块根本不需要任何参数。最后，块必须具有一个反向传播函数，用于计算梯度。幸运的是，在定义自己的块时，由于自动微分提供了一些幕后魔术(在2.5节中介绍) ，我们只需要担心参数和前向传播函数。

首先，我们回顾一下用于实现 MLPs 的代码(第4.3节)。下面的代码生成了一个网络，其中一个完全连接的隐藏层有256个单元，接着是一个完全连接的输出层有10个单元(没有激活函数)。

TODO:CODE

在这个例子中，我们通过实例化一个 nn 来构建我们的模型。顺序，按照应该执行的顺序将层作为参数传递。简而言之，不要。Sequential 定义了一种特殊的 Module，这个类在 PyTorch 中表示一个块。维护组成模块的有序列表。注意，两个完全连接的层中的每一个都是 Linear 类的一个实例，而 Linear 类本身就是 Module 的一个子类。前向传播(forward)函数也非常简单: 它将列表中的每个块链接在一起，将每个块的输出作为输入传递给下一个块。请注意，到目前为止，我们一直通过构造网(x)调用我们的模型来获得它们的输出。这实际上只是 net.forward (x)的简写，这是一个通过 Block 类的 _ call _ 函数实现的漂亮的 Python 技巧。

## 自定义块

也许培养直觉的最简单的方法就是我们自己实施一个方案。在我们实现自己的定制块之前，我们简要总结了每个块必须提供的基本功能:

1. 将输入数据作为其前向传播函数的参数。
2. 通过让正向传播函数返回一个值来生成输出。注意，输出的形状可能与输入的形状不同。例如，上面模型中的第一个完全连接层接收任意维度的输入，但返回维度256的输出。
3. 计算其输出相对于输入的梯度，可通过其反向传播函数访问。这通常是自动发生的。
4. 存储并提供对执行前向传播计算所需的那些参数的访问。
5. 根据需要初始化模型参数。

在以下代码段中，我们从头开始编写一个代码块，该代码块与一个MLP相对应，该MLP具有一个包含256个隐藏单元的隐藏层和一个10维输出层。请注意，下面的MLP类继承了代表块的类。我们将严重依赖父类的函数，仅提供我们自己的构造函数（Python中的__init__函数）和正向传播函数。

让我们首先关注前向传播函数。请注意，它将x作为输入，使用应用的激活函数计算隐藏表示，并输出其logit。在此MLP实现中，这两层都是实例变量。要了解为什么这样做是合理的，请想象实例化两个MLP net1和net2，并在不同的数据上对其进行训练。当然，我们希望它们代表两种不同的学习模型。

我们在构造函数中实例化MLP的层，然后在每次调用前向传播函数时调用这些层。请注意一些关键细节。首先，我们自定义的__init__函数通过super（）调用父类的__init__函数。__init __（）避免了重新编写适用于大多数块的样板代码的麻烦。然后，我们实例化两个完全连接的层，将它们分配给self.hidden和self.out。请注意，除非我们实现新的运算符，否则我们不必担心反向传播函数或参数初始化。系统将自动生成这些功能。让我们尝试一下。

TODO:CODE

块抽象的一个主要优点是它的多功能性。我们可以对一个块进行子类化以创建层（例如完全连接的层类），整个模型（例如上面的MLP类）或中等复杂度的各种组件。在接下来的几章中，我们将探讨这种多功能性，例如在解决卷积神经网络时。

## 顺序块

现在，我们可以仔细看看Sequential类的工作方式。回想一下，Sequential旨在将其他模块以菊花链方式链接在一起。要构建自己的简化MySequential，我们只需要定义两个关键函数：1.一个将块一个接一个地追加到列表的函数。2.前向传播函数，用于按块的附加顺序将输入传递给块链。

以下MySequential类提供与默认Sequential类相同的功能。

TODO:CODE

在 _ init _ 方法中，我们将每个块逐一添加到有序的 dictionary _ 模块中。你可能想知道为什么每个模块都有一个 _ modules 属性，为什么我们使用它而不是自己定义一个 Python 列表。简而言之，_ 模块的主要优点是在块的参数初始化过程中，系统知道在 _ 模块字典中查找参数也需要初始化的子块。

调用MySequential的前向传播函数时，每个添加的块均按添加顺序执行。现在，我们可以使用MySequential类重新实现MLP。

TODO:CODE

注意，MySequential的这种用法与我们之前为Sequential类编写的代码是一样的(如4.3节所述)。

## 在正向传播功能中执行代码

Sequential类使模型构建变得容易，允许我们在不定义自己的类的情况下组装新的体系结构。然而，并不是所有的架构都是简单的菊花链。当需要更大的灵活性时，我们将希望定义自己的块。例如，我们可能希望在前向传播函数中执行Python的控制流。此外，我们可能希望执行任意的数学操作，而不是简单地依赖于预定义的神经网络层。

您可能已经注意到，到目前为止，网络中的所有操作都是根据网络的激活及其参数进行操作的。但是，有时我们可能希望合并既不是以前层的结果也不是可更新参数的术语。我们称之为常数参数。我们来举个例子,想要一个层,计算函数f (x, w) = c⋅w⊤xf (x, w) = c⋅w⊤x, xx是输入,ww是我们的参数,和cc中某个指定的常数不更新优化。因此，我们实现了一个FixedHiddenMLP类，如下所示。

TODO:CODE

在此FixedHiddenMLP模型中，我们实现了一个隐藏层，其权重（self.rand_weight）在实例化时随机初始化，然后保持不变。该权重不是模型参数，因此永远不会通过反向传播进行更新。然后，网络将该“固定”层的输出通过一个完全连接的层。

请注意，在返回输出之前，我们的模型做了一些不寻常的事情。我们运行一个while循环，在条件下测试其L1L1范数大于11，然后将输出矢量除以22，直到满足条件为止。最后，我们返回x中条目的总和。据我们所知，没有标准的神经网络执行此操作。请注意，此特定操作在任何实际任务中可能都不有用。我们的目的只是向您展示如何将任意代码集成到神经网络计算流程中。

TODO:CODE

我们可以混合并匹配将块组装在一起的各种方式。在以下示例中，我们以一些创造性的方式嵌套了块。

TODO:CODE

## 编译

狂热的读者可能会开始担心其中某些操作的效率。毕竟，我们应该在高性能的深度学习库中进行大量的字典查找，代码执行以及许多其他Python事情。Python的全局解释器锁定问题众所周知。在深度学习的背景下，我们担心我们极快的GPU可能不得不等到一个微弱的CPU运行Python代码后才能运行另一项工作。加快Python速度的最佳方法是完全避免使用它。

Gluon做到这一点的一种方法是允许杂交，这将在后面描述。在这里，Python解释器在第一次调用时执行一个块。Gluon运行时会记录正在发生的事情，下次运行时会缩短对Python的调用。在某些情况下，这可以大大加快速度，但是，当控制流（如上）引导通过网络的不同通道上的不同分支时，需要格外小心。我们建议有兴趣的读者在阅读完本章后检查杂交部分（第12.1节），以了解有关编译的信息。

# 小结

* 层是块。
* 许多层可以包含一个块。
* 许多块可以组成一个块。
* 一个块可以包含代码。
* 块负责许多内务处理，包括参数初始化和反向传播。
* 层和块的顺序串联由顺序块处理。

## 练习

1. 如果将MySequential更改为将块存储在Python列表中，将会发生什么类型的问题？
1. 实现一个使用两个块作为参数的块，例如net1和net2，并在正向传播中返回两个网络的连接输出。这也称为并行块。
1. 假设您要串联同一网络的多个实例。实现一个工厂功能，该功能生成同一块的多个实例，并从中构建更大的网络。
