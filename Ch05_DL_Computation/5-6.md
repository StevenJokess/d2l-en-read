

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2020-07-22 22:39:39
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2020-07-22 23:08:20
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_deep-learning-computation/use-gpu.html
-->

# GPUs

在介绍中，我们讨论了过去二十年来计算的快速发展。简而言之，GPU性能自2000年以来每十年就增加1000倍。这提供了很大的机会，但也表明需要提供这样的性能。

TODO:table

在本节中，我们将开始讨论如何为您的研究利用这种计算性能。首先，通过使用单个GPU，稍后再介绍如何使用多个GPU和多个服务器（具有多个GPU）。

在本节中，我们将讨论如何使用单个NVIDIA GPU进行计算。首先，请确保您至少安装了一个NVIDIA GPU。然后，下载CUDA并按照提示设置适当的路径。这些准备工作完成后，可以使用nvidia-smi命令查看图形卡信息。

TODO:CODE

在PyTorch中，每个数组都有一个设备，我们通常将其称为上下文。到目前为止，默认情况下，所有变量和相关计算都已分配给CPU。通常，其他上下文可能是各种GPU。当我们在多台服务器上部署作业时，事情变得更加艰巨。通过智能地将数组分配给上下文，我们可以最大程度地减少在设备之间传输数据所花费的时间。例如，当在带有GPU的服务器上训练神经网络时，我们通常希望模型的参数存在于GPU上。

接下来，我们需要确认已安装GPU版本的PyTorch。如果已经安装了CPU版本的PyTorch，我们需要先将其卸载。例如，使用pip uninstall torch命令，然后根据您的CUDA版本安装相应的PyTorch版本。假设您已安装CUDA 9.0，则可以通过pip install torch-cu90安装支持CUDA 9.0的PyTorch版本。要运行本节中的程序，您至少需要两个GPU。

请注意，这对于大多数台式机来说可能是奢侈的，但是例如通过使用AWS EC2多GPU实例在云中很容易获得。几乎所有其他部分不需要多个GPU。相反，这只是为了说明数据如何在不同设备之间流动。

## 计算设备

我们可以指定用于存储和计算的设备，比如cpu和gpu。默认情况下，张量是在主存中创建的，然后使用CPU来计算。

在PyTorch中，可以通过torch.device（'cpu'）和torch.cuda.device（'cuda'）指示CPU和GPU。应该注意的是，cpu设备表示所有物理CPU和内存。这意味着PyTorch的计算将尝试使用所有CPU内核。但是，一个gpu设备仅代表一张卡和相应的内存。如果有多个GPU，我们使用torch.cuda.device（f'cuda {i}'）表示第一个GPU（ii从0开始）。同样，gpu：0和gpu是等效的。

TODO:CODE

我们可以查询可用GPU的数量。

TODO:CODE

现在我们定义了两个方便的函数，允许我们运行代码，即使请求的gpu不存在。

TODO:CODE

## 张量和GPU

默认情况下，张量是在CPU上创建的。我们可以查询张量所在的装置。

TODO:CODE

需要注意的是，无论何时我们想对多个术语进行操作，它们都需要在相同的上下文中。例如，如果我们将两个张量相加，我们需要确保这两个参数都在同一个设备上——否则框架将不知道在哪里存储结果，甚至不知道如何决定在哪里执行计算。

### 在GPU上的存储

有多种方法可以在GPU上存储张量。例如，我们可以在创建张量时指定存储设备。接下来，我们在第一个gpu上创建张量变量a。请注意，打印a时，设备信息已更改。在GPU上创建的张量仅消耗该GPU的内存。我们可以使用nvidia-smi命令查看GPU内存使用情况。通常，我们需要确保不创建超出GPU内存限制的数据。

假设您至少有两个GPU，下面的代码将在第二个GPU上创建一个随机数组。

### 复制

如果要计算x + y，则需要确定在哪里执行此操作。例如，如图5.6.1所示，我们可以将xx转移到第二个GPU并在那里执行操作。不要简单地添加x + y，因为这将导致异常。运行时引擎不知道该怎么办，无法在同一设备上找到数据并且失败。

图5.6.1 Copyto将阵列复制到目标设备

copyto将数据复制到另一台设备，以便我们可以添加它们。由于yy驻留在第二个GPU上，因此我们需要先将xx移动到那里，然后再添加两者。

TODO:CODE

现在，数据位于同一GPU（z和y都在）上，我们可以将它们加起来。

TODO:CODE

假设变量z已经在第二个GPU上运行。如果我们仍然调用z.cuda(1)会发生什么?它将返回z，而不是复制并分配新的内存。

假设变量z已经在第二个GPU上运行。如果我们在相同的设备范围内仍然调用z2 = z会发生什么?它将返回z，而不是复制并分配新的内存。

TODO:CODE

### 另外

人们使用GPU进行机器学习是因为他们期望它们会很快。但是在上下文之间传递变量很慢。因此，我们希望您100％确定在执行操作之前要先执行缓慢的操作。如果框架只是自动进行复制而不会崩溃，那么您可能不会意识到自己编写了一些慢速代码。

此外，在设备（CPU，GPU，其他机器）之间传输数据比计算要慢得多。这也使并行化变得更加困难，因为我们必须等待数据发送（或接收），然后才能进行更多操作。这就是为什么应格外小心进行复制操作的原因。根据经验，许多小手术要比一项大手术差很多。而且，一次执行多个操作要比插入到代码中的许多单个操作要好得多（除非您知道自己在做什么）。之所以如此，是因为如果一个设备必须先等待另一个设备才能执行其他操作，则此类操作可能会阻塞。这有点像是在队列中订购咖啡，而不是通过电话预先订购咖啡并发现咖啡准备就绪。

最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在主内存中，框架将首先将其复制到主内存，从而导致额外的传输开销。更糟糕的是，它现在受到可怕的Global Interpreter Lock的约束，这使一切都等待Python完成。

人们使用GPU进行机器学习是因为他们期望它们会很快。但是在上下文之间传递变量很慢。因此，我们希望您100％确定在执行操作之前要先执行缓慢的操作。如果框架只是自动进行复制而不会崩溃，那么您可能不会意识到自己编写了一些慢速代码。

此外，在设备（CPU，GPU，其他机器）之间传输数据比计算要慢得多。这也使并行化变得更加困难，因为我们必须等待数据发送（或接收），然后才能进行更多操作。这就是为什么应格外小心进行复制操作的原因。根据经验，许多小手术要比一项大手术差很多。而且，一次执行多个操作要比插入到代码中的许多单个操作要好得多（除非您知道自己在做什么）。之所以如此，是因为如果一个设备必须先等待另一个设备才能执行其他操作，则此类操作可能会阻塞。这有点像是在队列中订购咖啡，而不是通过电话预先订购咖啡并发现咖啡准备就绪。

最后，当我们打印张量或将张量转换为NumPy格式时，如果数据不在主内存中，框架将首先将其复制到主内存，从而导致额外的传输开销。更糟糕的是，它现在受到可怕的Global Interpreter Lock的约束，这使一切都等待Python完成。

## 神经网络和GPU

类似地，神经网络模型可以指定设备。以下代码将模型参数放到GPU上（下面将看到更多关于如何在GPU上运行模型的示例，只是因为它们会变得计算量更大）。

TODO:CODE

让我们确认模型参数存储在同一GPU上。

TODO:CODE

简而言之，只要所有数据和参数都在同一设备上，我们就可以有效地学习模型。在下面，我们将看到几个这样的示例。

## 小结

* 我们可以指定用于存储和计算的设备，例如CPU或GPU。默认情况下，数据在主存储器中创建，然后使用CPU进行计算。
* 该框架要求所有用于计算的输入数据都在同一设备上，无论是CPU还是同一GPU。
* 如果不小心移动数据，可能会损失大量性能。一个典型的错误如下：计算GPU上每个微型批处理的损失并在命令行上将其报告给用户（或将其记录在NumPy数组中）将触发全局解释器锁定，该锁定将使所有GPU停止运行。最好为GPU内部的日志分配内存，并且只移动较大的日志。

## 练习

1. 尝试执行较大的计算任务，例如大型矩阵的乘法，然后查看CPU和GPU之间的速度差异。只需少量计算的任务呢？
1. 我们应该如何在GPU上读写模型参数？
1. 测量计算100×100100×100矩阵的1000个矩阵-矩阵乘法并记录矩阵范数trMM⊤trMM⊤一次所需的时间与在GPU上保留日志并仅传输最终结果所需的时间。
1. 测量在两个GPU上同时执行两个矩阵矩阵乘法与在一个GPU上依次执行两个矩阵矩阵乘法所需的时间（提示：您应该看到几乎线性缩放）。
