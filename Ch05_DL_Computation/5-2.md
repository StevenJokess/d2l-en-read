

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-23 00:20:51
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-07-23 00:47:31
 * @Description:translate by machine
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_deep-learning-computation/parameters.html
 * https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html
-->

# 参数管理

一旦我们选择了一个架构并设置了超参数，我们就进入了训练循环，我们的目标是找到最小化目标函数的参数值。训练之后，我们需要这些参数来做出未来的预测。此外，我们有时希望提取参数，以便在其他上下文中重用它们，将模型保存到磁盘，以便在其他软件中执行，或者进行检查，以期获得科学的理解。

大多数情况下，我们可以忽略参数如何声明和操作的细节，依靠框架来完成繁重的工作。然而，当我们远离带有标准层的堆叠架构时，我们有时需要进入声明和操作参数的杂草中。在本节中，我们将讨论以下内容:

* 访问用于调试、诊断和可视化的参数。
* 参数初始化。
* 跨不同模型组件共享参数。

我们首先关注一个带有隐藏层的 MLP。

TODO:CODE

## 参数访问

让我们从如何访问您已经知道的模型中的参数开始。当通过 Sequential 类定义一个模型时，我们可以首先通过对模型的索引来访问任何层，就好像它是一个列表一样。每个层的参数可以方便地定位在它的属性中。我们可以检测的第二个完全连接层的参数如下。

TODO:CODE

输出结果告诉我们一些重要的事情。首先，这个完全连接的层包含两个参数，分别对应于该层的权值和偏置。两者都存储为单个精度浮点数。请注意，参数的名称允许我们唯一地标识每个层的参数，即使是在一个包含数百个层的网络中。

### 目标参数

注意，每个参数都表示为参数类的一个实例。要使用参数执行任何有用的操作，我们首先需要访问底层的数值。有几种方法可以做到这一点。一些更简单，而另一些则更一般。下面的代码从第二个神经网络层提取偏差，该层返回一个参数类实例，并进一步访问该参数的值。

参数是复杂的对象，包含数据、梯度和其他信息，这就是我们需要显式请求数据的原因。

除了数据之外，每个参数还提供了访问梯度的梯度方法。因为我们还没有为这个网络调用反向传播，所以它处于初始状态。

TODO:CODE

### 一次所有参数

当我们需要对所有参数执行操作时，一对一访问它们可能会变得乏味。当我们处理更复杂的块（例如嵌套块）时，情况变得尤为棘手，因为我们需要遍历整个树以提取每个子块的参数。下面我们展示了访问第一个完全连接层的参数与访问所有层的参数。

TODO:CODE

这为我们提供了另一种访问网络参数的方法：

TODO:CODE

### 从嵌套块中收集参数

让我们看看如果我们将多个块嵌套在一起，参数命名约定是如何工作的。为此，我们首先定义一个产生块的函数(可以说是块工厂)，然后将这些块合并到更大的块中。

TODO:CODE

现在我们已经设计了网络，让我们看看它是如何组织的。

TODO:CODE

### 参数初始化

现在我们知道了如何访问参数，让我们看一下如何正确地初始化它们。我们在4.8节中讨论了初始化的必要性。该框架为其层提供默认的随机初始化。但是，我们经常想根据各种其他协议初始化权重。该框架提供了最常用的协议，并且还允许创建客户初始化程序。

默认情况下，PyTorch 通过从根据输入和输出维度计算的范围中绘制均匀的权重和偏差矩阵来初始化。的 nn.init 模块提供了各种预置初始化方法。

### 内置初始化

让我们从调用内置初始化器开始。下面的代码将所有权重参数初始化为标准差为.01的高斯随机变量，而偏差参数设置为0。

TODO:CODE

我们还可以将所有参数初始化为给定的常量值（例如,1）。

TODO:CODE

我们还可以为某些块应用不同的初始化器。例如，下面我们使用Xavier初始化器初始化第一层，并将第二层初始化为一个常数值42。

## 自定义初始化

有时，框架没有提供我们需要的初始化方法。在下面的示例中，我们为以下奇怪的分布定义了一个初始化程序：

TODO:MATH

同样，我们实现了`my_init`函数以应用于`net`。

TODO:CODE

注意，我们总是可以选择直接设置参数。

TODO:CODE

## 共享模型参数

通常，我们希望跨多个层共享参数。稍后我们将看到，在学习单词嵌入时，使用相同的参数对单词进行编码和解码可能是明智的。在介绍第5.1节时，我们讨论了一个这样的例子。让我们看看如何更优雅地做这个。在下面我们分配一个密集的层，然后使用它的参数专门设置那些其他层。

TODO:CODE

此示例显示第二层和第三层的参数是绑定的。它们不仅相等，而且由相同的精确张量表示。因此，如果我们更改一个参数，那么另一个也会更改。您可能想知道，当绑定参数时，渐变会发生什么？ 由于模型参数包含梯度，因此第二隐藏层和第三隐藏层的梯度会在反向传播期间加在一起。

## 小结

* 我们有几种方法来访问、初始化和绑定模型参数。
* 我们可以使用自定义初始化。

## 练习

1. 使用5.1节中定义的FancyMLP并访问各个层的参数。
1. 查看初始化模块文档，了解不同的初始化器。
1. 构造一个包含共享参数层的多层感知器并训练它。在训练过程中，观察模型参数和各层的梯度。
1. 为什么共享参数是一个好主意?
