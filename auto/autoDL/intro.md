

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-10-08 19:37:02
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-08 19:46:30
 * @Description:
 * @TODO::
 * @Reference:
-->

自动化深度学习的目标是通过元学习的方法让机器学会自动调参优化。[1]


## NAS

### 搜索空间

可供搜索的一个网络结构集合
它的数字表示为：
·网络的结构（如神经网络的深度，即隐藏层个数，以及特定的隐藏层宽度）
·配置（如操作/网络间的链接类型、核的大小、过滤器的数量）

给定搜索空间，可以将其中的神经网络结构编码成该空间下的表示。这种搜索空间被称为marco（宏）搜索空间。marco搜索空间下的网络深度是固定的。

有向无环图

块单元

元架构

### 搜索算法

搜索算法是一个迭代过程，用于确定以何种规则来探索搜索空间。在搜索过程的每个步骤或迭代中，一个来自于搜索空间的样本会被生成，即子网络（child network）。所有的子网络在训练集上被训练，在验证集上的准确率作为目标被优化（或者是强化学习中的奖励）。搜索算法的目的是找到最佳子网络，例如最小化验证集损失或最大化奖励。

神经网络架构存在以下特点：1）评价函数未知，是一个黑箱优化问题，因为评价往往是在不可见的数据集上进行评价。2）神经网络结构搜索通常都是非线性问题，比较复杂。3）神经网络搜索是非凸问题，即局部最优解并非全局最优解。4）神经网络搜索问题通常是混合优化问题，即问题空间既包括离散空间，又包括连续空间。5）一次优化结果的评价非常耗时，大型的深度学习模型参数数亿计，运行一次结果需要几周的时间。6）在某些应用场景中，存在多个目标。例如：移动端的模型结构优化，既希望得到尽量高的准确率，又希望有非常好的模型计算效率。


#### 强化学习

又称评价学习，是一种重要的机器学习方法，它以“试错”的方式进行学习，通过与环境交互获得奖励来指导行为。在NAS任务中的强化学习问题和传统的强化学习问题略有不同，其主要流程是构建了一个RNN控制器，通过迭代的方式来更新控制器从而生成合适的架构。强化学习是一个很庞大的算法体系，采用不同的学习算法就是一种新的架构搜索方法。研究最广泛的一种搜索的算法就是使用Q-learning训练控制器，依次选择层的类型和相关的参数。

#### 进化算法

使用进化算法的一大好处是只要进化代数足够，就可以得到全局最优解，但是这也是非常消耗算力的一个过程，一个解决方法就是使用分布式计算来提高进化效率。用进化算法进行神经架构搜索

分级优化，以及逆强化学习和超网络的使用。
梯度优化
贝叶斯优化

### 加速方案

全部训练

部分训练

往往大规模的数据集训练效果更佳，但这是非常耗时的

权值共享

权值共享，通过对NAS搜索空间的优化实现了权值共享功能，大大节约了搜索时间，代表算法是ENAS（详见第11章）。

网络态射

网络转换/态射来探索搜索空间，它使用诸如插入层或添加跳过连接之类的操作将训练好的神经网络修改为新的结构。由于网络转换/态射从现有的训练网络开始，因此重用权重并且仅需为数不多的训练迭代来完成新结构的训练。

超网络

[1]: https://weread.qq.com/web/reader/62332d007190b92f62371aek3ef329302553ef815416990
