

<!--
 * @version:
 * @Author:  StevenJokess https://github.com/StevenJokess
 * @Date: 2020-07-13 15:56:24
 * @LastEditors:  StevenJokess https://github.com/StevenJokess
 * @LastEditTime: 2020-10-07 21:03:31
 * @Description:MT, improve 3
 * @TODO::
 * @Reference:http://preview.d2l.ai/d2l-en/master/chapter_attention-mechanisms/transformer.html
 * https://github.com/d2l-ai/d2l-en/blob/master/chapter_attention-mechanisms/transformer.md
-->

# Transformer
:label:`sec_transformer`

在前面的章节中，我们已经涵盖了主要的神经网络体系结构，如卷积神经网络(CNNs)和回归神经网络(RNNs)。让我们回顾一下它们的优缺点:

* **CNNs**很容易在一个层上并行化，但不能很好地捕获可变长度的顺序依赖关系。
* **RNNs**能够捕获长距离、可变长度的顺序信息，但是无法在一个序列内并行化。

为了结合 CNNs 和 RNNs 的优势，:cite:`Vaswani.Shazeer.Parmar.ea.2017`设计了一种使用注意力机制的新型结构。这种体系结构被称为 *Transformer*，它通过捕获循环序列和注意力来实现并行化，同时对序列中每个项目的位置进行编码。因此，*Transformer* 导致了一个具有显著缩短训练时间的兼容模型。

与:numref:`sec_seq2seq`中的 seq2seq 模型类似，Transformer 也是基于编码器-解码器结构的。然而，Transformer 不同于前者，它将 seq2seq 中的递归层替换为多头注意层，通过位置编码合并位置信息，并应用层归一化。在图10.3.1中，我们比较了 Transformer 和 seq2seq。

总之，这两个模型是相似的: 源序列嵌入被反馈到 n 个重复块中。最后一块的输出然后用作解码器的注意力存储器。将目标序列嵌入同样地输入到解码器中的 n 个重复块中，并通过对最后一个块的输出应用具有词汇表大小的稠密层来获得最终输出。

![变压器结构。](../img/transformer.svg)
:width:`500px`
:label:`fig_transformer`

另一方面，Transformer 与 seq2seq with attention 模型的区别如下:

1. **变压器块**: seq2seq 中的循环层被 Transformer 块替换。这块包含一个多头注意层和位置明智的前馈网络与两层的编码器。对于解码器，另一个多头注意层用于采取编码器的状态。
2. **添加与规范**: 多头注意层或位置前馈网络的输入输出由两个“添加与规范”层进行处理，该层包含一个剩余结构和一个层规范化层。
3. **位置编码**: 由于自注意层不区分序列中的项目顺序，因此使用位置编码层向每个序列项添加顺序信息。

在本节的其余部分中，我们将为您配备 Transformer 引入的每个新组件，并让您启动并运行构建机器翻译模型。

TODO:CODE

## 多头注意层

在讨论**多头注意层**之前，让我们快速的表达一下这个自我注意的架构。**自我注意模型**是一种普通的注意力模型，它的查询、键和值都是从序列输入的每个项中完全相同地复制出来的。正如我们在:numref:`fig_self_attention`中所示，自我注意为每个输入项输出相同长度的顺序输出。与循环层相比，自注意层的输出项可以并行计算，因此易于获得高效的实现。

[自注意结构。](../img/self-attention.svg)
:label:`fig_self_attention`

多头注意力层由h个平行的自我注意层组成，每个层称为头。对于每个头部，在进入关注层之前，我们将查询，键和值投影到三个密集层，其隐藏大小分别为$p_q$，$p_k$和$p_v$。这些注意头的输出被连接起来，然后由最终的致密层进行处理。

[多头注意力](../img/multi-head-attention.svg)

假设查询的维，键和值分别是$d_q$，$d_k$和$d_v$。然后，对于每个头i = 1，...，hi = 1，...，h，我们可以训练可学习的参数$\mathbf W_q^{(i)}\in\mathbb R^{p_q\times d_q}$,
$\mathbf W_k^{(i)}\in\mathbb R^{p_k\times d_k}$,
和 $\mathbf W_v^{(i)}\in\mathbb R^{p_v\times d_v}$.。因此，每个磁头的输出为

$$\mathbf o^{(i)} = \mathrm{attention}(\mathbf W_q^{(i)}\mathbf q, \mathbf W_k^{(i)}\mathbf k,\mathbf W_v^{(i)}\mathbf v),$$

其中，$\textrm{attention}$可以是任何关注层，例如我们在:numref:`sec_attention`中介绍的DotProductAttention和MLPAttention。

之后，来自每个hh注意头的长度为pvpv的输出被合并为长度为hpvhpv的输出，然后将其传递到具有dodo隐藏单元的最终密集层。该致密层的权重可以表示为Wo∈Rdo×hpvWo∈Rdo×hpv。结果，多头注意力输出将是

$$\mathbf o = \mathbf W_o \begin{bmatrix}\mathbf o^{(1)}\\\vdots\\\mathbf o^{(h)}\end{bmatrix}.$$

现在我们可以实现多头注意了。假设多头注意包含的头数`num_heads` $=h$，隐藏大小`num_hiddens` $=p_q=p_k=p_v$对于查询，键和值密集层而言是相同的。另外，由于多头注意在其输入和输出之间保持相同的维数，因此我们也具有输出要素大小$d_o =$ `num_hiddens`。

TODO:CODE

这是转置函数的定义`transpose_qkv`和`transpose_output`，它们是彼此的逆。

TODO:CODE

让我们在一个玩具示例中测试MultiHeadAttention模型。创建一个隐藏大小为$d_o = 100$的多头注意，输出将与输入共享相同的批处理大小和序列长度，但最后一个维度将等于`num_hiddens` $= 100$。

## 位置前馈网络

变压器模块中的另一个关键组件称为位置前馈网络（FFN）。它接受具有形状（批处理大小，序列长度，特征大小）的33维输入。位置FFN由应用于最后一个维度的两个密集层组成。由于序列中的每个位置项都使用相同的两个密集层，因此我们将其称为位置方式。实际上，这等效于应用两个1×1卷积层。

下面，PositionWiseFFN显示了如何使用两个隐藏层分别为ffn_num_hiddens和pw_num_outputs的密集层来实现位置FFN。

TODO:CODE

类似于多头注意，按位置前馈网络将仅更改输入的最后尺寸（特征尺寸）。另外，如果输入序列中的两个项目相同，则相应的输出也将相同。

TODO:CODE

让我们在一个玩具示例中测试MultiHeadAttention模型。 创建一个隐藏大小为do = 100的多头注意，输出将与输入共享相同的批处理大小和序列长度，但最后一个维度将等于num_hiddens = 100

## Position-wise前馈网络

变压器块的另一个关键部件称为位置前馈网络(FFN)。它接受带有形状(批量大小、序列长度、特征大小)的33维输入。位置上的FFN由适用于最后一个维度的两个密集层组成。由于序列中的每个位置项都使用了相同的两个稠密层，因此我们将其称为位置wise。实际上，它等价于应用两个1×11×1的卷积层。

下面，PositionWiseFFN展示了如何实现具有两个密集层的位置智能FFN，隐藏大小分别为ffn_num_hiddens和pw_num_outputs。

TODO:CODE

类似于多头注意，按位置前馈网络将仅更改输入的最后维度大小-特征维度。 另外，如果输入序列中的两项相同，则相应的输出也将相同。

TODO:CODE

## 加和正则化

除了Transformer块中的上述两个组件外，该块中的“ add and norm”还起着平稳连接其他层的输入和输出的关键作用。为说明起见，我们在多头注意力层和位置FFN网络之后都添加了一个包含残差结构的层和一个层归一化。层归一化与7.5节中的批量归一化相似。一个区别是，沿着最后一个维度（例如X.mean（axis = -1））而不是第一个批次维度（例如X.mean（axis = 0））计算层归一化的平均值和方差。图层归一化可以防止图层中的值范围变化太大，从而可以更快地进行训练并具有更好的泛化能力。

MXNet在nn块中实现了LayerNorm和BatchNorm。让我们将它们都调用，并在下面的示例中看到区别。

TODO:CODE

现在，让我们一起实现连接块AddNorm。AddNorm接受两个输入X和Y。我们可以将XX视为残差网络中的原始输入，将Y视为多头注意力层或位置FFN网络的输出。另外，我们在Y上应用辍学进行正则化。

TODO:CODE

由于剩余的连接，X和Y应该具有相同的形状。

TODO:CODE

## Positional Encoding

与循环层不同，多头注意层和位置前馈网络均独立计算序列中各项的输出。此功能使我们能够并行化计算，但无法为给定序列建模序列信息。为了更好地捕获顺序信息，Transformer模型使用位置编码来维护输入序列的位置信息。

为了说明，假设$X\in\mathbb R^{l\times d}$是示例的嵌入，其中$l$是序列长度，而$d$是嵌入大小。该位置编码层对X的位置$P\in\mathbb R^{l\times d}$进行编码，并输出$P+X$。

位置PP是一个二维矩阵，其中ii表示句子中的顺序，jj表示沿嵌入矢量维的位置。这样，就可以使用以下等式维护原始序列中的每个值：

$$P_{i, 2j} = \sin(i/10000^{2j/d}),$$

$$\quad P_{i, 2j+1} = \cos(i/10000^{2j/d}),$$

for $i=0,\ldots, l-1$ and $j=0,\ldots,\lfloor(d-1)/2\rfloor$.

:numref:`fig_positional_encoding` 说明了位置编码。

![Positional encoding.](../img/positional-encoding.svg)
:label:`fig_positional_encoding`

TODO:CODE

现在我们用一个4维的玩具模型来测试PositionalEncoding类。正如我们所看到的，第4维与第5维有相同的频率，但有不同的偏移(即相位)，因为一个是由正弦函数产生的，另一个是由余弦函数产生的。第6、第7维有较低的频率。

TODO:CODE

## 编码层

配备了Transformer的所有基本组件，让我们首先构建一个Transformer编码器模块。该编码器包含一个多头注意层，一个位置前馈网络和两个“ add and norm”连接块。如代码所示，对于EncoderBlock中的注意力模型和位置FFN模型，其输出尺寸均等于num_hiddens。这是由于残差块的性质所致，因为我们需要在“加法和范数”期间将这些输出加回到原始值。

TODO:CODE

由于剩余的连接，该块将不会更改输入形状。这意味着num_hiddens参数应等于最后一个维度的输入大小。在下面的玩具示例中，num_hiddens = 24 = 24，ffn_num_hiddens = 48 = 48，num_heads = 8 = 8以及辍学= 0.5 = 0.5。

TODO:CODE

现在是整个Transformer编码器的实现。使用Transformer编码器，' EncoderBlock '的$n$块一个接一个地堆积起来。由于有残余连接，嵌入层大小$d$与变压器块输出大小相同。还请注意，我们将嵌入输出乘以$\sqrt{d}$，以防止其值太小。

## 解码层

变压器解码器块看起来类似于变压器编码器块。但是，除了两个子层（多头注意层和位置编码网络）之外，解码器Transformer块还包含第三子层，该第三子层将多头注意应用于编码器堆栈的输出。与Transformer编码器块类似，Transformer解码器块采用“加法和范数”，即残差连接和层归一化来连接每个子层。

具体而言，在时间步t处，假设$\mathbf x_t$是当前输入，即查询。如:numref:`fig_self_attention_predict`所示，自我注意层的键和值包括当前查询以及所有过去的查询$\mathbf x_1, \ldots, \mathbf x_{t-1}$。

[在时间步t预测自我注意层。](../img/self-attention-predict.svg)
:label:`fig_self_attention_predict`

在训练期间，t-query的输出可以观察到所有先前的键值对。它导致与预测不同的行为。因此，在预测期间，我们可以通过将第t次查询的有效长度指定为t来消除不必要的信息。

TODO:CODE

与Transformer编码器块类似，num_hiddens应该等于$X$的最后尺寸。

TODO:CODE

整个Transformer解码器的结构与Transformer编码器相同，除了用于获得输出置信度分数的稠密层。

让我们实现Transformer解码器`TransformerDecoder`。除了`vocab_size`和`num_hiddens`之类的常规超参数外，Transformer解码器还需要Transformer编码器的输出`enc_outputs`和`env_valid_len`。

TODO:CODE

## 训练

最后，我们可以利用Transformer架构建立一个编码器-解码器模型。类似于:numref:`sec_seq2seq_attention`中的带有注意模型的seq2seq，我们使用以下超参数:两个Transformer块，嵌入大小和块输出大小都为$32$。此外，我们使用$4$个头，并设置隐藏大小为输出大小的两倍。

TODO:CODE

从训练时间和准确率上可以看出，与带有注意力模型的seq2seq模型相比，Transformer在每历元上的运行速度更快，开始时的收敛速度也更快。

我们可以使用经过训练的转换器来翻译一些简单的句子。

TODO:CODE

## 小结

* Transformer模型基于编码器-解码器体系结构。
* 多头注意层包含h个并行注意层。
* 位置前馈网络由应用于最后一个维度的两个密集层组成。
* 图层归一化与批次归一化的不同之处在于，沿最后一个维度（要素维度）而不是第一个维度（批量大小）进行归一化。
* 位置编码是将位置信息添加到Transformer模型的唯一位置。

## 练习

1. 尝试使用更大的epoch，并比较seq2seq模型和Transformer模型之间的损失。
1. 您还能想到位置编码的其他好处吗？
1. 比较层归一化和批量归一化，什么时候应用哪一个？
